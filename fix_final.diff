--- a/server/ab_snapshot.py
+++ b/server/ab_snapshot.py
@@ -1,215 +1,321 @@
 """
 ABSnapshot - Memory-Resident A/B Comparison Manager
 
-Implements FR-008, Literal
+Implements FR-008: In-memory A/B snapshots with instant toggle and glitch-free switching
+"""
+
+import logging
+from typing import Optional, Literal
 import copy
 import time
 
 from .preset_model import Preset, create_default_preset
 
 logger = logging.getLogger(__name__)
 
 
 ABSlot = Literal["A", "B"]
 
 
 class ABSnapshot:
     """
     A/B comparison snapshot manager
 
+    Features:
+    - Memory-resident snapshots (not auto-saved to disk)
     - Instant toggle between A and B states
     - Glitch-free switching with crossfade guard
     - State preservation across preset loads
     """
 
     CROSSFADE_GUARD_MS = 30  # Minimum time for smooth transition
 
-    def __init__(self) :
+    def __init__(self):
         """Initialize A/B snapshot manager"""
         self.snapshot_a: Optional[Preset] = None
         self.snapshot_b: Optional[Preset] = None
-        self.current_slot)
+        self.current_slot: Optional[ABSlot] = None
+        self.last_toggle_time = 0.0
 
-    def store_a(self, preset: Preset) :
+        logger.info("ABSnapshot initialized")
+
+    def store_a(self, preset: Preset):
         """
         Store current state as snapshot A
 
         Args:
-            preset)
+            preset: Preset to store in slot A
+        """
+        # Deep copy to prevent external modifications
+        self.snapshot_a = copy.deepcopy(preset)
         self.current_slot = "A"
 
-        logger.info("Stored preset in slot A, preset.name)
+        logger.info("Stored preset in slot A: %s", preset.name)
 
-    def store_b(self, preset: Preset) :
+    def store_b(self, preset: Preset):
         """
         Store current state as snapshot B
 
         Args:
-            preset)
+            preset: Preset to store in slot B
+        """
+        # Deep copy to prevent external modifications
+        self.snapshot_b = copy.deepcopy(preset)
         self.current_slot = "B"
 
-        logger.info("Stored preset in slot B, preset.name)
+        logger.info("Stored preset in slot B: %s", preset.name)
 
-    def get_a(self) :
+    def get_a(self) -> Optional[Preset]:
         """
         Get snapshot A
 
         Returns:
             Deep copy of snapshot A or None
         """
-        if self.snapshot_a)
+        if self.snapshot_a:
+            return copy.deepcopy(self.snapshot_a)
         return None
 
-    def get_b(self) :
+    def get_b(self) -> Optional[Preset]:
         """
         Get snapshot B
 
         Returns:
             Deep copy of snapshot B or None
         """
-        if self.snapshot_b)
+        if self.snapshot_b:
+            return copy.deepcopy(self.snapshot_b)
         return None
 
-    def toggle(self) :
+    def toggle(self) -> Optional[Preset]:
         """
         Toggle between A and B snapshots
 
         Implements FR-008 requirement for glitch-free switching
 
         Returns:
             Preset to apply or None if toggle not possible
 
         Raises:
             ValueError: If both snapshots are empty
         """
         if self.snapshot_a is None and self.snapshot_b is None:
-            raise ValueError("Cannot toggle)
+            raise ValueError("Cannot toggle: Both A and B snapshots are empty")
 
         # Enforce crossfade guard time
         current_time = time.time()
         time_since_last_toggle = (current_time - self.last_toggle_time) * 1000  # ms
 
-        if time_since_last_toggle < self.CROSSFADE_GUARD_MS)", time_since_last_toggle, self.CROSSFADE_GUARD_MS)
+        if time_since_last_toggle < self.CROSSFADE_GUARD_MS:
+            print(f"[ABSnapshot] Toggle too fast ({time_since_last_toggle:.1f}ms < {self.CROSSFADE_GUARD_MS}ms)")
             return None
 
         # Determine target slot
         if self.current_slot == "A":
             if self.snapshot_b is None:
-                raise ValueError("Cannot toggle to B)
+                raise ValueError("Cannot toggle to B: Snapshot B is empty")
             target_slot = "B"
             target_preset = self.get_b()
         elif self.current_slot == "B":
             if self.snapshot_a is None:
-                raise ValueError("Cannot toggle to A)
+                raise ValueError("Cannot toggle to A: Snapshot A is empty")
             target_slot = "A"
             target_preset = self.get_a()
         else:
             # No current slot - default to A
-            if self.snapshot_a)
-            elif self.snapshot_b)
+            if self.snapshot_a:
+                target_slot = "A"
+                target_preset = self.get_a()
+            elif self.snapshot_b:
+                target_slot = "B"
+                target_preset = self.get_b()
             else:
                 return None
 
         # Update state
         self.current_slot = target_slot
         self.last_toggle_time = current_time
 
-        logger.info("[ABSnapshot] Toggled to %s, target_slot, target_preset.name if target_preset else 'None')
+        print(f"[ABSnapshot] Toggled to {target_slot}: {target_preset.name if target_preset else 'None'}")
 
         return target_preset
 
-    def get_current_slot(self) :
+    def get_current_slot(self) -> Optional[ABSlot]:
         """
         Get currently active slot
 
-        Returns, 'B', or None
+        Returns:
+            'A', 'B', or None
         """
         return self.current_slot
 
-    def is_slot_occupied(self, slot) :
+    def is_slot_occupied(self, slot: ABSlot) -> bool:
         """
         Check if a slot has a snapshot stored
 
         Args:
             slot: 'A' or 'B'
 
         Returns:
             True if slot has a snapshot
         """
         if slot == "A":
             return self.snapshot_a is not None
-        else) :
+        else:
+            return self.snapshot_b is not None
+
+    def get_diff(self) -> Optional[dict]:
         """
         Get differences between A and B snapshots
 
         Returns:
             Dictionary of changes or None if comparison not possible
         """
-        if self.snapshot_a is None or self.snapshot_b is None)
+        if self.snapshot_a is None or self.snapshot_b is None:
+            return None
+
+        return self.snapshot_a.diff(self.snapshot_b)
 
-    def clear_a(self) :
+    def clear_a(self):
         """Clear snapshot A"""
         self.snapshot_a = None
-        if self.current_slot == "A")
+        if self.current_slot == "A":
+            self.current_slot = None
+        print("[ABSnapshot] Cleared A")
 
-    def clear_b(self) :
+    def clear_b(self):
         """Clear snapshot B"""
         self.snapshot_b = None
-        if self.current_slot == "B")
+        if self.current_slot == "B":
+            self.current_slot = None
+        print("[ABSnapshot] Cleared B")
 
-    def clear_all(self) :
+    def clear_all(self):
+        """Clear both snapshots"""
+        self.snapshot_a = None
+        self.snapshot_b = None
+        self.current_slot = None
+        print("[ABSnapshot] Cleared A and B")
+
+    def get_status(self) -> dict:
         """
         Get A/B status information
 
         Returns:
             Dictionary with current state
         """
         return {
-            'slot_a_occupied'),
-            'slot_b_occupied'),
-            'current_slot',
-            'slot_a_name',
-            'slot_b_name',
-            'time_since_last_toggle_ms') - self.last_toggle_time) * 1000 if self.last_toggle_time > 0 else None
+            'slot_a_occupied': self.is_slot_occupied("A"),
+            'slot_b_occupied': self.is_slot_occupied("B"),
+            'current_slot': self.current_slot,
+            'slot_a_name': self.snapshot_a.name if self.snapshot_a else None,
+            'slot_b_name': self.snapshot_b.name if self.snapshot_b else None,
+            'time_since_last_toggle_ms': (time.time() - self.last_toggle_time) * 1000 if self.last_toggle_time > 0 else None
         }
 
 
 # Self-test function
-def _self_test() :
-            logger.info("     coupling_strength, diff['engine.coupling_strength'])
-        logger.info("   ✓ Diff OK")
+def _self_test():
+    """Test ABSnapshot functionality"""
+    print("=" * 60)
+    print("ABSnapshot Self-Test")
+    print("=" * 60)
+
+    try:
+        # Initialize manager
+        print("\n1. Initializing AB manager...")
+        ab = ABSnapshot()
+        print("   ✓ Initialized")
+
+        # Create test presets
+        print("\n2. Creating test presets...")
+        preset_a = Preset(name="Preset A", tags=["test"])
+        preset_a.engine.coupling_strength = 0.5
+
+        preset_b = Preset(name="Preset B", tags=["test"])
+        preset_b.engine.coupling_strength = 1.5
+
+        print("   ✓ Presets created")
+
+        # Store snapshots
+        print("\n3. Storing snapshots...")
+        ab.store_a(preset_a)
+        ab.store_b(preset_b)
+
+        status = ab.get_status()
+        print(f"   Slot A: {status['slot_a_name']} (occupied={status['slot_a_occupied']})")
+        print(f"   Slot B: {status['slot_b_name']} (occupied={status['slot_b_occupied']})")
+        print(f"   Current: {status['current_slot']}")
+        assert status['slot_a_occupied']
+        assert status['slot_b_occupied']
+        print("   ✓ Snapshots stored")
+
+        # Test toggle
+        print("\n4. Testing toggle...")
+        assert ab.get_current_slot() == "B"  # Last store was B
+
+        toggled = ab.toggle()
+        assert toggled is not None
+        assert ab.get_current_slot() == "A"
+        print(f"   Toggled to: {ab.get_current_slot()}")
+
+        time.sleep(0.05)  # Wait for crossfade guard
+
+        toggled = ab.toggle()
+        assert toggled is not None
+        assert ab.get_current_slot() == "B"
+        print(f"   Toggled to: {ab.get_current_slot()}")
+
+        print("   ✓ Toggle OK")
+
+        # Test crossfade guard
+        print("\n5. Testing crossfade guard...")
+        time.sleep(0.05)
+        ab.toggle()  # Should work (enough time passed)
+        toggled = ab.toggle()  # Should return None (too fast)
+        assert toggled is None
+        print("   ✓ Crossfade guard working")
+
+        # Test diff
+        print("\n6. Testing diff...")
+        diff = ab.get_diff()
+        assert diff is not None
+        print(f"   Found {len(diff)} differences")
+        if 'engine.coupling_strength' in diff:
+            print(f"     coupling_strength: {diff['engine.coupling_strength']}")
+        print("   ✓ Diff OK")
 
         # Test clear
-        logger.info("\n7. Testing clear...")
+        print("\n7. Testing clear...")
         ab.clear_a()
         assert not ab.is_slot_occupied("A")
         assert ab.is_slot_occupied("B")
 
         ab.clear_all()
         assert not ab.is_slot_occupied("A")
         assert not ab.is_slot_occupied("B")
-        logger.info("   ✓ Clear OK")
+        print("   ✓ Clear OK")
 
         # Test empty toggle error
-        logger.error("\n8. Testing empty toggle error...")
-        try)
+        print("\n8. Testing empty toggle error...")
+        try:
+            ab.toggle()
             assert False, "Should have raised ValueError"
         except ValueError as e:
-            logger.error("   Caught expected error, e)
-            logger.error("   ✓ Error handling OK")
+            print(f"   Caught expected error: {e}")
+            print("   ✓ Error handling OK")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/adaptive_scaler.py b/server/adaptive_scaler.py
index 48cd547124c0cc572a161397683b8ce08708b966..b6f967f094cbfbd72c22fe40b6a850c9c14e7600 100644
--- a/server/adaptive_scaler.py
+++ b/server/adaptive_scaler.py
@@ -1,384 +1,515 @@
 """
-AdaptiveScaler - Feature 018) and interaction delay (<= 50 ms).
+AdaptiveScaler - Feature 018: Phi-Adaptive Benchmark
+
+Automatically scales visual complexity based on real-time performance metrics
+to maintain target frame rate (>= 30 fps) and interaction delay (<= 50 ms).
 
 Features:
 - FR-003: Auto-adjust visual load based on real-time metrics
 - SC-002: Maintain >= 30 fps continuous
 - SC-005: Auto-scaling response time < 0.5s
 - User Story 1: Adaptive Frame Scheduler
 
-Requirements, GPU <= 75%
+Requirements:
+- Frame-rate drops < 10% under load
+- CPU <= 60%, GPU <= 75%
 - Response time < 0.5s to stabilize fps
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import time
 import threading
 from typing import Optional, Dict, List, Callable
 from dataclasses import dataclass
 from collections import deque
 import numpy as np
 
 
 @dataclass
 class PerformanceMetrics:
     """Current performance metrics"""
     timestamp: float
     fps: float
     cpu_percent: float
     gpu_percent: float
     memory_mb: float
     latency_ms: float
     interaction_delay_ms: float
 
 
 @dataclass
 class VisualComplexity:
     """Visual complexity settings"""
-    level: int  # 0-5, 5=maximum
+    level: int  # 0-5: 0=minimal, 5=maximum
     enable_phi_breathing: bool
     enable_topology_links: bool
     enable_gradients: bool
     particle_count: int
     render_resolution: float  # 0.5-1.0
 
 
 @dataclass
 class ScalerConfig:
     """Configuration for AdaptiveScaler"""
     target_fps: float = 30.0
     min_fps: float = 28.0
     max_fps: float = 60.0
     target_latency_ms: float = 50.0
     max_latency_ms: float = 100.0
     max_cpu_percent: float = 60.0
     max_gpu_percent: float = 75.0
     scaling_response_time_s: float = 0.5
     sample_window_size: int = 30
     enable_logging: bool = False
 
 
 class AdaptiveScaler:
     """
     AdaptiveScaler - Automatic visual complexity adjustment
 
     Monitors real-time performance and adjusts visual settings to maintain
     target frame rate and interaction delay.
 
+    Features:
+    - Continuous performance monitoring
+    - Automatic complexity scaling
+    - Gradual transitions (< 0.5s response time)
     - Resource usage tracking
     """
 
-    def __init__(self, config: Optional[ScalerConfig]) : up
+    def __init__(self, config: Optional[ScalerConfig] = None):
+        """Initialize AdaptiveScaler"""
+        self.config = config or ScalerConfig()
+
+        # Performance history
+        self.metrics_history: deque = deque(maxlen=self.config.sample_window_size)
+
+        # Current visual complexity
+        self.complexity = VisualComplexity(
+            level=5,  # Start at maximum
+            enable_phi_breathing=True,
+            enable_topology_links=True,
+            enable_gradients=True,
+            particle_count=100,
+            render_resolution=1.0
+        )
+
+        # Scaling state
+        self.last_scale_time = time.time()
+        self.scaling_direction = 0  # -1: down, 0: stable, 1: up
         self.consecutive_low_fps = 0
         self.consecutive_high_fps = 0
 
         # Callbacks
         self.complexity_change_callback: Optional[Callable] = None
 
         # Monitoring
         self.is_running = False
-        self.monitor_thread)
-    def add_metrics(self, metrics: PerformanceMetrics) :
+        self.monitor_thread: Optional[threading.Thread] = None
+
+        # Statistics
+        self.scale_up_count = 0
+        self.scale_down_count = 0
+        self.total_adjustments = 0
+
+    def add_metrics(self, metrics: PerformanceMetrics):
         """
         Add performance metrics sample
 
         Args:
-            metrics)
+            metrics: Current performance metrics
+        """
+        self.metrics_history.append(metrics)
 
         # Check if scaling is needed
         self._check_scaling_needed()
 
-    @lru_cache(maxsize=128)
-    def _check_scaling_needed(self) :
+    def _check_scaling_needed(self):
+        """Check if visual complexity should be adjusted"""
+        if len(self.metrics_history) < 5:
+            return
+
+        # Get recent metrics
+        recent_metrics = list(self.metrics_history)[-10:]
+        avg_fps = np.mean([m.fps for m in recent_metrics])
+        avg_latency = np.mean([m.latency_ms for m in recent_metrics])
+        avg_cpu = np.mean([m.cpu_percent for m in recent_metrics])
+        avg_gpu = np.mean([m.gpu_percent for m in recent_metrics])
+
+        # Check if we need to scale down (reduce complexity)
+        should_scale_down = (
+            avg_fps < self.config.min_fps or
+            avg_latency > self.config.max_latency_ms or
+            avg_cpu > self.config.max_cpu_percent or
+            avg_gpu > self.config.max_gpu_percent
+        )
+
+        # Check if we can scale up (increase complexity)
+        should_scale_up = (
+            avg_fps > (self.config.target_fps * 1.5) and
+            avg_latency < (self.config.target_latency_ms * 0.5) and
+            avg_cpu < (self.config.max_cpu_percent * 0.7) and
+            avg_gpu < (self.config.max_gpu_percent * 0.7) and
+            self.complexity.level < 5
+        )
+
+        current_time = time.time()
+        time_since_last_scale = current_time - self.last_scale_time
+
+        # Only scale if enough time has passed (response time < 0.5s)
+        if time_since_last_scale < self.config.scaling_response_time_s:
             return
 
         if should_scale_down:
             self.consecutive_low_fps += 1
             self.consecutive_high_fps = 0
 
             # Scale down after 3 consecutive low readings
-            if self.consecutive_low_fps >= 3, avg_latency, avg_cpu, avg_gpu)
+            if self.consecutive_low_fps >= 3:
+                self._scale_down(avg_fps, avg_latency, avg_cpu, avg_gpu)
                 self.consecutive_low_fps = 0
 
-        elif should_scale_up)
-            if self.consecutive_high_fps >= 5, avg_latency, avg_cpu, avg_gpu)
+        elif should_scale_up:
+            self.consecutive_high_fps += 1
+            self.consecutive_low_fps = 0
+
+            # Scale up after 5 consecutive high readings (be conservative)
+            if self.consecutive_high_fps >= 5:
+                self._scale_up(avg_fps, avg_latency, avg_cpu, avg_gpu)
                 self.consecutive_high_fps = 0
 
-        else, fps: float, latency: float, cpu: float, gpu: float) :
+        else:
+            # Stable performance
+            self.consecutive_low_fps = 0
+            self.consecutive_high_fps = 0
+            self.scaling_direction = 0
+
+    def _scale_down(self, fps: float, latency: float, cpu: float, gpu: float):
         """
         Reduce visual complexity
 
         Args:
             fps: Current average FPS
             latency: Current average latency
             cpu: Current average CPU usage
             gpu: Current average GPU usage
         """
-        if self.complexity.level <= 0)
+        if self.complexity.level <= 0:
+            return
+
+        old_level = self.complexity.level
+        self.complexity.level -= 1
+        self.scaling_direction = -1
+        self.last_scale_time = time.time()
         self.scale_down_count += 1
         self.total_adjustments += 1
 
         # Adjust settings based on level
         self._update_complexity_settings()
 
         if self.config.enable_logging:
-            logger.info("[AdaptiveScaler] Scaled DOWN, old_level, self.complexity.level)
-            logger.info("  Reason, Latency=%sms, CPU=%s%, GPU=%s%", fps, latency, cpu, gpu)
+            print(f"[AdaptiveScaler] Scaled DOWN: {old_level} -> {self.complexity.level}")
+            print(f"  Reason: FPS={fps:.1f}, Latency={latency:.1f}ms, CPU={cpu:.1f}%, GPU={gpu:.1f}%")
 
         # Notify callback
-        if self.complexity_change_callback)
+        if self.complexity_change_callback:
+            self.complexity_change_callback(self.complexity)
 
-    def _scale_up(self, fps: float, latency: float, cpu: float, gpu: float) :
+    def _scale_up(self, fps: float, latency: float, cpu: float, gpu: float):
         """
         Increase visual complexity
 
         Args:
             fps: Current average FPS
             latency: Current average latency
             cpu: Current average CPU usage
             gpu: Current average GPU usage
         """
-        if self.complexity.level >= 5)
+        if self.complexity.level >= 5:
+            return
+
+        old_level = self.complexity.level
+        self.complexity.level += 1
+        self.scaling_direction = 1
+        self.last_scale_time = time.time()
         self.scale_up_count += 1
         self.total_adjustments += 1
 
         # Adjust settings based on level
         self._update_complexity_settings()
 
         if self.config.enable_logging:
-            logger.info("[AdaptiveScaler] Scaled UP, old_level, self.complexity.level)
-            logger.info("  Performance, Latency=%sms, CPU=%s%, GPU=%s%", fps, latency, cpu, gpu)
+            print(f"[AdaptiveScaler] Scaled UP: {old_level} -> {self.complexity.level}")
+            print(f"  Performance: FPS={fps:.1f}, Latency={latency:.1f}ms, CPU={cpu:.1f}%, GPU={gpu:.1f}%")
 
         # Notify callback
-        if self.complexity_change_callback)
+        if self.complexity_change_callback:
+            self.complexity_change_callback(self.complexity)
 
-    def _update_complexity_settings(self) :
+    def _update_complexity_settings(self):
         """Update visual settings based on complexity level"""
         level = self.complexity.level
 
         if level == 0:
             # Minimal - spectral grid only
             self.complexity.enable_phi_breathing = False
             self.complexity.enable_topology_links = False
             self.complexity.enable_gradients = False
             self.complexity.particle_count = 0
             self.complexity.render_resolution = 0.5
 
         elif level == 1:
             # Low - basic visualization
             self.complexity.enable_phi_breathing = False
             self.complexity.enable_topology_links = False
             self.complexity.enable_gradients = True
             self.complexity.particle_count = 20
             self.complexity.render_resolution = 0.6
 
         elif level == 2:
             # Medium-Low - add breathing
             self.complexity.enable_phi_breathing = True
             self.complexity.enable_topology_links = False
             self.complexity.enable_gradients = True
             self.complexity.particle_count = 40
             self.complexity.render_resolution = 0.7
 
         elif level == 3:
             # Medium - add topology
             self.complexity.enable_phi_breathing = True
             self.complexity.enable_topology_links = True
             self.complexity.enable_gradients = True
             self.complexity.particle_count = 60
             self.complexity.render_resolution = 0.8
 
         elif level == 4:
             # High - increased particles
             self.complexity.enable_phi_breathing = True
             self.complexity.enable_topology_links = True
             self.complexity.enable_gradients = True
             self.complexity.particle_count = 80
             self.complexity.render_resolution = 0.9
 
-        else) : Callable) :
+        else:  # level == 5
+            # Maximum - all features
+            self.complexity.enable_phi_breathing = True
+            self.complexity.enable_topology_links = True
+            self.complexity.enable_gradients = True
+            self.complexity.particle_count = 100
+            self.complexity.render_resolution = 1.0
+
+    def get_current_complexity(self) -> VisualComplexity:
+        """Get current visual complexity settings"""
+        return self.complexity
+
+    def set_complexity_callback(self, callback: Callable):
         """
         Set callback for complexity changes
 
         Args:
-            callback) :
+            callback: Function to call when complexity changes
+        """
+        self.complexity_change_callback = callback
+
+    def get_performance_summary(self) -> Dict:
         """
         Get performance summary statistics
 
         Returns:
             Summary dictionary
         """
         if not self.metrics_history:
             return {
-                "avg_fps",
-                "min_fps",
-                "max_fps",
-                "avg_latency_ms",
-                "max_latency_ms",
-                "avg_cpu_percent",
-                "avg_gpu_percent",
-                "current_complexity_level",
-                "scale_up_count",
-                "scale_down_count",
-                "total_adjustments")
+                "avg_fps": 0.0,
+                "min_fps": 0.0,
+                "max_fps": 0.0,
+                "avg_latency_ms": 0.0,
+                "max_latency_ms": 0.0,
+                "avg_cpu_percent": 0.0,
+                "avg_gpu_percent": 0.0,
+                "current_complexity_level": self.complexity.level,
+                "scale_up_count": self.scale_up_count,
+                "scale_down_count": self.scale_down_count,
+                "total_adjustments": self.total_adjustments
+            }
+
+        recent_metrics = list(self.metrics_history)
 
         fps_values = [m.fps for m in recent_metrics]
         latency_values = [m.latency_ms for m in recent_metrics]
         cpu_values = [m.cpu_percent for m in recent_metrics]
         gpu_values = [m.gpu_percent for m in recent_metrics]
 
         return {
-            "avg_fps")),
-            "min_fps")),
-            "max_fps")),
-            "avg_latency_ms")),
-            "max_latency_ms")),
-            "avg_cpu_percent")),
-            "avg_gpu_percent")),
-            "current_complexity_level",
-            "scale_up_count",
-            "scale_down_count",
-            "total_adjustments",
-            "meets_sc002") >= 30.0,
-            "meets_sc005") - self.last_scale_time) < 0.5 if self.total_adjustments > 0 else True
+            "avg_fps": float(np.mean(fps_values)),
+            "min_fps": float(np.min(fps_values)),
+            "max_fps": float(np.max(fps_values)),
+            "avg_latency_ms": float(np.mean(latency_values)),
+            "max_latency_ms": float(np.max(latency_values)),
+            "avg_cpu_percent": float(np.mean(cpu_values)),
+            "avg_gpu_percent": float(np.mean(gpu_values)),
+            "current_complexity_level": self.complexity.level,
+            "scale_up_count": self.scale_up_count,
+            "scale_down_count": self.scale_down_count,
+            "total_adjustments": self.total_adjustments,
+            "meets_sc002": np.mean(fps_values) >= 30.0,
+            "meets_sc005": (time.time() - self.last_scale_time) < 0.5 if self.total_adjustments > 0 else True
         }
 
-    def reset_statistics(self) -> None)
+    def reset_statistics(self):
+        """Reset all statistics"""
+        self.scale_up_count = 0
+        self.scale_down_count = 0
+        self.total_adjustments = 0
+        self.metrics_history.clear()
         self.consecutive_low_fps = 0
         self.consecutive_high_fps = 0
 
 
 # Self-test
-def _self_test() -> None)
-    logger.info("AdaptiveScaler Self-Test")
-    logger.info("=" * 60)
-    logger.info(str())
+def _self_test():
+    """Run basic self-test of AdaptiveScaler"""
+    print("=" * 60)
+    print("AdaptiveScaler Self-Test")
+    print("=" * 60)
+    print()
 
     all_ok = True
 
-    # Test 1)
+    # Test 1: Initialization
+    print("1. Testing Initialization...")
     config = ScalerConfig(
         enable_logging=True,
         target_fps=30.0,
         scaling_response_time_s=0.1  # Faster response for testing
-
+    )
     scaler = AdaptiveScaler(config)
 
     init_ok = scaler.complexity.level == 5
     all_ok = all_ok and init_ok
 
-    logger.info("   Initial complexity level, scaler.complexity.level)
-    logger.error("   [%s] Initialization", 'OK' if init_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Initial complexity level: {scaler.complexity.level}")
+    print(f"   [{'OK' if init_ok else 'FAIL'}] Initialization")
+    print()
 
-    # Test 2)...")
+    # Test 2: Scale down on low FPS
+    print("2. Testing Scale Down (low FPS)...")
 
     # Wait for initial response time to pass
     time.sleep(0.15)
 
     # Add metrics with low FPS (need at least 10 samples for averaging)
-    for i in range(15)),
+    for i in range(15):
+        metrics = PerformanceMetrics(
+            timestamp=time.time(),
             fps=20.0,  # Below target
             cpu_percent=50.0,
             gpu_percent=60.0,
             memory_mb=500.0,
             latency_ms=40.0,
             interaction_delay_ms=30.0
-
+        )
         scaler.add_metrics(metrics)
         time.sleep(0.02)
 
     scale_down_ok = scaler.complexity.level < 5
     all_ok = all_ok and scale_down_ok
 
-    logger.info("   Complexity after low FPS, scaler.complexity.level)
-    logger.error("   [%s] Scale down", 'OK' if scale_down_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Complexity after low FPS: {scaler.complexity.level}")
+    print(f"   [{'OK' if scale_down_ok else 'FAIL'}] Scale down")
+    print()
 
-    # Test 3)...")
+    # Test 3: Scale up on high FPS
+    print("3. Testing Scale Up (high FPS)...")
 
     # Wait for response time to pass
     time.sleep(0.15)
 
     # Add metrics with high FPS (need more samples for conservative scale-up)
-    for i in range(20)),
+    for i in range(20):
+        metrics = PerformanceMetrics(
+            timestamp=time.time(),
             fps=60.0,  # Above target
             cpu_percent=30.0,
             gpu_percent=40.0,
             memory_mb=500.0,
             latency_ms=20.0,
             interaction_delay_ms=15.0
-
+        )
         scaler.add_metrics(metrics)
         time.sleep(0.02)
 
     initial_level = scaler.complexity.level
     scale_up_ok = scaler.scale_up_count > 0
 
     all_ok = all_ok and scale_up_ok
 
-    logger.info("   Complexity after high FPS, scaler.complexity.level)
-    logger.info("   Scale up count, scaler.scale_up_count)
-    logger.error("   [%s] Scale up", 'OK' if scale_up_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Complexity after high FPS: {scaler.complexity.level}")
+    print(f"   Scale up count: {scaler.scale_up_count}")
+    print(f"   [{'OK' if scale_up_ok else 'FAIL'}] Scale up")
+    print()
 
-    # Test 4)
+    # Test 4: Performance summary
+    print("4. Testing Performance Summary...")
     summary = scaler.get_performance_summary()
 
     summary_ok = (
         summary['avg_fps'] > 0 and
         summary['total_adjustments'] > 0
+    )
 
     all_ok = all_ok and summary_ok
 
-    logger.info("   Avg FPS, summary['avg_fps'])
-    logger.info("   Total adjustments, summary['total_adjustments'])
-    logger.error("   [%s] Performance summary", 'OK' if summary_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Avg FPS: {summary['avg_fps']:.1f}")
+    print(f"   Total adjustments: {summary['total_adjustments']}")
+    print(f"   [{'OK' if summary_ok else 'FAIL'}] Performance summary")
+    print()
 
-    # Test 5)
+    # Test 5: Complexity settings
+    print("5. Testing Complexity Settings...")
 
     # Manually set to level 0
     scaler.complexity.level = 0
     scaler._update_complexity_settings()
 
     level0_ok = (
         not scaler.complexity.enable_phi_breathing and
         not scaler.complexity.enable_topology_links and
         scaler.complexity.render_resolution == 0.5
+    )
 
     all_ok = all_ok and level0_ok
 
-    print(f"   Level 0 settings, "
+    print(f"   Level 0 settings: breathing={scaler.complexity.enable_phi_breathing}, "
           f"topology={scaler.complexity.enable_topology_links}, "
           f"resolution={scaler.complexity.render_resolution}")
 
     # Set to level 5
     scaler.complexity.level = 5
     scaler._update_complexity_settings()
 
     level5_ok = (
         scaler.complexity.enable_phi_breathing and
         scaler.complexity.enable_topology_links and
         scaler.complexity.render_resolution == 1.0
+    )
 
     all_ok = all_ok and level5_ok
 
-    print(f"   Level 5 settings, "
+    print(f"   Level 5 settings: breathing={scaler.complexity.enable_phi_breathing}, "
           f"topology={scaler.complexity.enable_topology_links}, "
           f"resolution={scaler.complexity.render_resolution}")
-    logger.error("   [%s] Complexity settings", 'OK' if level0_ok and level5_ok else 'FAIL')
-    logger.info(str())
+    print(f"   [{'OK' if level0_ok and level5_ok else 'FAIL'}] Complexity settings")
+    print()
 
-    logger.info("=" * 60)
-    if all_ok)
-    else)
-    logger.info("=" * 60)
+    print("=" * 60)
+    if all_ok:
+        print("Self-Test PASSED")
+    else:
+        print("Self-Test FAILED - Review failures above")
+    print("=" * 60)
 
     return all_ok
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/api/__pycache__/__init__.cpython-311.pyc b/server/api/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index bc2f19ef9955933decb64c2df71b1516e3cdaf80..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 576
zcmZ`$!Ab)$5Y1Mlva%jMc#ygFP`xiA6crRyO50N@gmDwLp}WbNr0U-M3jGA}8~hWm
z{y?GNp(m4Fg)Zm>X5KJwX2P4-C<=+Q+1GOV9uV^BlV5XrWy@U`UI-&&l9F-2f+6X&
z@Y$8aUjGufWQ7vanlKzqg@FQ-8KxW(B`peR0J97R=fe@KvmsY(o^cb0UBEdLDu$j9
zRhw4X+So1>nj3CS4E+i~j?!(&xwH*h&aD=Sqg~ek?&e&V4N8WV%cP9qss_LWrcxQ}
z`Wi%I9U+FH5C82m+!&!`3<I7h&4f&kiVVwJCqQEse=L-?F`S`AsFOxL$>zp#9m8o2
zoCei6v~GHXIYrGLP`i&^{GdPdy6Ln&+LSVtQ0jrxpVBw7^v7K78LscSo?sy+lnN;<
rrH4hi+CHuia-!Rg=1*_-om*@KL9l2=!RCT^cR+Tcr?!83%ay+Yajv^C

diff --git a/server/audio_engine.py b/server/audio_engine.py
index 9722cc80c4ec766e107664d5d65403ca36db8f7a..292c7f8dea474de625b9672f0922cef2d557859b 100644
--- a/server/audio_engine.py
+++ b/server/audio_engine.py
@@ -1,102 +1,170 @@
 """
 AudioEngine - Compatibility Wrapper for Validation Scripts
 
 This module provides backward compatibility for validation and test scripts
 that expect an AudioEngine class. It wraps the AudioServer class which is
 the actual production implementation.
 
-Created)
-Purpose)
-
+Created: 2025-10-17 (Priority 1 Remediation)
+Purpose: Fix validate_soundlab_v1_final.py import errors
+"""
 
 from .audio_server import AudioServer
 from .chromatic_field_processor import ChromaticFieldProcessor
-from typing import Dict
 import numpy as np
 
 
 class AudioEngine:
+    """
+    AudioEngine wrapper for validation compatibility
+
+    This class wraps AudioServer to provide the interface expected by
+    validation scripts, particularly validate_soundlab_v1_final.py.
 
     The actual production implementation is AudioServer, which includes
     real-time audio I/O, metrics streaming, and latency management.
     This wrapper provides a simplified interface for testing and validation.
     """
 
-    def __init__(self, enable_logging: bool) :
+    def __init__(self, enable_logging: bool = False):
         """
         Initialize AudioEngine
 
         Args:
-            enable_logging: Enable metrics/latency logging (default)
+            enable_logging: Enable metrics/latency logging (default: False for tests)
         """
         # Create underlying AudioServer
         self._audio_server = AudioServer(enable_logging=enable_logging)
 
         # Expose processor for direct access (validation scripts expect this)
         self.processor = self._audio_server.processor
 
         # Configuration
         self.sample_rate = self._audio_server.SAMPLE_RATE
         self.buffer_size = self._audio_server.BUFFER_SIZE
 
-    @lru_cache(maxsize=128)
-    def start(self, calibrate_latency) :
+    def start(self, calibrate_latency: bool = False) -> bool:
         """
         Start audio processing
 
         Args:
             calibrate_latency: Run latency calibration
 
-    @lru_cache(maxsize=128)
-    def stop(self) : dict) :
+        Returns:
+            True if started successfully
+        """
+        return self._audio_server.start(calibrate_latency=calibrate_latency)
+
+    def stop(self):
+        """Stop audio processing"""
+        self._audio_server.stop()
+
+    def apply_preset(self, preset_data: dict):
         """
         Apply preset configuration
 
         Args:
-            preset_data)
+            preset_data: Preset dictionary
+        """
+        self._audio_server.apply_preset(preset_data)
 
-    def update_parameter(self, param_type, channel, param_name, value) :
+    def update_parameter(self, param_type: str, channel: int, param_name: str, value: float) -> bool:
         """
         Update a parameter in real-time
 
         Args:
-            param_type, 'global', 'phi')
-            channel) or None for global
+            param_type: Type of parameter ('channel', 'global', 'phi')
+            channel: Channel index (0-7) or None for global
             param_name: Parameter name
             value: New value
 
-        Returns, channel, param_name, value)
+        Returns:
+            True if successful
+        """
+        return self._audio_server.update_parameter(param_type, channel, param_name, value)
 
-    def get_current_parameters(self) :
+    def get_current_parameters(self) -> dict:
         """
         Get current parameter values
 
-    def get_latest_metrics(self) :
+        Returns:
+            Dictionary with all current parameters
+        """
+        return self._audio_server.get_current_parameters()
+
+    def get_latest_metrics(self):
         """
         Get latest metrics frame
 
-    def get_latest_latency(self) :
+        Returns:
+            Latest MetricsFrame or None
+        """
+        return self._audio_server.get_latest_metrics()
+
+    def get_latest_latency(self):
         """
         Get latest latency frame
 
-    @property
-    @lru_cache(maxsize=128)
-    def is_running(self) : {'coupling_strength')
-        logger.info("   ✓ Preset application OK")
+        Returns:
+            Latest LatencyFrame or None
+        """
+        return self._audio_server.get_latest_latency()
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+    @property
+    def is_running(self) -> bool:
+        """Check if engine is running"""
+        return self._audio_server.is_running
+
+
+# Self-test function
+def _self_test():
+    """Test AudioEngine wrapper"""
+    print("=" * 60)
+    print("AudioEngine Wrapper Self-Test")
+    print("=" * 60)
+
+    try:
+        print("\n1. Testing AudioEngine initialization...")
+        engine = AudioEngine()
+
+        assert engine.sample_rate == 48000
+        assert engine.buffer_size == 512
+        assert hasattr(engine, 'processor')
+        assert isinstance(engine.processor, ChromaticFieldProcessor)
+        assert not engine.is_running
+
+        print("   ✓ Initialization OK")
+        print(f"   ✓ Processor type: {type(engine.processor).__name__}")
+
+        print("\n2. Testing processor access...")
+        # Test that we can access processor methods
+        assert hasattr(engine.processor, 'processBlock')
+        print("   ✓ processor.processBlock() available")
+
+        print("\n3. Testing preset application...")
+        test_preset = {
+            'name': 'Test Preset',
+            'engine': {'coupling_strength': 1.5}
+        }
+        engine.apply_preset(test_preset)
+        print("   ✓ Preset application OK")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
 
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    success = _self_test()
 
-    if not success)
+    if not success:
+        import sys
+        sys.exit(1)
diff --git a/server/audio_server.py b/server/audio_server.py
index 3c1a075085bbf2a870594903b3c93028b90b7be9..11ac991bcd9fe923bd801439046c728d6a3d19ff 100644
--- a/server/audio_server.py
+++ b/server/audio_server.py
@@ -1,605 +1,726 @@
 """
 AudioServer - Real-Time Audio Processing Pipeline
 
-Integrates all components)
+Integrates all components:
+- sounddevice audio I/O
+- ChromaticFieldProcessor (D-ASE engine)
 - Φ-Modulation
 - 8-channel → stereo downmix
 - Latency compensation
 - Metrics generation and streaming
 - Synchronized timestamping
 
 Implements FR-001, FR-002, FR-003, FR-004, FR-005, FR-008
 """
 
 import numpy as np
 import sounddevice as sd
 import time
 import threading
 from queue import Queue, Empty
 from typing import Optional, Callable, Dict
 import traceback
 
 from .chromatic_field_processor import ChromaticFieldProcessor
 from .phi_modulator_controller import PhiModulatorController
 from .downmix import StereoDownmixer
 from .latency_manager import LatencyManager
 from .metrics_frame import MetricsFrame, create_default_metrics_frame
 from .latency_frame import LatencyFrame
 from .metrics_logger import MetricsLogger
 from .latency_logger import LatencyLogger
 
 
 class AudioServer:
     """
     Complete real-time audio processing server
 
+    Features:
+    - 48kHz @ 512 samples audio processing
+    - <10ms end-to-end latency
+    - Real-time metrics at ≥30Hz
+    - Latency compensation and drift correction
+    - Thread-safe component integration
+    """
+
+    # Audio configuration (from FR-001)
     SAMPLE_RATE = 48000
     BUFFER_SIZE = 512
     INPUT_CHANNELS = 1  # Mono input
     OUTPUT_CHANNELS = 2  # Stereo output
 
     # Performance targets
     TARGET_LATENCY_MS = 10.0
     TARGET_METRICS_FPS = 30
 
     def __init__(self,
-                 input_device,
-                 output_device,
-                 enable_logging):
+                 input_device: Optional[int] = None,
+                 output_device: Optional[int] = None,
+                 enable_logging: bool = True):
         """
         Initialize audio server
 
         Args:
-            input_device)
-            output_device)
-            enable_logging)
+            input_device: Input device index (None = default)
+            output_device: Output device index (None = default)
+            enable_logging: Enable metrics/latency logging
+        """
+        print("=" * 60)
         print("AudioServer Initialization")
         print("=" * 60)
 
         self.input_device = input_device
         self.output_device = output_device
         self.enable_logging = enable_logging
 
         # Audio stream
-        self.stream)
+        self.stream: Optional[sd.Stream] = None
+        self.is_running = False
+
+        # Processing components
+        print("\n[AudioServer] Initializing processing components...")
 
         self.processor = ChromaticFieldProcessor(
             sample_rate=self.SAMPLE_RATE,
             buffer_size=self.BUFFER_SIZE
+        )
 
         self.phi_controller = PhiModulatorController(
             sample_rate=self.SAMPLE_RATE
+        )
 
         self.downmixer = StereoDownmixer(
             num_channels=8,
             strategy="spatial"
+        )
 
         self.latency_manager = LatencyManager(
             sample_rate=self.SAMPLE_RATE,
             buffer_size=self.BUFFER_SIZE,
             input_device=input_device,
             output_device=output_device
+        )
 
         # Logging
         self.metrics_logger: Optional[MetricsLogger] = None
         self.latency_logger: Optional[LatencyLogger] = None
 
-        if enable_logging)
+        if enable_logging:
+            print("\n[AudioServer] Initializing loggers...")
             self.metrics_logger = MetricsLogger()
             self.latency_logger = LatencyLogger()
 
         # Metrics/latency frame queues (for API/WebSocket consumption)
         self.metrics_queue = Queue(maxsize=2)  # Double buffer
         self.latency_queue = Queue(maxsize=2)
 
         # Callbacks for external consumers (e.g., WebSocket streamers)
-        self.metrics_callback, None]] = None
-        self.latency_callback, None]] = None
+        self.metrics_callback: Optional[Callable[[MetricsFrame], None]] = None
+        self.latency_callback: Optional[Callable[[LatencyFrame], None]] = None
 
         # Performance tracking
         self.callback_count = 0
         self.last_metrics_time = 0.0
         self.last_latency_update = 0.0
         self.processing_time_history = []
 
         # State
-        self.current_preset)
+        self.current_preset: Optional[Dict] = None
+
+        print("\n[AudioServer] ✓ Initialization complete")
         print("=" * 60)
 
-    def _audio_callback(self, indata, outdata,
-                       frames, time_info, status))
+    def _audio_callback(self, indata: np.ndarray, outdata: np.ndarray,
+                       frames: int, time_info, status):
+        """
+        Audio callback - processes audio in real-time
+
+        This is the critical path - must complete within buffer duration (~10.7ms)
 
         Args:
-            indata, 1)
-            outdata, 2)
-            frames)
+            indata: Input audio (frames, 1)
+            outdata: Output audio (frames, 2)
+            frames: Number of frames (should be BUFFER_SIZE)
             time_info: Timing information
-            status)
+            status: Stream status
+        """
+        callback_start = time.perf_counter()
 
         try:
             # Check for buffer issues
             if status:
-                print(f"[AudioServer] Stream status)
+                print(f"[AudioServer] Stream status: {status}")
 
             # Get monotonic timestamp for synchronization
             callback_time = time.time()
 
             # Extract mono input
             input_mono = indata[:, 0] if indata.shape[1] > 0 else np.zeros(frames)
 
             # --- Update Φ-Modulation ---
             # Pass audio block for envelope following
             phi_state = self.phi_controller.update(audio_block=input_mono)
 
             # --- Process through D-ASE Engine ---
             # Returns 8-channel output + metrics
             multi_channel, metrics_dict = self.processor.processBlock(
                 input_mono,
                 phi_phase=phi_state.phase,
                 phi_depth=phi_state.depth
+            )
 
             # --- Downmix 8-channel → Stereo ---
             stereo = self.downmixer.downmix(multi_channel)
 
             # --- Apply Latency Compensation ---
             compensated = self.latency_manager.compensate_block(stereo)
 
             # --- Update Latency Timing ---
             self.latency_manager.update_timing(callback_time)
 
             # --- Write to Output ---
-            outdata[,
+            outdata[:] = compensated
+
+            # --- Generate Metrics Frame ---
+            metrics_frame = self._create_metrics_frame(
+                metrics_dict,
                 phi_state,
                 callback_time
+            )
 
             # --- Generate Latency Frame ---
             latency_frame = self.latency_manager.get_current_frame()
             latency_frame.timestamp = callback_time
 
             # Calculate actual processing time
             processing_time_ms = (time.perf_counter() - callback_start) * 1000.0
             self.processing_time_history.append(processing_time_ms)
-            if len(self.processing_time_history) > 100)
+            if len(self.processing_time_history) > 100:
+                self.processing_time_history.pop(0)
 
             # Update CPU load estimate
             buffer_duration_ms = (self.BUFFER_SIZE / self.SAMPLE_RATE) * 1000.0
             cpu_load = processing_time_ms / buffer_duration_ms
             latency_frame.cpu_load = cpu_load
 
             # --- Publish Metrics (at target rate) ---
             time_since_metrics = callback_time - self.last_metrics_time
             metrics_interval = 1.0 / self.TARGET_METRICS_FPS
 
-            if time_since_metrics >= metrics_interval)
-                try)
+            if time_since_metrics >= metrics_interval:
+                # Try to enqueue metrics (non-blocking)
+                try:
+                    self.metrics_queue.put_nowait(metrics_frame)
 
                     # Call external callback if set
-                    if self.metrics_callback)
+                    if self.metrics_callback:
+                        self.metrics_callback(metrics_frame)
 
                     # Log to file
-                    if self.metrics_logger)
+                    if self.metrics_logger:
+                        self.metrics_logger.log_frame(metrics_frame)
 
                     self.last_metrics_time = callback_time
 
-                except, skip this frame
+                except:
+                    pass  # Queue full, skip this frame
 
             # --- Publish Latency Updates (every 100ms) ---
             time_since_latency = callback_time - self.last_latency_update
 
             if time_since_latency >= 0.1:  # 10 Hz
-                try)
+                try:
+                    self.latency_queue.put_nowait(latency_frame)
 
                     # Call external callback if set
-                    if self.latency_callback)
+                    if self.latency_callback:
+                        self.latency_callback(latency_frame)
 
                     # Log to file
-                    if self.latency_logger)
+                    if self.latency_logger:
+                        self.latency_logger.log_frame(latency_frame)
 
                     self.last_latency_update = callback_time
 
-                except, skip this frame
+                except:
+                    pass  # Queue full, skip this frame
 
             self.callback_count += 1
 
             # Warn if processing time exceeds threshold (80% of buffer duration)
             if processing_time_ms > buffer_duration_ms * 0.8:
-                print(f"[AudioServer] WARNING: High CPU load: {processing_time_ms:.2f} ms / {buffer_duration_ms:.2f} ms ({cpu_load*100)")
+                print(f"[AudioServer] WARNING: High CPU load: {processing_time_ms:.2f} ms / {buffer_duration_ms:.2f} ms ({cpu_load*100:.1f}%)")
 
         except Exception as e:
-            print(f"[AudioServer] ERROR in audio callback)
+            print(f"[AudioServer] ERROR in audio callback: {e}")
             traceback.print_exc()
             # Fill output with silence on error
             outdata.fill(0)
 
-    def _create_metrics_frame(self, metrics_dict, phi_state, timestamp) :
+    def _create_metrics_frame(self, metrics_dict: Dict, phi_state, timestamp: float) -> MetricsFrame:
         """
         Create MetricsFrame from engine metrics and Φ state
 
         Args:
             metrics_dict: Metrics from ChromaticFieldProcessor
             phi_state: Current PhiState
             timestamp: Monotonic timestamp
 
-        Returns,
+        Returns:
+            Complete MetricsFrame
+        """
+        frame = MetricsFrame(
+            timestamp=timestamp,
             ici=metrics_dict.get('ici', 0.0),
             phase_coherence=metrics_dict.get('phase_coherence', 0.0),
             spectral_centroid=metrics_dict.get('spectral_centroid', 0.0),
             criticality=metrics_dict.get('criticality', 0.0),
             consciousness_level=metrics_dict.get('consciousness_level', 0.0),
             state='IDLE',  # Will be classified by classify_state()
             phi_phase=phi_state.phase,
             phi_depth=phi_state.depth,
             phi_mode=phi_state.mode
+        )
 
         # Classify state based on metrics
         frame.state = frame.classify_state()
 
         return frame
 
-    def start(self, calibrate_latency) :
+    def start(self, calibrate_latency: bool = False) -> bool:
         """
         Start audio server
 
         Args:
             calibrate_latency: Run latency calibration before starting
 
         Returns:
             True if started successfully
         """
-        if self.is_running)
+        if self.is_running:
+            print("[AudioServer] Already running")
             return True
 
         print("\n[AudioServer] Starting audio server...")
 
         # Optional latency calibration
-        if calibrate_latency)
+        if calibrate_latency:
+            print("\n[AudioServer] Running latency calibration...")
             print("[AudioServer] Ensure audio loopback is connected!")
 
             success = self.latency_manager.calibrate()
 
             if not success:
-                print("[AudioServer] WARNING, continuing with default latency estimates")
-            else)
+                print("[AudioServer] WARNING: Calibration failed, continuing with default latency estimates")
+            else:
+                print("[AudioServer] ✓ Calibration complete")
 
                 # Log calibration event
-                if self.latency_logger)
+                if self.latency_logger:
+                    latency_frame = self.latency_manager.get_current_frame()
                     self.latency_logger.log_calibration_event(True, {
-                        'total_latency_ms',
-                        'quality')
+                        'total_latency_ms': latency_frame.total_measured_ms,
+                        'quality': latency_frame.calibration_quality
+                    })
 
-        try)
-            print(f"[AudioServer]   Sample rate)
-            print(f"[AudioServer]   Buffer size)
-            print(f"[AudioServer]   Input device)
-            print(f"[AudioServer]   Output device)
+        try:
+            # Create audio stream
+            print(f"\n[AudioServer] Opening audio stream...")
+            print(f"[AudioServer]   Sample rate: {self.SAMPLE_RATE} Hz")
+            print(f"[AudioServer]   Buffer size: {self.BUFFER_SIZE} samples")
+            print(f"[AudioServer]   Input device: {self.input_device or 'default'}")
+            print(f"[AudioServer]   Output device: {self.output_device or 'default'}")
 
             self.stream = sd.Stream(
                 samplerate=self.SAMPLE_RATE,
                 blocksize=self.BUFFER_SIZE,
                 channels=(self.INPUT_CHANNELS, self.OUTPUT_CHANNELS),
                 dtype=np.float32,
                 callback=self._audio_callback,
                 device=(self.input_device, self.output_device),
                 latency='low'
+            )
 
             # Start stream
             self.stream.start()
             self.is_running = True
 
             # Reset performance counters
             self.callback_count = 0
             self.last_metrics_time = time.time()
             self.last_latency_update = time.time()
             self.processing_time_history = []
 
             print("[AudioServer] ✓ Audio stream started")
             print("[AudioServer] Processing audio in real-time...")
 
             return True
 
         except Exception as e:
-            print(f"[AudioServer] ✗ Failed to start audio stream)
+            print(f"[AudioServer] ✗ Failed to start audio stream: {e}")
             traceback.print_exc()
             return False
 
     def stop(self):
         """Stop audio server"""
-        if not self.is_running)
+        if not self.is_running:
+            print("[AudioServer] Not running")
             return
 
         print("\n[AudioServer] Stopping audio server...")
 
         try:
             # Stop stream
-            if self.stream)
+            if self.stream:
+                self.stream.stop()
                 self.stream.close()
                 self.stream = None
 
             self.is_running = False
 
             # Close loggers
-            if self.metrics_logger)
+            if self.metrics_logger:
+                self.metrics_logger.close()
 
-            if self.latency_logger)
+            if self.latency_logger:
+                self.latency_logger.close()
 
             print("[AudioServer] ✓ Audio server stopped")
 
             # Print statistics
             self._print_statistics()
 
         except Exception as e:
-            print(f"[AudioServer] Error stopping)
+            print(f"[AudioServer] Error stopping: {e}")
             traceback.print_exc()
 
-    def _print_statistics(self))
+    def _print_statistics(self):
+        """Print session statistics"""
+        print("\n" + "=" * 60)
         print("Audio Server Statistics")
         print("=" * 60)
 
-        print(f"\nAudio Processing)
-        print(f"  Callbacks)
-        print(f"  Sample rate)
-        print(f"  Buffer size)
+        print(f"\nAudio Processing:")
+        print(f"  Callbacks: {self.callback_count}")
+        print(f"  Sample rate: {self.SAMPLE_RATE} Hz")
+        print(f"  Buffer size: {self.BUFFER_SIZE} samples")
 
-        if self.processing_time_history)
+        if self.processing_time_history:
+            avg_time = np.mean(self.processing_time_history)
             max_time = np.max(self.processing_time_history)
             buffer_duration = (self.BUFFER_SIZE / self.SAMPLE_RATE) * 1000.0
 
-            print(f"\nProcessing Performance)
-            print(f"  Average: {avg_time:.2f} ms / {buffer_duration:.2f} ms ({avg_time/buffer_duration*100)")
-            print(f"  Peak: {max_time:.2f} ms ({max_time/buffer_duration*100)")
+            print(f"\nProcessing Performance:")
+            print(f"  Average: {avg_time:.2f} ms / {buffer_duration:.2f} ms ({avg_time/buffer_duration*100:.1f}%)")
+            print(f"  Peak: {max_time:.2f} ms ({max_time/buffer_duration*100:.1f}%)")
 
         # Latency statistics
         latency_stats = self.latency_manager.get_statistics()
-        print(f"\nLatency)
-        print(f"  Calibrated)
-        print(f"  Total measured: {latency_stats['latency']['total_measured_ms'])
-        print(f"  Effective: {latency_stats['effective_latency_ms'])
-        print(f"  Aligned)
+        print(f"\nLatency:")
+        print(f"  Calibrated: {latency_stats['calibrated']}")
+        print(f"  Total measured: {latency_stats['latency']['total_measured_ms']:.2f} ms")
+        print(f"  Effective: {latency_stats['effective_latency_ms']:.2f} ms")
+        print(f"  Aligned: {latency_stats['aligned']}")
 
         # Drift statistics
         drift_stats = latency_stats['drift']
-        print(f"\nDrift)
-        print(f"  Current: {drift_stats['current_drift_ms'])
-        print(f"  Rate: {drift_stats['drift_rate_ms_per_sec'])
-        print(f"  Cumulative: {drift_stats['cumulative_drift_ms'])
+        print(f"\nDrift:")
+        print(f"  Current: {drift_stats['current_drift_ms']:.3f} ms")
+        print(f"  Rate: {drift_stats['drift_rate_ms_per_sec']:.4f} ms/s")
+        print(f"  Cumulative: {drift_stats['cumulative_drift_ms']:.3f} ms")
 
         # Logging statistics
-        if self.metrics_logger)
-            print(f"\nMetrics Logging)
-            print(f"  Frames)
-            print(f"  Average FPS: {metrics_stats['average_fps'])
-            print(f"  Gaps)
-
-        if self.latency_logger)
-            print(f"\nLatency Logging)
-            print(f"  Frames)
-            print(f"  Average FPS: {latency_log_stats['average_fps'])
+        if self.metrics_logger:
+            metrics_stats = self.metrics_logger.get_session_statistics()
+            print(f"\nMetrics Logging:")
+            print(f"  Frames: {metrics_stats['frame_count']}")
+            print(f"  Average FPS: {metrics_stats['average_fps']:.1f}")
+            print(f"  Gaps: {metrics_stats['gap_count']}")
+
+        if self.latency_logger:
+            latency_log_stats = self.latency_logger.get_session_statistics()
+            print(f"\nLatency Logging:")
+            print(f"  Frames: {latency_log_stats['frame_count']}")
+            print(f"  Average FPS: {latency_log_stats['average_fps']:.1f}")
 
         print("=" * 60)
 
-    def apply_preset(self, preset_data):
+    def apply_preset(self, preset_data: Dict):
         """
         Apply preset configuration
 
         Args:
             preset_data: Preset dictionary
         """
-        print(f"[AudioServer] Applying preset, 'Unknown')}")
+        print(f"[AudioServer] Applying preset: {preset_data.get('name', 'Unknown')}")
 
         try:
             # Update engine parameters
             if 'engine' in preset_data:
                 engine = preset_data['engine']
 
                 # Update frequencies and amplitudes
-                if 'frequencies' in engine, dtype=np.float64)
+                if 'frequencies' in engine:
+                    self.processor.frequencies = np.array(engine['frequencies'], dtype=np.float64)
 
-                if 'amplitudes' in engine, dtype=np.float64)
+                if 'amplitudes' in engine:
+                    self.processor.amplitudes = np.array(engine['amplitudes'], dtype=np.float64)
 
                 if 'coupling_strength' in engine:
                     self.processor.coupling_strength = engine['coupling_strength']
 
             # Update Φ parameters
             if 'phi' in preset_data:
                 phi = preset_data['phi']
 
-                if 'mode' in phi)
+                if 'mode' in phi:
+                    self.phi_controller.set_mode(phi['mode'])
 
-                if 'depth' in phi) == 'manual')
+                if 'depth' in phi:
+                    # Set manual depth if in manual mode
+                    if self.phi_controller.get_current_mode() == 'manual':
+                        self.phi_controller.sources['manual'].set_depth(phi['depth'])
 
-                if 'phase' in phi) == 'manual')
+                if 'phase' in phi:
+                    if self.phi_controller.get_current_mode() == 'manual':
+                        self.phi_controller.sources['manual'].set_phase(phi['phase'])
 
             # Update downmix parameters
             if 'downmix' in preset_data:
                 downmix = preset_data['downmix']
 
-                if 'strategy' in downmix)
+                if 'strategy' in downmix:
+                    self.downmixer.set_strategy(downmix['strategy'])
 
-                if 'weights_l' in downmix, np.array(downmix['weights_l']))
+                if 'weights_l' in downmix:
+                    self.downmixer.set_weights('L', np.array(downmix['weights_l']))
 
-                if 'weights_r' in downmix, np.array(downmix['weights_r']))
+                if 'weights_r' in downmix:
+                    self.downmixer.set_weights('R', np.array(downmix['weights_r']))
 
             self.current_preset = preset_data
             print("[AudioServer] ✓ Preset applied")
 
         except Exception as e:
-            print(f"[AudioServer] ERROR applying preset)
+            print(f"[AudioServer] ERROR applying preset: {e}")
             traceback.print_exc()
 
-    def get_latest_metrics(self) :
+    def get_latest_metrics(self) -> Optional[MetricsFrame]:
+        """
+        Get latest metrics frame (non-blocking)
+
+        Returns:
             Latest MetricsFrame or None
         """
-        try)
-        except Empty) :
+        try:
+            return self.metrics_queue.get_nowait()
+        except Empty:
+            return None
+
+    def get_latest_latency(self) -> Optional[LatencyFrame]:
+        """
+        Get latest latency frame (non-blocking)
+
+        Returns:
             Latest LatencyFrame or None
         """
-        try)
-        except Empty, param_type, channel, param_name, value) :
+        try:
+            return self.latency_queue.get_nowait()
+        except Empty:
+            return None
+
+    def update_parameter(self, param_type: str, channel: Optional[int], param_name: str, value: float) -> bool:
         """
         Update a single parameter in real-time
 
         Args:
-            param_type, 'global', 'phi')
-            channel) for channel parameters, None for global
-            param_name, 'amplitude', 'coupling_strength', etc.)
+            param_type: Type of parameter ('channel', 'global', 'phi')
+            channel: Channel index (0-7) for channel parameters, None for global
+            param_name: Parameter name ('frequency', 'amplitude', 'coupling_strength', etc.)
             value: New value
 
         Returns:
             True if parameter was updated successfully
         """
         try:
             if param_type == 'channel' and channel is not None:
                 if channel < 0 or channel >= 8:
-                    print(f"[AudioServer] Invalid channel)
+                    print(f"[AudioServer] Invalid channel: {channel}")
                     return False
 
-                if param_name == 'frequency')
+                if param_name == 'frequency':
+                    self.processor.frequencies[channel] = float(value)
                     return True
 
-                elif param_name == 'amplitude')
+                elif param_name == 'amplitude':
+                    self.processor.amplitudes[channel] = float(value)
                     return True
 
-                elif param_name == 'enabled')
+                elif param_name == 'enabled':
+                    # Store enabled state (multiply amplitude by 0 if disabled)
                     # We'll need to track original amplitude separately
-                    if not hasattr(self, '_original_amplitudes'))
+                    if not hasattr(self, '_original_amplitudes'):
+                        self._original_amplitudes = self.processor.amplitudes.copy()
 
                     if bool(value):
                         # Enable: restore original amplitude
                         self.processor.amplitudes[channel] = self._original_amplitudes[channel]
                     else:
                         # Disable: save original and set to 0
                         self._original_amplitudes[channel] = self.processor.amplitudes[channel]
                         self.processor.amplitudes[channel] = 0.0
                     return True
 
             elif param_type == 'global':
-                if param_name == 'coupling_strength')
+                if param_name == 'coupling_strength':
+                    self.processor.coupling_strength = float(value)
                     return True
 
-                elif param_name == 'gain')
-                    if not hasattr(self.downmixer, 'gain'))
+                elif param_name == 'gain':
+                    # Overall output gain (applied to downmixer)
+                    if not hasattr(self.downmixer, 'gain'):
+                        self.downmixer.gain = 1.0
+                    self.downmixer.gain = float(value)
                     return True
 
             elif param_type == 'phi':
                 if param_name == 'phase':
                     # Set phase for manual mode
-                    if 'manual' in self.phi_controller.sources))
+                    if 'manual' in self.phi_controller.sources:
+                        self.phi_controller.sources['manual'].set_phase(float(value))
                         return True
 
                 elif param_name == 'depth':
                     # Set depth for manual mode
-                    if 'manual' in self.phi_controller.sources))
+                    if 'manual' in self.phi_controller.sources:
+                        self.phi_controller.sources['manual'].set_depth(float(value))
                         return True
 
-                elif param_name == 'mode'))
+                elif param_name == 'mode':
+                    # Switch phi modulation mode
+                    self.phi_controller.set_mode(str(value))
                     return True
 
-            print(f"[AudioServer] Unknown parameter)
+            print(f"[AudioServer] Unknown parameter: {param_type}.{param_name}")
             return False
 
         except Exception as e:
-            print(f"[AudioServer] Error updating parameter)
+            print(f"[AudioServer] Error updating parameter: {e}")
             traceback.print_exc()
             return False
 
-    def get_current_parameters(self) :
+    def get_current_parameters(self) -> Dict:
         """
         Get current parameter values for all channels
 
         Returns:
             Dictionary with all current parameter values
         """
         return {
             'channels': [
                 {
-                    'index',
-                    'frequency'),
-                    'amplitude'),
-                    'enabled') > 0.001
+                    'index': i,
+                    'frequency': float(self.processor.frequencies[i]),
+                    'amplitude': float(self.processor.amplitudes[i]),
+                    'enabled': float(self.processor.amplitudes[i]) > 0.001
                 }
                 for i in range(8)
             ],
             'global': {
-                'coupling_strength'),
-                'gain', 'gain', 1.0)
+                'coupling_strength': float(self.processor.coupling_strength),
+                'gain': getattr(self.downmixer, 'gain', 1.0)
             },
             'phi': {
-                'phase').phase,
-                'depth').depth,
-                'mode')
+                'phase': self.phi_controller.get_current_state().phase,
+                'depth': self.phi_controller.get_current_state().depth,
+                'mode': self.phi_controller.get_current_mode()
             }
         }
 
 
 # Self-test function
-def _self_test())
+def _self_test():
+    """Test AudioServer initialization"""
+    print("=" * 60)
     print("AudioServer Self-Test")
     print("=" * 60)
 
-    try)
+    try:
+        # Test initialization
+        print("\n1. Testing AudioServer initialization...")
         server = AudioServer(enable_logging=False)
 
         assert server.SAMPLE_RATE == 48000
         assert server.BUFFER_SIZE == 512
         assert not server.is_running
 
         print("   ✓ Initialization OK")
 
         # Test preset application (without running)
         print("\n2. Testing preset application...")
         test_preset = {
-            'name',
+            'name': 'Test Preset',
             'engine': {
-                'coupling_strength',
+                'coupling_strength': 1.5
+            },
             'phi': {
-                'mode',
-                'depth',
+                'mode': 'manual',
+                'depth': 0.618
+            },
             'downmix': {
-                'strategy')
+                'strategy': 'energy'
+            }
+        }
+
+        server.apply_preset(test_preset)
         print("   ✓ Preset application OK")
 
         print("\n" + "=" * 60)
         print("Self-Test PASSED ✓")
         print("=" * 60)
-        print("\nNote)
-        print("To test audio processing)
+        print("\nNote: Full audio test requires audio hardware")
+        print("To test audio processing:")
         print("  1. Ensure audio input/output devices are available")
-        print("  2. Run)
+        print("  2. Run: python audio_server.py")
         print("  3. Follow interactive prompts")
 
         return True
 
     except Exception as e:
-        print(f"\n✗ Self-Test FAILED)
+        print(f"\n✗ Self-Test FAILED: {e}")
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    # Run self-test
+    success = _self_test()
 
-    if success)
+    if success:
+        print("\n" + "=" * 60)
         print("Interactive Audio Server Test")
         print("=" * 60)
 
-        response = input("\nRun interactive audio test? (requires audio I/O) [y/N])
+        response = input("\nRun interactive audio test? (requires audio I/O) [y/N]: ")
 
-        if response.lower() == 'y')
+        if response.lower() == 'y':
+            print("\n[Test] Creating audio server...")
             server = AudioServer(enable_logging=True)
 
-            calibrate = input("\nRun latency calibration? (requires loopback) [y/N])
+            calibrate = input("\nRun latency calibration? (requires loopback) [y/N]: ")
             should_calibrate = calibrate.lower() == 'y'
 
             print("\n[Test] Starting audio server...")
-            if server.start(calibrate_latency=should_calibrate))
+            if server.start(calibrate_latency=should_calibrate):
+                print("\n✓ Audio server running!")
                 print("\nSpeak into microphone or play audio...")
                 print("Press Enter to stop")
 
                 input()
 
                 print("\n[Test] Stopping audio server...")
                 server.stop()
 
                 print("\n✓ Test complete")
-            else)
-
-"""  # auto-closed missing docstring
+            else:
+                print("\n✗ Failed to start audio server")
diff --git a/server/auto_phi.py b/server/auto_phi.py
index f7f31cc2d79a7bf699e16f43ea8923d3b0994d98..bd41c2eeaf78ad6f35d5046c8fded1322e2f8843 100644
--- a/server/auto_phi.py
+++ b/server/auto_phi.py
@@ -1,59 +1,64 @@
 """
 Auto-Φ Learner - Adaptive Modulation Control
 Feature 011: Automatically adjusts Φ-depth and Φ-phase to maintain near-critical balance
 
 Implements:
 - FR-001: AutoPhiLearner class
 - FR-002: Metrics stream subscription
 - FR-003: Adaptive control law
-
+- FR-004: Tunable parameters (k, γ)
 - FR-005: WebSocket parameter updates
 - FR-006: Enable/disable toggle
 - FR-007: Performance logging
 
-Control Law) × Δt
+Control Law:
+  phi_depth ← phi_depth + k × (1.0 – criticality) × Δt
   phi_phase ← phi_phase + γ × d(coherence)/dt
 
 Success Criteria:
 - SC-001: Criticality within ±0.05 for >90% of runtime
 - SC-002: Reaction time ≤ 5s to disturbances
 - SC-003: CPU load < 5%
+- SC-004: Toggle changes state immediately
+"""
 
 import asyncio
 import time
 import numpy as np
 from typing import Optional, Callable, Dict, List
 from dataclasses import dataclass
 import json
 from collections import deque
 
 
 @dataclass
 class AutoPhiConfig:
     """Configuration for Auto-Φ Learner"""
-    enabled)
+    enabled: bool = False
+
+    # Control gains (FR-004)
     k_depth: float = 0.25      # Depth control gain
     gamma_phase: float = 0.1   # Phase control gain
 
     # Target setpoint
     target_criticality: float = 1.0
 
     # Update rate
     update_interval: float = 0.1  # 10 Hz
 
     # Smoothing
     smoothing_window: int = 30  # 30 samples @ 10Hz = 3s
 
     # Safety limits
     depth_min: float = 0.0
     depth_max: float = 1.0
     phase_min: float = 0.0
     phase_max: float = 1.0
 
     # Disturbance detection
     disturbance_threshold: float = 0.15  # 15% change
 
     # Logging
     enable_logging: bool = True
     log_interval: float = 1.0  # Log stats every 1s
 
@@ -67,298 +72,437 @@ class AutoPhiState:
 
     # Current metrics
     criticality: float = 0.0
     coherence: float = 0.0
 
     # Control state
     criticality_error: float = 0.0
     coherence_derivative: float = 0.0
 
     # Performance tracking
     in_range_count: int = 0
     total_count: int = 0
     last_disturbance_time: float = 0.0
     settled: bool = True
 
     # Timing
     last_update: float = 0.0
 
 
 class AutoPhiLearner:
     """
     Adaptive Φ-modulation controller
 
     Automatically adjusts phi_depth and phi_phase to maintain system criticality near 1.0
 
-    Features, config: Optional[AutoPhiConfig]) :
+    Features:
+    - Feedback control based on criticality and phase coherence
+    - Smooth adaptation with configurable gains
+    - Disturbance detection and recovery
+    - Performance logging
+    - Enable/disable toggle
+    """
+
+    def __init__(self, config: Optional[AutoPhiConfig] = None):
         """
         Initialize Auto-Φ Learner
 
         Args:
-            config)
+            config: AutoPhiConfig (uses defaults if None)
         """
         self.config = config or AutoPhiConfig()
         self.state = AutoPhiState()
 
         # Metrics history for smoothing (FR-002)
         self.criticality_history = deque(maxlen=self.config.smoothing_window)
         self.coherence_history = deque(maxlen=self.config.smoothing_window)
 
         # Performance tracking (FR-007)
         self.criticality_log: List[float] = []
         self.depth_log: List[float] = []
         self.phase_log: List[float] = []
-        self.timestamps)
-        self.update_callback, float], None]] = None
+        self.timestamps: List[float] = []
 
-        # External bias (for Feature 013)
+        # Last logged time
+        self.last_log_time = 0.0
+
+        # Callback for parameter updates (FR-005)
+        self.update_callback: Optional[Callable[[str, float], None]] = None
+
+        # External bias (for Feature 013: State Memory integration)
         self.external_bias: float = 0.0
 
         # Task control
         self.running = False
-        self.task)
-        logger.info("[AutoPhiLearner]   k_depth=%s, gamma_phase=%s", self.config.k_depth, self.config.gamma_phase)
-        logger.error("[AutoPhiLearner]   target_criticality=%s", self.config.target_criticality)
-        logger.info("[AutoPhiLearner]   enabled=%s", self.config.enabled)
+        self.task: Optional[asyncio.Task] = None
+
+        print("[AutoPhiLearner] Initialized")
+        print(f"[AutoPhiLearner]   k_depth={self.config.k_depth}, gamma_phase={self.config.gamma_phase}")
+        print(f"[AutoPhiLearner]   target_criticality={self.config.target_criticality}")
+        print(f"[AutoPhiLearner]   enabled={self.config.enabled}")
 
-    def set_enabled(self, enabled: bool) :
-            enabled, False to disable
+    def set_enabled(self, enabled: bool):
+        """
+        Enable or disable learner (FR-006, SC-004)
+
+        Args:
+            enabled: True to enable, False to disable
         """
         if self.config.enabled == enabled:
             return
 
         self.config.enabled = enabled
         self.state.enabled = enabled
 
-        if enabled)
+        if enabled:
+            print("[AutoPhiLearner] ENABLED - starting adaptive control")
             # Reset state
             self.state.last_update = time.time()
             self.state.in_range_count = 0
             self.state.total_count = 0
-        else)
+        else:
+            print("[AutoPhiLearner] DISABLED - manual control restored")
+
+    def process_metrics(self, metrics_frame) -> bool:
+        """
+        Process incoming metrics frame (FR-002)
 
-    @lru_cache(maxsize=128)
-    def process_metrics(self, metrics_frame) :
-            metrics_frame, phase_coherence, etc.
+        Args:
+            metrics_frame: MetricsFrame with criticality, phase_coherence, etc.
 
         Returns:
             True if update was applied
         """
-        if not self.config.enabled)
+        if not self.config.enabled:
+            return False
+
+        current_time = time.time()
 
         # Check if we should update (rate limiting)
-        if current_time - self.state.last_update < self.config.update_interval, 'criticality', 0.0)
+        if current_time - self.state.last_update < self.config.update_interval:
+            return False
+
+        # Extract metrics
+        criticality = getattr(metrics_frame, 'criticality', 0.0)
         coherence = getattr(metrics_frame, 'phase_coherence', 0.0)
 
-        # Edge case)
+        # Edge case: Invalid metrics → skip (FR-002 edge case)
         if criticality == 0.0 and coherence == 0.0:
-            logger.warning("[AutoPhiLearner] WARNING, skipping update")
+            print("[AutoPhiLearner] WARNING: Invalid metrics, skipping update")
             return False
 
         # Add to history
         self.criticality_history.append(criticality)
         self.coherence_history.append(coherence)
 
         # Need enough history for derivative
-        if len(self.criticality_history) < 2)
+        if len(self.criticality_history) < 2:
+            return False
+
+        # Compute smoothed values
+        criticality_smooth = np.mean(self.criticality_history)
         coherence_smooth = np.mean(self.coherence_history)
 
         # Store in state
         self.state.criticality = criticality_smooth
         self.state.coherence = coherence_smooth
 
         # Compute time delta
         dt = current_time - self.state.last_update
         self.state.last_update = current_time
 
         # Apply control law (FR-003)
         updated = self._apply_control_law(criticality_smooth, coherence_smooth, dt)
 
         # Track performance (FR-007)
         self._track_performance(criticality_smooth, current_time)
 
         # Log if needed
-        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval)
+        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval:
+            self._log_stats()
             self.last_log_time = current_time
 
         return updated
 
-    def _apply_control_law(self, criticality, coherence, dt) :
+    def _apply_control_law(self, criticality: float, coherence: float, dt: float) -> bool:
+        """
+        Apply adaptive control law (FR-003)
+
+        Control equations:
+          phi_depth ← phi_depth + k × (target – criticality) × Δt
+          phi_phase ← phi_phase + γ × d(coherence)/dt
+
+        Args:
             criticality: Current criticality value
             coherence: Current phase coherence
             dt: Time delta since last update
 
         Returns:
             True if parameters were updated
         """
         updated = False
 
         # --- Depth Control ---
         # Error signal: positive when criticality is below target
         criticality_error = self.config.target_criticality - criticality
         self.state.criticality_error = criticality_error
 
         # Proportional control
         depth_delta = self.config.k_depth * criticality_error * dt
 
-        # Add external bias (Feature 013)
+        # Add external bias (Feature 013: predictive feed-forward)
         depth_delta += self.external_bias
 
         new_depth = self.state.phi_depth + depth_delta
 
         # Clamp to limits (FR-003 edge case)
         new_depth = np.clip(new_depth, self.config.depth_min, self.config.depth_max)
 
         # Apply if changed significantly (avoid noise)
-        if abs(new_depth - self.state.phi_depth) > 0.001)
-            if self.update_callback, float(new_depth))
+        if abs(new_depth - self.state.phi_depth) > 0.001:
+            self.state.phi_depth = new_depth
+
+            # Send update (FR-005)
+            if self.update_callback:
+                self.update_callback('phi_depth', float(new_depth))
 
             updated = True
 
         # --- Phase Control ---
         # Compute coherence derivative (rate of change)
-        if len(self.coherence_history) >= 2) / dt
+        if len(self.coherence_history) >= 2:
+            coherence_prev = self.coherence_history[-2]
+            coherence_derivative = (coherence - coherence_prev) / dt
             self.state.coherence_derivative = coherence_derivative
 
             # Apply phase adjustment
             phase_delta = self.config.gamma_phase * coherence_derivative
 
             new_phase = self.state.phi_phase + phase_delta
 
             # Wrap phase to [0, 1] range
             new_phase = new_phase % 1.0
 
             # Apply if changed significantly
-            if abs(new_phase - self.state.phi_phase) > 0.001)
-                if self.update_callback, float(new_phase))
+            if abs(new_phase - self.state.phi_phase) > 0.001:
+                self.state.phi_phase = new_phase
+
+                # Send update (FR-005)
+                if self.update_callback:
+                    self.update_callback('phi_phase', float(new_phase))
 
                 updated = True
 
         return updated
 
-    def _track_performance(self, criticality: float, current_time: float) :
+    def _track_performance(self, criticality: float, current_time: float):
+        """
+        Track performance metrics (FR-007, SC-001, SC-002)
+
+        Args:
             criticality: Current criticality value
             current_time: Current timestamp
         """
-        # Check if in range (SC-001)
+        # Check if in range (SC-001: ±0.05 tolerance)
         in_range = abs(criticality - self.config.target_criticality) <= 0.05
 
-        if in_range)
+        if in_range:
+            self.state.in_range_count += 1
+
+        self.state.total_count += 1
+
+        # Detect disturbances (SC-002)
         if abs(self.state.criticality_error) > self.config.disturbance_threshold:
             if self.state.settled:
                 # Disturbance detected
                 self.state.last_disturbance_time = current_time
                 self.state.settled = False
-                logger.error("[AutoPhiLearner] Disturbance detected, error=%s", criticality, self.state.criticality_error)
-        else)
+                print(f"[AutoPhiLearner] Disturbance detected: criticality={criticality:.3f}, error={self.state.criticality_error:.3f}")
+        else:
+            # Check if settled (within tolerance)
             if not self.state.settled and in_range:
                 settling_time = current_time - self.state.last_disturbance_time
                 self.state.settled = True
-                logger.info("[AutoPhiLearner] SETTLED in %ss (SC-002)", settling_time)
+                print(f"[AutoPhiLearner] SETTLED in {settling_time:.2f}s (SC-002: target <=5s)")
 
         # Append to log
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            self.criticality_log.append(criticality)
             self.depth_log.append(self.state.phi_depth)
             self.phase_log.append(self.state.phi_phase)
             self.timestamps.append(current_time)
 
-    @lru_cache(maxsize=128)
-    def _log_stats(self) :
-            crit_mean = np.mean(self.criticality_log[-30)  # Last 30 samples
-            crit_std = np.std(self.criticality_log[-30)
+    def _log_stats(self):
+        """Log performance statistics (FR-007)"""
+        if self.state.total_count == 0:
+            return
+
+        # Calculate in-range percentage (SC-001)
+        in_range_percent = (self.state.in_range_count / self.state.total_count) * 100
+
+        # Calculate criticality statistics
+        if len(self.criticality_log) > 0:
+            crit_mean = np.mean(self.criticality_log[-30:])  # Last 30 samples
+            crit_std = np.std(self.criticality_log[-30:])
         else:
             crit_mean = 0.0
             crit_std = 0.0
 
-        print(f"[AutoPhiLearner] Stats: criticality={crit_mean:.3f}±{crit_std, "
-              f"in_range={in_range_percent, "
-              f"phi_depth={self.state.phi_depth, phi_phase={self.state.phi_phase)
+        print(f"[AutoPhiLearner] Stats: criticality={crit_mean:.3f}±{crit_std:.3f}, "
+              f"in_range={in_range_percent:.1f}%, "
+              f"phi_depth={self.state.phi_depth:.3f}, phi_phase={self.state.phi_phase:.3f}")
+
+    def get_statistics(self) -> Dict:
+        """
+        Get performance statistics (FR-007)
 
-    def get_statistics(self) :
+        Returns:
             Dictionary with performance metrics
         """
         if self.state.total_count == 0:
             return {
-                'enabled',
-                'criticality_mean',
-                'criticality_std',
-                'in_range_percent',
-                'settled',
-                'phi_depth',
-                'phi_phase') * 100
-
-        if len(self.criticality_log) > 0))
+                'enabled': self.config.enabled,
+                'criticality_mean': 0.0,
+                'criticality_std': 0.0,
+                'in_range_percent': 0.0,
+                'settled': True,
+                'phi_depth': self.state.phi_depth,
+                'phi_phase': self.state.phi_phase
+            }
+
+        in_range_percent = (self.state.in_range_count / self.state.total_count) * 100
+
+        if len(self.criticality_log) > 0:
+            crit_mean = float(np.mean(self.criticality_log))
             crit_std = float(np.std(self.criticality_log))
         else:
             crit_mean = 0.0
             crit_std = 0.0
 
         return {
-            'enabled',
-            'criticality_mean',
-            'criticality_std',
-            'in_range_percent',
-            'in_range_count',
-            'total_count',
-            'settled',
-            'phi_depth',
-            'phi_phase',
-            'criticality_error',
-            'coherence_derivative') :
+            'enabled': self.config.enabled,
+            'criticality_mean': crit_mean,
+            'criticality_std': crit_std,
+            'in_range_percent': in_range_percent,
+            'in_range_count': self.state.in_range_count,
+            'total_count': self.state.total_count,
+            'settled': self.state.settled,
+            'phi_depth': self.state.phi_depth,
+            'phi_phase': self.state.phi_phase,
+            'criticality_error': self.state.criticality_error,
+            'coherence_derivative': self.state.coherence_derivative
+        }
+
+    def reset_statistics(self):
+        """Reset performance statistics"""
+        self.state.in_range_count = 0
+        self.state.total_count = 0
+        self.state.last_disturbance_time = 0.0
+        self.state.settled = True
+
+        self.criticality_log.clear()
+        self.depth_log.clear()
+        self.phase_log.clear()
+        self.timestamps.clear()
+
+        print("[AutoPhiLearner] Statistics reset")
+
+    def export_logs(self) -> Dict:
         """
         Export performance logs for analysis
 
         Returns:
             Dictionary with time-series data
         """
         return {
-            'timestamps',
-            'criticality',
-            'phi_depth',
-            'phi_phase',
+            'timestamps': self.timestamps,
+            'criticality': self.criticality_log,
+            'phi_depth': self.depth_log,
+            'phi_phase': self.phase_log,
             'config': {
-                'k_depth',
-                'gamma_phase',
-                'target_criticality') :
+                'k_depth': self.config.k_depth,
+                'gamma_phase': self.config.gamma_phase,
+                'target_criticality': self.config.target_criticality
+            }
+        }
+
+
+# Self-test function
+def _self_test():
+    """Test AutoPhiLearner"""
+    print("=" * 60)
+    print("AutoPhiLearner Self-Test")
+    print("=" * 60)
+
+    # Create mock metrics frame
+    class MockMetrics:
+        def __init__(self, criticality, phase_coherence):
             self.criticality = criticality
             self.phase_coherence = phase_coherence
 
-    # Test 1)
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = AutoPhiConfig(enabled=True, k_depth=0.5, gamma_phase=0.1)
     learner = AutoPhiLearner(config)
 
     assert learner.config.enabled == True
     assert learner.config.k_depth == 0.5
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Test 2)
-    logger.info("\n2. Testing enable/disable toggle...")
+    # Test 2: Toggle enable/disable (SC-004)
+    print("\n2. Testing enable/disable toggle...")
     learner.set_enabled(False)
     assert learner.config.enabled == False
     learner.set_enabled(True)
     assert learner.config.enabled == True
-    logger.info("   OK)
+    print("   OK: Toggle")
 
-    # Test 3)
+    # Test 3: Process metrics and control law
+    print("\n3. Testing control law...")
 
     # Set update callback
     updates = []
-    @lru_cache(maxsize=128)
-    def mock_callback(param, value) : Control law applied, initial_depth, learner.state.phi_depth)
+    def mock_callback(param, value):
+        updates.append((param, value))
+
+    learner.update_callback = mock_callback
 
-    # Test 4)
+    # Wait for initial update interval to pass
+    time.sleep(0.15)
+
+    # Send low criticality (should increase depth)
+    initial_depth = learner.state.phi_depth
+    metrics = MockMetrics(criticality=0.5, phase_coherence=0.8)
+
+    # Send first metrics frame
+    result1 = learner.process_metrics(metrics)
+    print(f"   First process_metrics: {result1}")
+
+    # Wait for update interval and send second frame (needed for derivative calculation)
+    time.sleep(0.15)
+    metrics2 = MockMetrics(criticality=0.5, phase_coherence=0.82)
+    result2 = learner.process_metrics(metrics2)
+    print(f"   Second process_metrics: {result2}")
+
+    # Depth should increase (criticality < target)
+    print(f"   Depth: {initial_depth:.3f} -> {learner.state.phi_depth:.3f}")
+    print(f"   Updates: {updates}")
+    assert learner.state.phi_depth > initial_depth, f"Expected depth to increase, got {learner.state.phi_depth}"
+    print(f"   OK: Control law applied: depth {initial_depth:.3f} -> {learner.state.phi_depth:.3f}")
+
+    # Test 4: Statistics tracking
+    print("\n4. Testing statistics...")
     stats = learner.get_statistics()
     assert 'criticality_mean' in stats
     assert 'in_range_percent' in stats
     assert stats['enabled'] == True
-    logger.info("   OK: Statistics, stats['in_range_percent'])
+    print(f"   OK: Statistics: in_range={stats['in_range_percent']:.1f}%")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/benchmark_runner.py b/server/benchmark_runner.py
index 485867968ee533046f47db28a1080c7102bd23b4..dc1c3c61994dc37b2de3d31754256482a43fe17b 100644
--- a/server/benchmark_runner.py
+++ b/server/benchmark_runner.py
@@ -1,44 +1,44 @@
 """
-BenchmarkRunner - Feature 018, frame rate, resource usage, and
+BenchmarkRunner - Feature 018: Phi-Adaptive Benchmark
+
+Automated benchmark suite for Phi-Matrix Dashboard performance testing.
+Executes comprehensive tests for latency, frame rate, resource usage, and
 adaptive scaling.
 
 Features:
 - FR-001: Automated benchmark suite
 - FR-002: Collect comprehensive metrics
 - FR-004: Persist benchmark summary
 - All User Stories: Complete benchmark workflow
 
-Requirements,000 frames
+Requirements:
+- Test latency across 10,000 frames
 - Validate adaptive scaling under load
 - Monitor memory and resource stability
 - Generate performance profile
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import time
 import os
 import json
 import psutil
 from typing import Optional, Dict, List
 from dataclasses import dataclass, asdict
 from datetime import datetime
 import numpy as np
 
 from .adaptive_scaler import AdaptiveScaler, ScalerConfig, PerformanceMetrics
 from .sync_profiler import SyncProfiler, ProfilerConfig
 
 
 @dataclass
 class BenchmarkConfig:
     """Configuration for benchmark suite"""
     # Test durations
     latency_test_frames: int = 10000
     sustained_test_duration_s: float = 300.0  # 5 minutes
     memory_test_duration_s: float = 300.0  # 5 minutes
 
     # Performance targets
     target_fps: float = 30.0
     max_latency_ms: float = 100.0
@@ -68,354 +68,491 @@ class BenchmarkResults:
     latency_stats: Dict
     latency_distribution: Dict
 
     # Frame rate results
     fps_stats: Dict
 
     # Resource usage
     cpu_stats: Dict
     gpu_stats: Dict
     memory_stats: Dict
 
     # Adaptive scaling
     scaling_stats: Dict
 
     # Success criteria
     success_criteria: Dict
 
     # Performance profile
     optimal_settings: Dict
 
 
 class BenchmarkRunner:
     """
     BenchmarkRunner - Automated performance test suite
 
-    Executes comprehensive benchmarks for, config: Optional[BenchmarkConfig]) :
+    Executes comprehensive benchmarks for:
+    - WebSocket latency and synchronization
+    - Sustained frame rate under load
+    - Memory and resource stability
+    - Adaptive scaling performance
+    - Auto-calibration of optimal settings
+    """
+
+    def __init__(self, config: Optional[BenchmarkConfig] = None):
+        """Initialize BenchmarkRunner"""
+        self.config = config or BenchmarkConfig()
+
+        # Components
+        self.profiler = SyncProfiler(ProfilerConfig(
+            max_samples=self.config.latency_test_frames,
+            enable_logging=False
+        ))
+
+        self.scaler = AdaptiveScaler(ScalerConfig(
+            target_fps=self.config.target_fps,
+            max_cpu_percent=self.config.max_cpu_percent,
+            max_gpu_percent=self.config.max_gpu_percent,
+            enable_logging=self.config.enable_logging
+        )) if self.config.enable_adaptive_scaling else None
+
+        # Resource monitoring
+        self.process = psutil.Process()
+        self.start_memory_mb = 0.0
+
+        # Results
+        self.results: Optional[BenchmarkResults] = None
+
+    def run_full_benchmark(self) -> BenchmarkResults:
         """
         Run complete benchmark suite
 
-        logger.info("Phi-Matrix Dashboard - Full Benchmark Suite")
-        logger.info("=" * 70)
-        logger.info(str())
+        Returns:
+            BenchmarkResults with all test results
+        """
+        print("=" * 70)
+        print("Phi-Matrix Dashboard - Full Benchmark Suite")
+        print("=" * 70)
+        print()
 
         start_time = time.time()
         self.start_memory_mb = self.process.memory_info().rss / (1024 * 1024)
 
         # Test 1: Latency Benchmark
-        logger.info("Test 1)
-        logger.info("-" * 70)
+        print("Test 1: WebSocket Latency Benchmark")
+        print("-" * 70)
         latency_stats, latency_dist = self._run_latency_benchmark()
-        logger.info(str())
+        print()
 
         # Test 2: Frame Rate Benchmark
-        logger.info("Test 2)
-        logger.info("-" * 70)
+        print("Test 2: Sustained Frame Rate Benchmark")
+        print("-" * 70)
         fps_stats = self._run_fps_benchmark()
-        logger.info(str())
+        print()
 
         # Test 3: Resource Usage Benchmark
-        logger.info("Test 3)
-        logger.info("-" * 70)
+        print("Test 3: Resource Usage & Stability Benchmark")
+        print("-" * 70)
         cpu_stats, gpu_stats, memory_stats = self._run_resource_benchmark()
-        logger.info(str())
+        print()
 
         # Test 4: Adaptive Scaling Benchmark
         if self.scaler:
-            logger.info("Test 4)
-            logger.info("-" * 70)
+            print("Test 4: Adaptive Scaling Benchmark")
+            print("-" * 70)
             scaling_stats = self._run_adaptive_scaling_benchmark()
-            logger.info(str())
-        else, fps_stats, memory_stats, scaling_stats
+            print()
+        else:
+            scaling_stats = {}
+
+        # Evaluate success criteria
+        success_criteria = self._evaluate_success_criteria(
+            latency_stats, fps_stats, memory_stats, scaling_stats
+        )
 
         # Generate optimal settings
         optimal_settings = self._generate_optimal_settings(
             latency_stats, fps_stats, cpu_stats, gpu_stats
+        )
 
         duration = time.time() - start_time
 
         # Create results
         self.results = BenchmarkResults(
             timestamp=datetime.now().isoformat(),
             duration_s=duration,
             latency_stats=latency_stats,
             latency_distribution=latency_dist,
             fps_stats=fps_stats,
             cpu_stats=cpu_stats,
             gpu_stats=gpu_stats,
             memory_stats=memory_stats,
             scaling_stats=scaling_stats,
             success_criteria=success_criteria,
             optimal_settings=optimal_settings
+        )
 
         # Print summary
         self._print_summary()
 
         return self.results
 
-    @lru_cache(maxsize=128)
-    def _run_latency_benchmark(self) :
-                logger.info("    Progress, i + 1, self.config.latency_test_frames)
+    def _run_latency_benchmark(self) -> tuple:
+        """
+        Test WebSocket latency across 10,000 frames
+
+        Returns:
+            (latency_stats, latency_distribution)
+        """
+        print(f"  Running {self.config.latency_test_frames} latency measurements...")
+
+        for i in range(self.config.latency_test_frames):
+            # Simulate ping/pong
+            ping = self.profiler.create_ping_message()
+
+            # Simulate network delay (0.5-5ms)
+            delay_ms = np.random.uniform(0.5, 5.0)
+            time.sleep(delay_ms / 1000.0)
+
+            pong = {
+                "type": "pong",
+                "ping_id": ping['ping_id'],
+                "server_time": time.time() * 1000.0,
+                "client_time": ping['client_time']
+            }
+
+            self.profiler.handle_pong_message(pong)
+
+            if (i + 1) % 1000 == 0:
+                print(f"    Progress: {i + 1}/{self.config.latency_test_frames}")
 
         # Cleanup any timeouts
         self.profiler.cleanup_pending_pings()
 
         # Get statistics
         stats = self.profiler.get_statistics()
         dist = self.profiler.get_latency_distribution(bucket_size_ms=10.0)
 
-        logger.info("  Completed %s measurements", stats['total_samples'])
-        logger.info("  Avg latency, stats['avg_latency_ms'])
-        logger.info("  Max latency, stats['max_latency_ms'])
-        logger.info("  P95 latency, stats['p95_latency_ms'])
-        logger.info("  Success rate, stats['success_rate'])
+        print(f"  Completed {stats['total_samples']} measurements")
+        print(f"  Avg latency: {stats['avg_latency_ms']:.2f} ms")
+        print(f"  Max latency: {stats['max_latency_ms']:.2f} ms")
+        print(f"  P95 latency: {stats['p95_latency_ms']:.2f} ms")
+        print(f"  Success rate: {stats['success_rate']:.1f}%")
 
         return stats, dist
 
-    def _run_fps_benchmark(self) :
+    def _run_fps_benchmark(self) -> Dict:
         """
         Test sustained frame rate for 5 minutes
 
-        Returns)...", test_duration)
+        Returns:
+            FPS statistics
+        """
+        test_duration = 60.0  # 60 seconds for faster testing
+        print(f"  Running sustained FPS test ({test_duration:.0f}s)...")
 
         start_time = time.time()
         frame_times = []
         frame_count = 0
 
-        while (time.time() - start_time) < test_duration)
+        while (time.time() - start_time) < test_duration:
+            frame_start = time.time()
 
             # Simulate frame rendering
             time.sleep(1.0 / 60.0)  # Target 60 FPS
 
             frame_end = time.time()
             frame_time_ms = (frame_end - frame_start) * 1000.0
             frame_times.append(frame_time_ms)
             frame_count += 1
 
         elapsed = time.time() - start_time
         avg_fps = frame_count / elapsed
         avg_frame_time = float(np.mean(frame_times))
         min_frame_time = float(np.min(frame_times))
         max_frame_time = float(np.max(frame_times))
 
         # Calculate frame time percentiles
         p95_frame_time = float(np.percentile(frame_times, 95))
         p99_frame_time = float(np.percentile(frame_times, 99))
 
         # Check for frame drops
         target_frame_time = 1000.0 / self.config.target_fps
         frame_drops = sum(1 for ft in frame_times if ft > target_frame_time * 1.5)
         frame_drop_rate = (frame_drops / frame_count) * 100.0
 
         stats = {
-            "total_frames",
-            "duration_s",
-            "avg_fps",
-            "avg_frame_time_ms",
-            "min_frame_time_ms",
-            "max_frame_time_ms",
-            "p95_frame_time_ms",
-            "p99_frame_time_ms",
-            "frame_drops",
+            "total_frames": frame_count,
+            "duration_s": elapsed,
+            "avg_fps": avg_fps,
+            "avg_frame_time_ms": avg_frame_time,
+            "min_frame_time_ms": min_frame_time,
+            "max_frame_time_ms": max_frame_time,
+            "p95_frame_time_ms": p95_frame_time,
+            "p99_frame_time_ms": p99_frame_time,
+            "frame_drops": frame_drops,
             "frame_drop_rate_percent": frame_drop_rate
         }
 
-        logger.info("  Total frames, frame_count)
-        logger.info("  Average FPS, avg_fps)
-        logger.info("  Frame drops)", frame_drops, frame_drop_rate)
+        print(f"  Total frames: {frame_count}")
+        print(f"  Average FPS: {avg_fps:.1f}")
+        print(f"  Frame drops: {frame_drops} ({frame_drop_rate:.1f}%)")
 
         return stats
 
-    @lru_cache(maxsize=128)
-    def _run_resource_benchmark(self) : GPU stats would require additional library like GPUtil
+    def _run_resource_benchmark(self) -> tuple:
+        """
+        Monitor CPU, GPU, and memory usage
+
+        Returns:
+            (cpu_stats, gpu_stats, memory_stats)
+        """
+        test_duration = 60.0  # 60 seconds for faster testing
+        print(f"  Monitoring resources ({test_duration:.0f}s)...")
+
+        cpu_samples = []
+        memory_samples = []
+
+        start_time = time.time()
+        initial_memory = self.process.memory_info().rss / (1024 * 1024)
+
+        while (time.time() - start_time) < test_duration:
+            # CPU usage
+            cpu_percent = self.process.cpu_percent(interval=0.1)
+            cpu_samples.append(cpu_percent)
+
+            # Memory usage
+            memory_mb = self.process.memory_info().rss / (1024 * 1024)
+            memory_samples.append(memory_mb)
+
+            time.sleep(1.0)
+
+        final_memory = self.process.memory_info().rss / (1024 * 1024)
+        memory_increase = final_memory - initial_memory
+        memory_increase_percent = (memory_increase / initial_memory) * 100.0
+
+        cpu_stats = {
+            "avg_cpu_percent": float(np.mean(cpu_samples)),
+            "max_cpu_percent": float(np.max(cpu_samples)),
+            "p95_cpu_percent": float(np.percentile(cpu_samples, 95))
+        }
+
+        # Note: GPU stats would require additional library like GPUtil
         gpu_stats = {
-            "avg_gpu_percent",  # Placeholder
-            "max_gpu_percent",
-            "note")"
+            "avg_gpu_percent": 0.0,  # Placeholder
+            "max_gpu_percent": 0.0,
+            "note": "GPU monitoring not implemented (requires GPUtil)"
         }
 
         memory_stats = {
-            "initial_memory_mb",
-            "final_memory_mb",
-            "memory_increase_mb",
-            "memory_increase_percent",
-            "avg_memory_mb")),
-            "max_memory_mb"))
+            "initial_memory_mb": initial_memory,
+            "final_memory_mb": final_memory,
+            "memory_increase_mb": memory_increase,
+            "memory_increase_percent": memory_increase_percent,
+            "avg_memory_mb": float(np.mean(memory_samples)),
+            "max_memory_mb": float(np.max(memory_samples))
         }
 
-        logger.info("  CPU, max=%s%", cpu_stats['avg_cpu_percent'], cpu_stats['max_cpu_percent'])
-        logger.info("  Memory)", memory_increase, memory_increase_percent)
+        print(f"  CPU: avg={cpu_stats['avg_cpu_percent']:.1f}%, max={cpu_stats['max_cpu_percent']:.1f}%")
+        print(f"  Memory: increase={memory_increase:.1f}MB ({memory_increase_percent:.1f}%)")
 
         return cpu_stats, gpu_stats, memory_stats
 
-    def _run_adaptive_scaling_benchmark(self) :
+    def _run_adaptive_scaling_benchmark(self) -> Dict:
         """
         Test adaptive scaling performance
 
+        Returns:
+            Scaling statistics
+        """
+        print(f"  Testing adaptive scaling...")
+
         # Simulate performance degradation
-        logger.info("    Phase 1)")
-        for i in range(20)),
+        print(f"    Phase 1: Low performance (triggering scale down)")
+        for i in range(20):
+            metrics = PerformanceMetrics(
+                timestamp=time.time(),
                 fps=20.0,  # Below target
                 cpu_percent=65.0,  # Above target
                 gpu_percent=80.0,  # Above target
                 memory_mb=500.0,
                 latency_ms=60.0,
                 interaction_delay_ms=55.0
-
+            )
             self.scaler.add_metrics(metrics)
             time.sleep(0.05)
 
         time.sleep(0.2)  # Wait for scaling response
 
         # Simulate performance recovery
-        logger.info("    Phase 2)")
-        for i in range(30)),
+        print(f"    Phase 2: High performance (triggering scale up)")
+        for i in range(30):
+            metrics = PerformanceMetrics(
+                timestamp=time.time(),
                 fps=60.0,  # Above target
                 cpu_percent=30.0,  # Below target
                 gpu_percent=40.0,  # Below target
                 memory_mb=500.0,
                 latency_ms=20.0,
                 interaction_delay_ms=15.0
-
+            )
             self.scaler.add_metrics(metrics)
             time.sleep(0.05)
 
         stats = self.scaler.get_performance_summary()
 
-        logger.info("  Scale adjustments, stats['total_adjustments'])
-        logger.info("  Final complexity level, stats['current_complexity_level'])
+        print(f"  Scale adjustments: {stats['total_adjustments']}")
+        print(f"  Final complexity level: {stats['current_complexity_level']}")
 
         return stats
 
     def _evaluate_success_criteria(
-        self, latency_stats, fps_stats,
-        memory_stats, scaling_stats) :
+        self, latency_stats: Dict, fps_stats: Dict,
+        memory_stats: Dict, scaling_stats: Dict
+    ) -> Dict:
         """
         Evaluate all success criteria
 
         Returns:
             Success criteria evaluation
         """
         criteria = {
-            "SC-001 Round-trip latency <= 100ms", False),
-            "SC-002 Frame rate >= 30 fps",
-            "SC-003 Interaction delay <= 50ms", False),
-            "SC-004 Memory increase < 5%",
-            "SC-005 Auto-scaling response < 0.5s", True) if scaling_stats else True
+            "SC-001 Round-trip latency <= 100ms": latency_stats.get('meets_sc001', False),
+            "SC-002 Frame rate >= 30 fps": fps_stats['avg_fps'] >= 30.0,
+            "SC-003 Interaction delay <= 50ms": latency_stats.get('meets_sc003', False),
+            "SC-004 Memory increase < 5%": memory_stats['memory_increase_percent'] < 5.0,
+            "SC-005 Auto-scaling response < 0.5s": scaling_stats.get('meets_sc005', True) if scaling_stats else True
         }
 
         criteria["all_passed"] = all(criteria.values())
 
         return criteria
 
     def _generate_optimal_settings(
-        self, latency_stats, fps_stats,
-        cpu_stats, gpu_stats) :
+        self, latency_stats: Dict, fps_stats: Dict,
+        cpu_stats: Dict, gpu_stats: Dict
+    ) -> Dict:
         """
         Generate optimal performance profile
 
         Returns:
             Optimal settings dictionary
         """
         # Determine optimal frame rate target
         avg_fps = fps_stats['avg_fps']
         if avg_fps >= 60:
             optimal_fps = 60
         elif avg_fps >= 45:
             optimal_fps = 45
         else:
             optimal_fps = 30
 
         # Determine optimal buffer size based on latency
         avg_latency = latency_stats['avg_latency_ms']
         if avg_latency < 10:
             optimal_buffer_ms = 10
         elif avg_latency < 25:
             optimal_buffer_ms = 20
         else:
             optimal_buffer_ms = 30
 
         # Determine optimal visual complexity
         avg_cpu = cpu_stats['avg_cpu_percent']
         if avg_cpu < 30:
             optimal_complexity = 5  # Maximum
         elif avg_cpu < 45:
             optimal_complexity = 4
         else:
             optimal_complexity = 3
 
         return {
-            "target_fps",
-            "audio_buffer_ms",
-            "visual_complexity_level",
-            "enable_phi_breathing",
-            "enable_topology_links",
-            "enable_gradients",
-            "render_resolution")
+            "target_fps": optimal_fps,
+            "audio_buffer_ms": optimal_buffer_ms,
+            "visual_complexity_level": optimal_complexity,
+            "enable_phi_breathing": optimal_complexity >= 3,
+            "enable_topology_links": optimal_complexity >= 4,
+            "enable_gradients": optimal_complexity >= 2,
+            "render_resolution": 0.5 + (optimal_complexity / 10.0)
         }
 
-    def _print_summary(self) :
+    def _print_summary(self):
         """Print benchmark summary"""
-        if not self.results)
-        logger.info("Benchmark Summary")
-        logger.info("=" * 70)
-        logger.info(str())
+        if not self.results:
+            return
 
-        logger.info("Success Criteria)
+        print("=" * 70)
+        print("Benchmark Summary")
+        print("=" * 70)
+        print()
+
+        print("Success Criteria:")
         for criterion, passed in self.results.success_criteria.items():
-            if criterion == "all_passed", status, criterion)
+            if criterion == "all_passed":
+                continue
+            status = "[PASS]" if passed else "[FAIL]"
+            print(f"  {status} {criterion}")
 
-        logger.info(str())
+        print()
         all_passed = self.results.success_criteria["all_passed"]
-        if all_passed)
-        else)
+        if all_passed:
+            print("[PASS] ALL SUCCESS CRITERIA MET")
+        else:
+            print("[FAIL] SOME CRITERIA NOT MET - Review results above")
 
-        logger.info(str())
-        logger.info("Optimal Settings)
+        print()
+        print("Optimal Settings:")
         for key, value in self.results.optimal_settings.items():
-            logger.info("  %s, key, value)
+            print(f"  {key}: {value}")
 
-        logger.info(str())
-        logger.info("=" * 70)
+        print()
+        print("=" * 70)
 
-    def save_results(self) :
+    def save_results(self):
         """Save benchmark results to files"""
-        if not self.results)
+        if not self.results:
+            print("No results to save")
             return
 
         # Create directories
         os.makedirs(self.config.logs_dir, exist_ok=True)
         os.makedirs(self.config.config_dir, exist_ok=True)
 
         # Convert results to JSON-serializable format
         results_dict = asdict(self.results)
         results_json = json.loads(json.dumps(results_dict, default=str))
 
         # Save benchmark report
         report_path = os.path.join(
             self.config.logs_dir,
             f"phi_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
+        )
 
-        with open(report_path, 'w') as f, f, indent=2)
+        with open(report_path, 'w') as f:
+            json.dump(results_json, f, indent=2)
 
-        logger.info("Benchmark report saved to, report_path)
+        print(f"Benchmark report saved to: {report_path}")
 
         # Save performance profile
         profile_path = os.path.join(
             self.config.config_dir,
             "performance_profile.json"
+        )
 
-        with open(profile_path, 'w') as f, f, indent=2)
+        with open(profile_path, 'w') as f:
+            json.dump(self.results.optimal_settings, f, indent=2)
 
-        logger.info("Performance profile saved to, profile_path)
+        print(f"Performance profile saved to: {profile_path}")
 
 
 # Command-line interface
-if __name__ == "__main__",  # Reduced for faster testing
+if __name__ == "__main__":
+    config = BenchmarkConfig(
+        latency_test_frames=1000,  # Reduced for faster testing
         sustained_test_duration_s=60.0,  # 1 minute
         memory_test_duration_s=60.0,  # 1 minute
         enable_adaptive_scaling=True,
         enable_logging=True
+    )
 
     runner = BenchmarkRunner(config)
     results = runner.run_full_benchmark()
     runner.save_results()
diff --git a/server/calibrate_sensors.py b/server/calibrate_sensors.py
index 06aac23429cde638b0a18d805bf71140ffd6d71a..4c111810462194541e8e25fe0961aae713838d2a 100644
--- a/server/calibrate_sensors.py
+++ b/server/calibrate_sensors.py
@@ -1,135 +1,148 @@
 #!/usr/bin/env python3
 """
 Sensor Calibration Utility - Feature 023 (FR-007)
 
 Calibrates Φ-sensors and stores calibration data to config/sensors.json
 
 Usage:
     python calibrate_sensors.py --duration 10000
 
 Requirements:
 - FR-007: Calibration routine stores offsets in /config/sensors.json
-- SC-005, Dict, List, Optional, Tuple
+- SC-005: Calibration residual error < 2%
+"""
+
 import asyncio
 import argparse
 import json
 import logging
 from pathlib import Path
 from datetime import datetime
 from .sensor_manager import SensorManager
 
 
 logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
 logger = logging.getLogger(__name__)
 
 
-async def calibrate_sensors(duration_ms, config_path):
+async def calibrate_sensors(duration_ms: int = 10000, config_path: str = "config/sensors.json"):
     """
     Perform sensor calibration and save results
 
     Args:
         duration_ms: Calibration duration in milliseconds
-        config_path)
-    logger.info("Soundlab Φ-Sensor Calibration Utility")
-    logger.info("Feature 023 - Hardware Validation")
-    logger.info("=" * 70)
-    logger.info(str())
+        config_path: Path to save calibration config
+    """
+    print("=" * 70)
+    print("Soundlab Φ-Sensor Calibration Utility")
+    print("Feature 023 - Hardware Validation")
+    print("=" * 70)
+    print()
 
     # Create sensor manager
     config = {
-        'simulation_mode',  # Set to False for real hardware
-        'enable_watchdog',  # Disable watchdog during calibration
+        'simulation_mode': True,  # Set to False for real hardware
+        'enable_watchdog': False,  # Disable watchdog during calibration
     }
 
     logger.info("Initializing sensor manager...")
     manager = SensorManager(config)
 
     # Start acquisition
     logger.info("Starting sensor acquisition...")
     await manager.start()
 
     # Wait for sensors to stabilize
     logger.info("Stabilizing sensors (2 seconds)...")
     await asyncio.sleep(2.0)
 
     # Perform calibration
     logger.info("Performing calibration (%d ms)...", duration_ms)
-    logger.info("\n%s", '='*70)
-    logger.info("Calibration in progress, duration_ms)
-    logger.info("%s", '='*70)
+    print(f"\n{'='*70}")
+    print(f"Calibration in progress: {duration_ms} ms")
+    print(f"{'='*70}")
 
     calibration = await manager.calibrate(duration_ms=duration_ms)
 
-    logger.info("\nCalibration Results)
-    logger.info("-" * 70)
-    logger.info("Samples collected, calibration['samples'])
-    logger.info("Duration, calibration['duration_ms'])
-    logger.error("Residual error, calibration['residual_error'])
-    logger.info(str())
+    print("\nCalibration Results:")
+    print("-" * 70)
+    print(f"Samples collected: {calibration['samples']}")
+    print(f"Duration: {calibration['duration_ms']} ms")
+    print(f"Residual error: {calibration['residual_error']:.2f}%")
+    print()
 
     # Show channel ranges
     for channel in ['phi_depth', 'phi_phase', 'coherence', 'criticality']:
         if channel in calibration:
             data = calibration[channel]
-            logger.info("%s, channel)
-            logger.info("  Min, data['min'])
-            logger.info("  Max, data['max'])
-            logger.info("  Mean, data['mean'])
-            logger.info("  Std, data['std'])
-            logger.info(str())
+            print(f"{channel}:")
+            print(f"  Min:  {data['min']:.4f}")
+            print(f"  Max:  {data['max']:.4f}")
+            print(f"  Mean: {data['mean']:.4f}")
+            print(f"  Std:  {data['std']:.4f}")
+            print()
 
     # Check residual error (SC-005)
     if calibration['residual_error'] >= 2.0:
-        logger.error("⚠ WARNING, calibration['residual_error'])
-        logger.info("  Calibration may be inaccurate. Consider recalibrating.")
-    else, calibration['residual_error'])
+        print(f"⚠ WARNING: Residual error {calibration['residual_error']:.2f}% exceeds 2% threshold")
+        print("  Calibration may be inaccurate. Consider recalibrating.")
+    else:
+        print(f"✓ Residual error {calibration['residual_error']:.2f}% within acceptable range")
 
     # Save calibration
     config_file = Path(config_path)
     config_file.parent.mkdir(parents=True, exist_ok=True)
 
     logger.info("Saving calibration to %s...", config_path)
     await manager.save_calibration(calibration, str(config_file))
 
-    logger.info(str())
-    logger.info("=" * 70)
-    logger.info("✓ Calibration saved to, config_path)
-    logger.info("=" * 70)
+    print()
+    print("=" * 70)
+    print(f"✓ Calibration saved to: {config_path}")
+    print("=" * 70)
 
     # Stop acquisition
     await manager.stop()
 
     return calibration
 
 
-def main() -> None)
+def main():
+    parser = argparse.ArgumentParser(
+        description="Calibrate Φ-sensors and save calibration data"
+    )
 
     parser.add_argument(
         '--duration',
         type=int,
         default=10000,
-        help='Calibration duration in milliseconds (default)'
+        help='Calibration duration in milliseconds (default: 10000)'
+    )
 
     parser.add_argument(
         '--output',
         type=str,
         default='config/sensors.json',
-        help='Output path for calibration data (default)'
+        help='Output path for calibration data (default: config/sensors.json)'
+    )
 
     parser.add_argument(
         '--verbose',
         action='store_true',
         help='Enable verbose logging'
+    )
 
     args = parser.parse_args()
 
-    if args.verbose).setLevel(logging.DEBUG)
+    if args.verbose:
+        logging.getLogger().setLevel(logging.DEBUG)
 
     # Run calibration
     asyncio.run(calibrate_sensors(
         duration_ms=args.duration,
         config_path=args.output
     ))
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    main()
diff --git a/server/chromatic_field_processor.py b/server/chromatic_field_processor.py
index e5737781eac04ba2606a6313fb6285f66ee60bd3..10302bcf532a12fe4bba77ae5e61080237dcdd57 100644
--- a/server/chromatic_field_processor.py
+++ b/server/chromatic_field_processor.py
@@ -1,315 +1,385 @@
-# ==========================================================
-# chromatic_field_processor_fixed.py  (Section 1/4)
-# ==========================================================
-# Purpose, formatting, and logger safety
-# ==========================================================
-
+"""
+ChromaticFieldProcessor - Python wrapper around D-ASE AnalogCellularEngineAVX2
+
+Provides:
+- 8×8 channel (64 oscillator) real-time audio processing
+- Φ-modulated multi-channel output
+- Comprehensive metrics calculation (ICI, Phase Coherence, Spectral Centroid, Consciousness Level)
+- Low-latency block processing (target: <6ms)
+"""
+
+import sys
+import os
 import numpy as np
-import asyncio
+from scipy import signal
+from typing import Dict, Optional, Tuple
 import time
-import logging
-from functools import lru_cache
-from typing import Any, Dict, List, Optional, Tuple
+from .ici_engine import IntegratedChromaticInformation, ICIConfig
 
-logger = logging.getLogger(__name__)
+# Add sase amp fixed to path to import dase_engine
+DASE_PATH = os.path.join(os.path.dirname(__file__), '..', 'sase amp fixed')
+if DASE_PATH not in sys.path:
+    sys.path.insert(0, DASE_PATH)
 
+try:
+    import dase_engine
+    DASE_AVAILABLE = True
+except ImportError as e:
+    print(f"Warning: D-ASE engine not available: {e}")
+    print(f"Build with: cd '{DASE_PATH}' && python setup.py build_ext --inplace")
+    DASE_AVAILABLE = False
 
-class ChromaticFieldProcessor:
-        num_channels,
-        sample_rate,
-        fft_size,
-        overlap,
-        gain,
-        chromatic_weight,
-        enable_phi_sync,
-    ) :
-        self.num_channels = num_channels
-        self.sample_rate = sample_rate
-        self.fft_size = fft_size
-        self.overlap = overlap
-        self.gain = gain
-        self.chromatic_weight = chromatic_weight
-        self.enable_phi_sync = enable_phi_sync
-
-        self.last_frame_time: Optional[float] = None
-        self.phi_sync_state, Any] = {
-            "phase",
-            "last_update"),
-            "enabled",
-        
-        }
-                # ----------------------------------------
-        # Auto-Φ learner defaults (prevents AttributeError)
-        # ----------------------------------------
-        self.auto_phi_enabled = False
-        self._last_metrics = {"valid": False}
-
-
-        # Buffers
-        self.input_buffer: List[np.ndarray] = []
-        self.output_buffer: List[np.ndarray] = []
-
-        # Diagnostic and metrics tracking
-        self.block_count = 0
-        self.avg_process_time = 0.0
-        self.latency_ms = 0.0
-        self.max_latency_ms = 0.0
-        self.last_fft: Optional[np.ndarray] = None
-        self.last_ifft, rate=%dHz, FFT=%d",
-            num_channels,
-            sample_rate,
-            fft_size,
-
-    # ==========================================================
-    # Core Processing Pipeline
-    # ==========================================================
-
-    def process_block(self, input_block) :
-            if input_block.ndim == 1, axis=0)
-
-            # Apply gain normalization
-            block = np.clip(input_block * self.gain, -1.0, 1.0)
-
-            # FFT transform
-            spectrum = np.fft.rfft(block, n=self.fft_size)
-            magnitude = np.abs(spectrum)
-            phase = np.angle(spectrum)
-
-            # Apply chromatic weighting
-            weighted_magnitude = magnitude * self.chromatic_weight
-
-            # Reconstruct signal
-            processed = np.fft.irfft(weighted_magnitude * np.exp(1j * phase), n=self.fft_size)
-            processed = processed[:, )
-
-            # Track performance
-            elapsed_ms = (time.perf_counter() - start_time) * 1000.0
-            self.block_count += 1
-            self.avg_process_time = (
-                (self.avg_process_time * (self.block_count - 1)) + elapsed_ms
-            ) / self.block_count
-            self.max_latency_ms = max(self.max_latency_ms, elapsed_ms)
-
-            logger.info(f"   ✓ Block processed in {elapsed_ms)
-
-            return processed
 
-        except Exception as e:
-            logger.exception("Error processing block, e)
-            return np.zeros_like(input_block)
+class ChromaticFieldProcessor:
+    """
+    Wrapper around D-ASE AnalogCellularEngineAVX2 for chromatic field processing
 
+    Implements 8×8 channel configuration (64 total nodes) with Φ-modulation support
+    """
 
+    PHI = 1.618033988749895  # Golden ratio
+    PHI_INV = 0.618033988749895  # 1/Φ
 
+    def __init__(self, num_channels: int = 8, sample_rate: int = 48000, block_size: int = 512):
+        """
+        Initialize ChromaticFieldProcessor
 
+        Args:
+            num_channels: Number of channels per dimension (8 = 64 total nodes)
+            sample_rate: Audio sample rate in Hz (48000 for spec compliance)
+            block_size: Processing block size in samples (512 for spec compliance)
+        """
+        self.num_channels = num_channels
+        self.num_nodes = num_channels * num_channels  # 8×8 = 64
+        self.sample_rate = sample_rate
+        self.block_size = block_size
+
+        # Initialize D-ASE engine
+        if not DASE_AVAILABLE:
+            raise RuntimeError("D-ASE engine not available. Cannot initialize processor.")
+
+        self.engine = dase_engine.AnalogCellularEngine(self.num_nodes)
+        print(f"[ChromaticFieldProcessor] Initialized with {self.num_nodes} nodes @ {sample_rate}Hz")
+
+        # Check CPU features
+        has_avx2 = dase_engine.CPUFeatures.has_avx2()
+        has_fma = dase_engine.CPUFeatures.has_fma()
+        print(f"[ChromaticFieldProcessor] CPU Features: AVX2={has_avx2}, FMA={has_fma}")
+
+        # Metrics storage
+        self.last_metrics = {
+            'ici': 0.0,
+            'phase_coherence': 0.0,
+            'spectral_centroid': 0.0,
+            'consciousness_level': 0.0
+        }
 
+        # Performance tracking
+        self.process_time_history = []
+        self.max_history_length = 100
+
+        # Multi-channel output buffer [channels, samples]
+        self.output_buffer = np.zeros((self.num_channels, self.block_size), dtype=np.float32)
+
+        # Initialize ICI Engine (Feature 014)
+        ici_config = ICIConfig(
+            num_channels=self.num_channels,
+            fft_size=self.block_size,
+            smoothing_alpha=0.2,
+            use_rfft=True,
+            output_matrix=False,
+            enable_logging=False
+        )
+        self.ici_engine = IntegratedChromaticInformation(ici_config)
+
+    def processBlock(self,
+                     input_block: np.ndarray,
+                     phi_phase: float = 0.0,
+                     phi_depth: float = 0.5) -> np.ndarray:
+        """
+        Process single audio block through D-ASE engine with Φ-modulation
 
+        Args:
+            input_block: float32[block_size] mono input signal
+            phi_phase: Φ-phase offset in radians [0, 2π]
+            phi_depth: Φ-modulation depth [0.0, 1.0]
 
-# ======================== END OF SECTION 1 ========================
-# (Next, async metrics, and smoothing)
-# Paste below this divider when ready
-# ==========================================================
-# chromatic_field_processor_fixed.py  (Section 2/4)
-# ==========================================================
-# Φ-Synchronization, async broadcast support, and safety checks
-# ==========================================================
+        Returns:
+            float32[num_channels, block_size] multi-channel output
 
-    async def update_phi_sync(self, delta_phase) :
-            state = {
-                "phase",
-                "avg_process_time_ms", 3),
-                "max_latency_ms", 3),
-                "block_count",
-            }
-            await websocket.send_json(state)
-            logger.debug("Broadcasted Φ-state → %s", state)
-        except Exception as e:
-            logger.error("WebSocket broadcast failed, e)
-
-    async def run_realtime_loop(
-        self,
-        input_stream,
-        websocket=None,
-        refresh_interval,
-    ) :
+        Raises:
+            ValueError: If input_block shape is incorrect
         """
-        Primary async loop)
-        try:
-            async for block in input_stream)
-                smoothed = await self.smooth_transition(processed)
+        start_time = time.perf_counter()
+
+        # Validate input
+        if input_block.shape[0] != self.block_size:
+            raise ValueError(f"Input block must be {self.block_size} samples, got {input_block.shape[0]}")
+
+        # Ensure float32
+        if input_block.dtype != np.float32:
+            input_block = input_block.astype(np.float32)
+
+        # Generate Φ-modulation envelope
+        modulation = self._generatePhiModulation(phi_phase, phi_depth)
+
+        # Process through each channel group (8 channels)
+        for ch_idx in range(self.num_channels):
+            # Calculate node range for this channel
+            node_start = ch_idx * self.num_channels
+            node_end = node_start + self.num_channels
+
+            # Process block through cellular engine for this channel group
+            # Apply Φ-rotated modulation for each channel
+            channel_phase_offset = ch_idx * self.PHI_INV * 2 * np.pi
+            channel_mod = np.roll(modulation, int(channel_phase_offset * self.block_size / (2*np.pi)))
+
+            # Process each sample in the block
+            for sample_idx in range(self.block_size):
+                # Input signal modulated by Φ-envelope
+                modulated_input = input_block[sample_idx] * channel_mod[sample_idx]
+
+                # Control signal varies with golden ratio
+                control_signal = np.cos(sample_idx * self.PHI_INV / self.block_size * 2 * np.pi)
+
+                # Process through D-ASE engine (simplified: using first node of group)
+                node_idx = node_start
+                if node_idx < len(self.engine.nodes):
+                    # Process through single representative node for this channel
+                    output_sample = self.engine.nodes[node_idx].process_signal_avx2(
+                        float(modulated_input),
+                        float(control_signal * phi_depth),
+                        0.0  # aux_signal
+                    )
+                    self.output_buffer[ch_idx, sample_idx] = output_sample
+                else:
+                    self.output_buffer[ch_idx, sample_idx] = 0.0
+
+        # Record processing time
+        elapsed = time.perf_counter() - start_time
+        self.process_time_history.append(elapsed)
+        if len(self.process_time_history) > self.max_history_length:
+            self.process_time_history.pop(0)
+
+        # Calculate metrics (lightweight version for real-time)
+        self._updateMetrics(self.output_buffer)
+
+        return self.output_buffer.copy()
+
+    def _generatePhiModulation(self, phi_phase: float, phi_depth: float) -> np.ndarray:
+        """
+        Generate Φ-modulated envelope for one block
 
-                if websocket)
+        Args:
+            phi_phase: Phase offset [0, 2π]
+            phi_depth: Modulation depth [0, 1]
 
-                await self.update_phi_sync(delta_phase=np.mean(smoothed) * 0.01)
-                await asyncio.sleep(refresh_interval)
-        except asyncio.CancelledError)
-        except Exception as e:
-            logger.exception("Runtime error in real-time loop, e)
-        finally)
+        Returns:
+            float32[block_size] modulation envelope
+        """
+        # Golden ratio modulation frequency
+        # f_mod = sample_rate / Φ ≈ 29,665 Hz for 48kHz
+        phi_freq = self.sample_rate * self.PHI_INV
 
+        # Generate time vector for this block
+        t = np.arange(self.block_size) / self.sample_rate
 
+        # Φ-modulated sinusoid
+        modulation = 1.0 + phi_depth * np.sin(2 * np.pi * phi_freq * t + phi_phase)
 
+        return modulation.astype(np.float32)
 
+    def _updateMetrics(self, output: np.ndarray):
+        """
+        Calculate and update metrics from multi-channel output
 
+        Metrics calculated:
+        - ICI (Inter-Channel Interference): Cross-correlation between channels
+        - Phase Coherence: Phase alignment across channels
+        - Spectral Centroid: Center of mass of spectrum
+        - Consciousness Level: Composite metric
 
+        Args:
+            output: float32[num_channels, block_size] multi-channel signal
+        """
+        try:
+            # ICI: Use full spectral-phase integration engine (Feature 014)
+            ici_value, _ = self.ici_engine.process_block(output)
+            self.last_metrics['ici'] = ici_value
+
+            # Phase Coherence: Using Hilbert transform
+            # (Simplified for real-time: just measure phase variance)
+            phases = []
+            for ch in range(self.num_channels):
+                # Extract instantaneous phase via analytic signal
+                analytic = signal.hilbert(output[ch])
+                phase = np.angle(analytic)
+                phases.append(np.mean(phase))
+
+            phase_std = np.std(phases)
+            self.last_metrics['phase_coherence'] = max(0.0, 1.0 - phase_std / np.pi)
+
+            # Spectral Centroid: Weighted mean of frequencies
+            # Calculate for all channels combined
+            combined = np.mean(output, axis=0)
+            spectrum = np.abs(np.fft.rfft(combined))
+            freqs = np.fft.rfftfreq(self.block_size, 1.0 / self.sample_rate)
+
+            total_mag = np.sum(spectrum)
+            if total_mag > 0:
+                centroid = np.sum(spectrum * freqs) / total_mag
+            else:
+                centroid = 0.0
+            self.last_metrics['spectral_centroid'] = centroid
+
+            # Consciousness Level: Composite metric
+            # Balance of coherence (order), diversity (low ICI), and spectral richness
+            coherence_component = self.last_metrics['phase_coherence']
+            diversity_component = 1.0 - self.last_metrics['ici']
+            spectral_component = min(1.0, self.last_metrics['spectral_centroid'] / (self.sample_rate / 2))
+
+            consciousness = (
+                0.4 * coherence_component +
+                0.3 * diversity_component +
+                0.3 * spectral_component
+            )
+            self.last_metrics['consciousness_level'] = np.clip(consciousness, 0.0, 1.0)
 
-# ======================== END OF SECTION 2 ========================
-# (Next, adaptive learning, and performance diagnostics)
-# Paste below this divider when ready
-# ==========================================================
-# chromatic_field_processor_fixed.py  (Section 3/4)
-# ==========================================================
-# Metrics (ICI, phase coherence, centroid, criticality, state)
-# + Small, isolated Auto-Φ learner (optional)
-# ==========================================================
+        except Exception as e:
+            print(f"[ChromaticFieldProcessor] Warning: Metrics calculation failed: {e}")
+            # Keep last known values
 
-    # -----------------------------
-    # Utility: safe norms and helpers
-    # -----------------------------
-    @staticmethod
-    def _safe_norm(vec) :
-        if values.size == 0))
+    def getMetrics(self) -> Dict[str, float]:
+        """
+        Get latest calculated metrics
+
+        Returns:
+            Dictionary with keys:
+                - ici: Inter-channel interference [0, 1]
+                - phase_coherence: Phase alignment [0, 1]
+                - spectral_centroid: Frequency in Hz
+                - consciousness_level: Composite metric [0, 1]
+        """
+        return self.last_metrics.copy()
 
-    @staticmethod
-    def _phase_signs(frame) :
+    def getPerformanceStats(self) -> Dict[str, float]:
         """
-        Compute per-block metrics from an [frames x channels] array.
-        - outputs, shape (N, C). If mono, treat as (N,1).
+        Get processing performance statistics
+
+        Returns:
+            Dictionary with:
+                - avg_process_time_ms: Average processing time
+                - max_process_time_ms: Maximum processing time
+                - min_process_time_ms: Minimum processing time
+                - avg_cpu_load: Estimated CPU load [0, 1]
         """
-        if outputs is None:
-            return {"valid", "reason": "no_outputs"}
-
-        if outputs.ndim == 1, 1)
-
-        n_frames, n_channels = outputs.shape
-        if n_frames == 0 or n_channels == 0:
-            return {"valid", "reason", "sample_rate", 48_000))
-
-        # --- ICI (Integrated Chromatic Information) ---
-        # energy per channel (L2 norm), then pairwise normalized product
-        energies = np.sqrt(np.sum(outputs * outputs, axis=0))  # (C,)
-        mean_energy = float(np.mean(energies)) if energies.size else 0.0
-        ici_pairs = []
-        for i in range(n_channels), n_channels))
-                ici_pairs.append(num / den)
-        ici_raw = self._safe_mean(np.asarray(ici_pairs, dtype=np.float64))
-        # normalize ICI roughly to [0..1]
-        ici = float(np.clip(ici_raw / (mean_energy + 1e-6), 0.0, 1.0))
-
-        # --- Phase coherence (adjacent channels sign correlation) ---
-        # sign proxy in time-domain
-        signs = self._phase_signs(outputs)  # (N, C)
-        if n_channels > 1):
-                a = signs[:, c].astype(np.float64)
-                b = signs[:, c + 1].astype(np.float64)
-                # cosine similarity of ±1 streams equals mean(a*b)
-                corr = float(np.mean(a * b))
-                # map from [-1..1] :
-            spectral_centroid_hz = 0.0
-        else) / mag_sum)
-        spectral_centroid_norm = float(np.clip(spectral_centroid_hz / (sr / 2.0), 0.0, 1.0))
-
-        # --- Chromatic energy (aggregate amplitude) ---
-        chromatic_energy = float(np.mean(np.abs(outputs)))
-
-        # --- Consciousness state classification ---
-        state = self._classify_state(
-            ici=ici,
-            coherence=phase_coherence,
-            centroid_hz=spectral_centroid_hz,
-
-        # --- Criticality (edge-of-chaos ~ ICI≈0.5) ---
-        criticality = float(1.0 - 2.0 * abs(ici - 0.5))
-        criticality = float(np.clip(criticality, 0.0, 1.0))
-
-        # --- Aggregate & cache ---
-        metrics = {
-            "ici",
-            "phase_coherence",
-            "spectral_centroid",
-            "spectral_centroid_norm",
-            "chromatic_energy",
-            "criticality",
-            "consciousness_state",
-            "valid",
-        }
-        self._last_metrics = metrics
-        logger.debug("Metrics → %s", metrics)
-        return metrics
-
-    # ----------------------------
-    # Simple state classifier
-    # ----------------------------
-    @staticmethod
-    def _classify_state(ici, coherence, centroid_hz) :
-        # thresholds mirror your spec’s intent; adjust freely
-        if ici < 0.1 and coherence < 0.2:
-            return "COMA"
-        if centroid_hz < 10.0 and coherence < 0.4:
-            return "SLEEP"
-        if ici < 0.3 and coherence < 0.5:
-            return "DROWSY"
-        if 0.3 <= ici <= 0.7 and coherence >= 0.4:
-            return "AWAKE"
-        if ici > 0.7 and coherence > 0.7:
-            return "ALERT"
-        if ici > 0.9 and coherence > 0.9, isolated, easy to disable)
-    # ==========================================================
-    def set_auto_phi_enabled(self, enabled) :
+        if not self.process_time_history:
+            return {
+                'avg_process_time_ms': 0.0,
+                'max_process_time_ms': 0.0,
+                'min_process_time_ms': 0.0,
+                'avg_cpu_load': 0.0
+            }
+
+        times_ms = [t * 1000 for t in self.process_time_history]
+        block_time_ms = (self.block_size / self.sample_rate) * 1000
+
         return {
-            "avg_process_time_ms", 3),
-            "max_latency_ms", 3),
-            "block_count"),
+            'avg_process_time_ms': np.mean(times_ms),
+            'max_process_time_ms': np.max(times_ms),
+            'min_process_time_ms': np.min(times_ms),
+            'avg_cpu_load': np.mean(times_ms) / block_time_ms if block_time_ms > 0 else 0.0
         }
 
-    def get_last_metrics(self) :
-        """Return a snapshot combining performance and metrics."""
-        state = {
-            "performance"),
-            "metrics"),
-            "phi": {
-                "phase", 0.0)),
-                "depth", 0.0)),
-                "enabled", False)),
-            },
+    def reset(self):
+        """Reset all internal state and integrators"""
+        print("[ChromaticFieldProcessor] Resetting processor state")
+
+        # Reset D-ASE engine nodes
+        for node in self.engine.nodes:
+            node.reset_integrator()
+
+        # Reset metrics
+        self.last_metrics = {
+            'ici': 0.0,
+            'phase_coherence': 0.0,
+            'spectral_centroid': 0.0,
+            'consciousness_level': 0.0
         }
-        logger.debug("State summary → %s", state)
-        return state
 
-    # ------------------------------------------------------
-    # Validation / sanity self-test
-    # ------------------------------------------------------
-    def self_test(self) :
-        """
-        Runs a small offline test,
-        process through the pipeline, compute metrics, and verify stability.
-        """
-        logger.info("Running self-test sweep…")
-
-        # generate 1-second sine sweep
-        sr = self.sample_rate
-        t = np.linspace(0, 1.0, sr, endpoint=False)
-        sweep = np.sin(2 * np.pi * (220 * (t**2)))  # chirp-like
-        multi = np.stack([sweep * (i + 1) / self.num_channels for i in range(self.num_channels)])
-
-        processed = self.process_block(multi)
-        metrics = self.compute_metrics(processed)
-        self.auto_phi_step(metrics)
-
-        summary = self.get_state_summary()
-        logger.info("Self-test complete.  ICI=%.3f  coherence=%.3f",
-                    metrics.get("ici", 0.0), metrics.get("phase_coherence", 0.0))
-        return summary
-
-
-# ==========================================================
-# Standalone module check
-# ==========================================================
-if __name__ == "__main__", format="%(levelname)s | %(message)s")
-    proc = ChromaticFieldProcessor()
-    result = proc.self_test()
-    print("\n=== Self-Test Summary ===")
-    for k, v in result.items():
-        print(f"{k})
-    print("=========================\n")
-
-"""  # auto-closed missing docstring
+        # Clear performance history
+        self.process_time_history.clear()
+
+        # Clear output buffer
+        self.output_buffer.fill(0.0)
+
+    def __del__(self):
+        """Cleanup on destruction"""
+        print("[ChromaticFieldProcessor] Shutting down")
+
+
+# Self-test function
+def _self_test():
+    """Run basic self-test of ChromaticFieldProcessor"""
+    print("=" * 60)
+    print("ChromaticFieldProcessor Self-Test")
+    print("=" * 60)
+
+    if not DASE_AVAILABLE:
+        print("ERROR: D-ASE engine not available. Cannot run self-test.")
+        return False
+
+    try:
+        # Create processor
+        print("\n1. Initializing processor...")
+        processor = ChromaticFieldProcessor(num_channels=8, sample_rate=48000, block_size=512)
+        print("   ✓ Processor initialized")
+
+        # Generate test signal
+        print("\n2. Generating test signal (1kHz sine)...")
+        t = np.linspace(0, 512/48000, 512, endpoint=False)
+        test_signal = (np.sin(2 * np.pi * 1000 * t) * 0.5).astype(np.float32)
+        print(f"   ✓ Test signal generated: {test_signal.shape}")
+
+        # Process block
+        print("\n3. Processing test block...")
+        start = time.perf_counter()
+        output = processor.processBlock(test_signal, phi_phase=0.0, phi_depth=0.5)
+        elapsed_ms = (time.perf_counter() - start) * 1000
+        print(f"   ✓ Block processed in {elapsed_ms:.2f}ms")
+        print(f"   Output shape: {output.shape}")
+        print(f"   Output range: [{np.min(output):.3f}, {np.max(output):.3f}]")
+
+        # Check latency requirement
+        if elapsed_ms < 6.0:
+            print(f"   ✓ Latency OK ({elapsed_ms:.2f}ms < 6ms target)")
+        else:
+            print(f"   ⚠ Latency HIGH ({elapsed_ms:.2f}ms > 6ms target)")
+
+        # Get metrics
+        print("\n4. Checking metrics...")
+        metrics = processor.getMetrics()
+        for key, value in metrics.items():
+            print(f"   {key}: {value:.4f}")
+        print("   ✓ Metrics calculated")
+
+        # Performance stats
+        print("\n5. Performance statistics...")
+        perf = processor.getPerformanceStats()
+        for key, value in perf.items():
+            print(f"   {key}: {value:.4f}")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
+        import traceback
+        traceback.print_exc()
+        return False
+
+
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/chromatic_visualizer.py b/server/chromatic_visualizer.py
index 2f2bb84d187e87a7781b0072c67e7823d60a874a..c5e2014ac1b7bad6060188bb5a605ab63f283893 100644
--- a/server/chromatic_visualizer.py
+++ b/server/chromatic_visualizer.py
@@ -1,345 +1,597 @@
 """
 ChromaticVisualizer - Feature 016: Chromatic Consciousness Visualizer
 
 Synesthetic visualization linking 8-channel audio spectrum and Φ modulation
 to color-coded cognitive "chromatic" states.
 
 Features:
-- FR-001, amplitude → brightness in real time
-
+- FR-001: Map frequency → hue, amplitude → brightness in real time
+- FR-002: Apply Φ phase offset to color rotation (golden angle ±137.5°)
 - FR-003: Render ≥ 30 fps
 - FR-004: Show coherence and criticality links between channels
 - FR-005: UI toggle between modes
 
 Requirements:
 - SC-001: Color output matches channel frequencies within ±3 Hz
-
+- SC-002: Φ-breathing visible at correct period (< 2% error)
 - SC-003: Frame rate ≥ 30 fps continuous
-
+- SC-004: Coupling overlay reflects metric values accurately (±5%)
+- SC-005: CPU usage < 15% / GPU < 40%
+"""
 
 import time
 import math
 import numpy as np
 from typing import List, Dict, Optional, Tuple
 from dataclasses import dataclass, asdict
 from collections import deque
 import threading
 
 
 # Golden angle in degrees
 GOLDEN_ANGLE = 137.5077640500378
 
 
 @dataclass
 class ChannelChroma:
     """Chromatic state for a single channel"""
     channel_id: int
     frequency: float          # Hz
     amplitude: float          # 0-1
     hue: float               # 0-360 degrees
     saturation: float        # 0-1
-    lightness)
+    lightness: float         # 0-1 (brightness)
     phi_rotation: float      # Golden angle rotation from Φ
 
 
 @dataclass
 class ChromaticState:
     """Complete chromatic visualization state"""
     timestamp: float
 
     # Channel chromatic states
     channels: List[ChannelChroma]
 
     # Global Φ state
     phi_phase: float         # 0-2π
     phi_depth: float         # 0.618-1.618
-    phi_breathing)
+    phi_breathing: float     # Breathing cycle value (0-1)
 
     # Consciousness metrics
     ici: float
     coherence: float
-    criticality)
+    criticality: float
+
+    # Topology data (coupling between channels)
     coupling_matrix: List[List[float]]  # 8x8 matrix
-    coherence_links, to, strength}
+    coherence_links: List[Dict]          # List of {from, to, strength}
 
 
 @dataclass
 class VisualizerConfig:
     """Configuration for ChromaticVisualizer"""
     num_channels: int = 8
 
     # Frequency → Hue mapping
-    min_frequency)
-    max_frequency)
+    min_frequency: float = 20.0      # Hz (maps to hue 0°)
+    max_frequency: float = 2000.0    # Hz (maps to hue 360°)
     frequency_scale: str = "log"     # "linear" or "log"
 
     # Amplitude → Brightness mapping
     min_amplitude: float = 0.0       # Maps to lightness 0
     max_amplitude: float = 1.0       # Maps to lightness 1
     amplitude_gamma: float = 2.2     # Gamma correction for brightness
 
     # Φ modulation
     phi_rotation_enabled: bool = True
     phi_breathing_enabled: bool = True
-    phi_breathing_frequency)
+    phi_breathing_frequency: float = 1.5  # Hz (1-2 Hz typical)
 
     # Topology overlay
     coupling_threshold: float = 0.3  # Minimum coupling to show link
     coherence_threshold: float = 0.5  # Minimum coherence for strong link
 
     # Performance
     target_fps: int = 60
     enable_logging: bool = False
 
 
 class ColorMapper:
     """
     ColorMapper - Converts frequency/amplitude to HSL color space
 
+    Handles:
+    - Frequency → Hue mapping (FR-001)
+    - Amplitude → Lightness mapping (FR-001)
+    - Φ phase → Golden angle rotation (FR-002)
+    """
 
+    def __init__(self, config: VisualizerConfig):
+        """Initialize ColorMapper"""
+        self.config = config
 
-    """
+    def frequency_to_hue(self, frequency: float) -> float:
+        """
+        Convert frequency to hue (FR-001, SC-001)
 
-    def __init__(self, config: VisualizerConfig) :
+        Args:
             frequency: Frequency in Hz
 
+        Returns:
+            Hue in degrees (0-360)
         """
         # Clamp frequency to valid range
         freq = np.clip(frequency, self.config.min_frequency, self.config.max_frequency)
 
-        if self.config.frequency_scale == "log")
+        if self.config.frequency_scale == "log":
+            # Logarithmic mapping (better for audio)
             log_freq = np.log10(freq)
             log_min = np.log10(self.config.min_frequency)
             log_max = np.log10(self.config.max_frequency)
             normalized = (log_freq - log_min) / (log_max - log_min)
-        else) / (self.config.max_frequency - self.config.min_frequency)
+        else:
+            # Linear mapping
+            normalized = (freq - self.config.min_frequency) / (self.config.max_frequency - self.config.min_frequency)
 
         # Map to hue (0-360)
         hue = normalized * 360.0
 
         return hue
 
-    @lru_cache(maxsize=128)
-    def amplitude_to_lightness(self, amplitude) :
-            amplitude)
+    def amplitude_to_lightness(self, amplitude: float) -> float:
+        """
+        Convert amplitude to lightness with gamma correction (FR-001)
+
+        Args:
+            amplitude: Amplitude (0-1)
 
+        Returns:
+            Lightness (0-1)
         """
         # Clamp amplitude
         amp = np.clip(amplitude, 0.0, 1.0)
 
         # Apply gamma correction for perceptual brightness
         lightness = np.power(amp, 1.0 / self.config.amplitude_gamma)
 
         return lightness
 
-    @lru_cache(maxsize=128)
-    def apply_phi_rotation(self, base_hue, phi_phase) :
+    def apply_phi_rotation(self, base_hue: float, phi_phase: float) -> float:
+        """
+        Apply Φ phase offset using golden angle rotation (FR-002)
+
+        Args:
             base_hue: Base hue in degrees
-            phi_phase)
+            phi_phase: Φ phase (0-2π)
 
+        Returns:
+            Rotated hue in degrees (0-360)
         """
-        if not self.config.phi_rotation_enabled) → ±137.5° golden angle
+        if not self.config.phi_rotation_enabled:
+            return base_hue
+
+        # Convert Φ phase to rotation angle
+        # Full rotation (2π) → ±137.5° golden angle
         rotation = (phi_phase / (2 * np.pi)) * GOLDEN_ANGLE
 
         # Apply rotation
         rotated_hue = (base_hue + rotation) % 360.0
 
         return rotated_hue
 
 
 class PhiAnimator:
     """
     PhiAnimator - Applies golden-angle rotation & breathing cycle
 
-    Handles, SC-002)
-
+    Handles:
+    - Φ-breathing animation (User Story 2, SC-002)
+    - Golden angle rotation (FR-002)
     """
 
-    def __init__(self, config: VisualizerConfig) :
+    def __init__(self, config: VisualizerConfig):
+        """Initialize PhiAnimator"""
+        self.config = config
+        self.start_time = time.time()
+
+    def compute_breathing_cycle(self, current_time: float, phi_depth: float) -> float:
+        """
+        Compute Φ-breathing cycle value (User Story 2, SC-002)
+
+        Args:
             current_time: Current timestamp
-            phi_depth)
+            phi_depth: Φ depth (0.618-1.618)
 
+        Returns:
+            Breathing value (0-1)
         """
-        if not self.config.phi_breathing_enabled, no breathing
+        if not self.config.phi_breathing_enabled:
+            return 0.5  # Mid-level, no breathing
 
         # Elapsed time
         elapsed = current_time - self.start_time
 
         # Breathing frequency
         freq = self.config.phi_breathing_frequency
 
         # Sine wave pattern (User Story 2)
         # Normalized to 0-1 range
         breathing = 0.5 + 0.5 * np.sin(2 * np.pi * freq * elapsed)
 
         # Modulate by Φ depth (deeper Φ → stronger breathing)
         depth_factor = (phi_depth - 0.618) / (1.618 - 0.618)  # Normalize to 0-1
         breathing = 0.5 + (breathing - 0.5) * depth_factor
 
         return breathing
 
 
 class TopologyOverlay:
     """
     TopologyOverlay - Draws phase-coupling geometry
 
-    Handles, FR-004)
+    Handles:
+    - Coherence links between channels (User Story 3, FR-004)
     - Coupling matrix visualization
     """
 
-    def __init__(self, config: VisualizerConfig) :
+    def __init__(self, config: VisualizerConfig):
+        """Initialize TopologyOverlay"""
+        self.config = config
+
+    def compute_coherence_links(self, coupling_matrix: np.ndarray, coherence: float) -> List[Dict]:
+        """
+        Compute coherence links from coupling matrix (User Story 3, FR-004, SC-004)
+
+        Args:
             coupling_matrix: 8x8 coupling matrix
             coherence: Global coherence value
 
-        Returns, to, strength}
+        Returns:
+            List of link dictionaries with {from, to, strength}
         """
         links = []
         n = coupling_matrix.shape[0]
 
-        for i in range(n), n))
+        for i in range(n):
+            for j in range(i + 1, n):  # Upper triangle only (symmetric)
                 coupling_strength = abs(coupling_matrix[i, j])
 
                 # Filter by threshold
                 if coupling_strength >= self.config.coupling_threshold:
                     # Modulate by global coherence
                     link_strength = coupling_strength * coherence
 
                     links.append({
-                        "from",
-                        "to",
-                        "strength"),
-                        "width"),  # Line width proportional to coupling
-                        "intensity", 1.0))  # Glow intensity
+                        "from": i,
+                        "to": j,
+                        "strength": float(link_strength),
+                        "width": float(coupling_strength),  # Line width proportional to coupling
+                        "intensity": float(min(link_strength, 1.0))  # Glow intensity
                     })
 
         return links
 
-    @lru_cache(maxsize=128)
-    def compute_symmetry_ring(self, ici) :
+    def compute_symmetry_ring(self, ici: float) -> Dict:
+        """
+        Compute symmetry ring parameters based on ICI (User Story 3)
+
+        Args:
             ici: Integrated Chroma Intensity
 
-        Returns)  # 1.0 when ICI = 0.5, 0.0 when ICI = 0 or 1
+        Returns:
+            Ring parameters
+        """
+        # ICI ≈ 0.5 → perfect symmetry ring
+        symmetry_score = 1.0 - 2.0 * abs(ici - 0.5)  # 1.0 when ICI = 0.5, 0.0 when ICI = 0 or 1
 
         return {
-            "symmetry_score", 0.0, 1.0)),
-            "radius",  # Relative radius
+            "symmetry_score": float(np.clip(symmetry_score, 0.0, 1.0)),
+            "radius": 0.8,  # Relative radius
             "rotation": 0.0  # Could rotate based on Φ phase
         }
 
 
 class ChromaticVisualizer:
     """
     ChromaticVisualizer - Main visualization engine
 
-    Handles, config: Optional[VisualizerConfig]) :
-            channel_frequencies)
-            channel_amplitudes)
-            phi_phase)
-            phi_depth)
+    Handles:
+    - Real-time chromatic state computation
+    - Color mapping and Φ modulation
+    - Topology overlay generation
+    - State streaming for frontend
+    """
+
+    def __init__(self, config: Optional[VisualizerConfig] = None):
+        """Initialize ChromaticVisualizer"""
+        self.config = config or VisualizerConfig()
+
+        # Components
+        self.color_mapper = ColorMapper(self.config)
+        self.phi_animator = PhiAnimator(self.config)
+        self.topology_overlay = TopologyOverlay(self.config)
+
+        # State
+        self.current_state: Optional[ChromaticState] = None
+        self.state_lock = threading.Lock()
+
+        # Performance tracking
+        self.frame_count = 0
+        self.last_frame_time = time.time()
+        self.fps = 0.0
+        self.avg_frame_time_ms = 0.0
+
+        # State history (for smoothing)
+        self.state_history = deque(maxlen=10)
+
+    def update_state(self,
+                    channel_frequencies: List[float],
+                    channel_amplitudes: List[float],
+                    phi_phase: float,
+                    phi_depth: float,
+                    ici: float,
+                    coherence: float,
+                    criticality: float,
+                    coupling_matrix: Optional[np.ndarray] = None):
+        """
+        Update chromatic state from metrics (FR-001, FR-002, FR-004)
+
+        Args:
+            channel_frequencies: List of 8 channel frequencies (Hz)
+            channel_amplitudes: List of 8 channel amplitudes (0-1)
+            phi_phase: Φ phase (0-2π)
+            phi_depth: Φ depth (0.618-1.618)
             ici: Integrated Chroma Intensity
             coherence: Phase coherence
             criticality: Consciousness level
-            coupling_matrix)
+            coupling_matrix: 8x8 coupling matrix (optional)
         """
         frame_start = time.time()
 
         # Compute Φ-breathing cycle
         phi_breathing = self.phi_animator.compute_breathing_cycle(frame_start, phi_depth)
 
         # Compute channel chromatic states
         channels = []
-        for i in range(min(len(channel_frequencies), self.config.num_channels)))
+        for i in range(min(len(channel_frequencies), self.config.num_channels)):
+            freq = channel_frequencies[i]
+            amp = channel_amplitudes[i]
+
+            # Base hue from frequency (FR-001)
             base_hue = self.color_mapper.frequency_to_hue(freq)
 
             # Apply Φ rotation (FR-002)
             hue = self.color_mapper.apply_phi_rotation(base_hue, phi_phase)
 
             # Brightness from amplitude (FR-001)
             lightness = self.color_mapper.amplitude_to_lightness(amp)
 
             # Modulate brightness by Φ-breathing (User Story 2)
-            if self.config.phi_breathing_enabled)
+            if self.config.phi_breathing_enabled:
+                lightness = lightness * (0.5 + 0.5 * phi_breathing)
 
             channels.append(ChannelChroma(
                 channel_id=i,
                 frequency=freq,
                 amplitude=amp,
                 hue=hue,
                 saturation=1.0,  # Full saturation for vivid colors
                 lightness=lightness,
                 phi_rotation=hue - base_hue
             ))
 
         # Generate coupling matrix if not provided
-        if coupling_matrix is None, self.config.num_channels)
+        if coupling_matrix is None:
+            coupling_matrix = np.random.rand(self.config.num_channels, self.config.num_channels)
             coupling_matrix = (coupling_matrix + coupling_matrix.T) / 2  # Symmetric
 
         # Compute coherence links (FR-004)
         coherence_links = self.topology_overlay.compute_coherence_links(coupling_matrix, coherence)
 
         # Create chromatic state
-        with self.state_lock,
+        with self.state_lock:
+            self.current_state = ChromaticState(
+                timestamp=frame_start,
                 channels=channels,
                 phi_phase=phi_phase,
                 phi_depth=phi_depth,
                 phi_breathing=phi_breathing,
                 ici=ici,
                 coherence=coherence,
                 criticality=criticality,
                 coupling_matrix=coupling_matrix.tolist(),
                 coherence_links=coherence_links
+            )
 
             # Add to history
             self.state_history.append(self.current_state)
 
         # Update performance metrics (SC-003)
         frame_time = time.time() - frame_start
         self.frame_count += 1
 
         time_delta = time.time() - self.last_frame_time
-        if time_delta >= 1.0)
+        if time_delta >= 1.0:
+            self.fps = self.frame_count / time_delta
+            self.frame_count = 0
+            self.last_frame_time = time.time()
 
         self.avg_frame_time_ms = frame_time * 1000.0
 
-    def get_current_state(self) :
+    def get_current_state(self) -> Optional[Dict]:
         """
         Get current chromatic state as dictionary
 
         Returns:
             State dictionary or None
         """
         with self.state_lock:
-            if self.current_state)
+            if self.current_state:
+                return asdict(self.current_state)
             return None
 
-    def get_performance_stats(self) :
+    def get_performance_stats(self) -> Dict:
+        """
+        Get performance statistics (SC-003, SC-005)
+
+        Returns:
             Performance stats dictionary
         """
         return {
-            "fps",
-            "avg_frame_time_ms",
-            "target_fps",
-            "meets_sc003": self.fps >= 30.0  # SC-003) :3]:
-        logger.info("      ch%s <: ±3 Hz
+            "fps": self.fps,
+            "avg_frame_time_ms": self.avg_frame_time_ms,
+            "target_fps": self.config.target_fps,
+            "meets_sc003": self.fps >= 30.0  # SC-003: ≥ 30 fps
+        }
+
+
+# Self-test
+def _self_test():
+    """Run basic self-test of ChromaticVisualizer"""
+    print("=" * 60)
+    print("ChromaticVisualizer Self-Test")
+    print("=" * 60)
+    print()
+
+    all_ok = True
+
+    # Test 1: ColorMapper
+    print("1. Testing ColorMapper...")
+    config = VisualizerConfig()
+    mapper = ColorMapper(config)
+
+    # Test frequency → hue
+    hue_low = mapper.frequency_to_hue(20.0)  # Min frequency
+    hue_high = mapper.frequency_to_hue(2000.0)  # Max frequency
+    hue_mid = mapper.frequency_to_hue(200.0)  # Mid frequency
+
+    hue_ok = 0 <= hue_low < 30 and 330 < hue_high <= 360 and 100 < hue_mid < 200
+    all_ok = all_ok and hue_ok
+
+    print(f"   Hue mapping: 20Hz->{hue_low:.1f}deg, 200Hz->{hue_mid:.1f}deg, 2000Hz->{hue_high:.1f}deg")
+    print(f"   [{'OK' if hue_ok else 'FAIL'}] Frequency to Hue mapping (FR-001)")
+
+    # Test amplitude to lightness
+    lightness_zero = mapper.amplitude_to_lightness(0.0)
+    lightness_full = mapper.amplitude_to_lightness(1.0)
+
+    lightness_ok = lightness_zero == 0.0 and lightness_full == 1.0
+    all_ok = all_ok and lightness_ok
+
+    print(f"   Lightness mapping: amp=0->{lightness_zero:.2f}, amp=1->{lightness_full:.2f}")
+    print(f"   [{'OK' if lightness_ok else 'FAIL'}] Amplitude to Lightness mapping (FR-001)")
+
+    # Test Phi rotation
+    phi_rotation = mapper.apply_phi_rotation(180.0, np.pi)  # Half rotation
+    rotation_ok = 240 < phi_rotation < 260  # Expect ~248.75deg (180 + 137.5/2)
+    all_ok = all_ok and rotation_ok
+
+    print(f"   Phi rotation: 180deg + pi phase -> {phi_rotation:.1f}deg (expected ~249deg)")
+    print(f"   [{'OK' if rotation_ok else 'FAIL'}] Phi golden angle rotation (FR-002)")
+    print()
+
+    # Test 2: PhiAnimator
+    print("2. Testing PhiAnimator...")
+    animator = PhiAnimator(config)
+
+    # Test breathing cycle
+    time.sleep(0.1)
+    breathing = animator.compute_breathing_cycle(time.time(), 1.0)
+
+    breathing_ok = 0.0 <= breathing <= 1.0
+    all_ok = all_ok and breathing_ok
+
+    print(f"   Breathing value: {breathing:.3f}")
+    print(f"   [{'OK' if breathing_ok else 'FAIL'}] Phi-breathing cycle (User Story 2)")
+    print()
+
+    # Test 3: TopologyOverlay
+    print("3. Testing TopologyOverlay...")
+    overlay = TopologyOverlay(config)
+
+    # Create test coupling matrix
+    coupling = np.array([
+        [1.0, 0.8, 0.3, 0.1, 0.0, 0.0, 0.0, 0.0],
+        [0.8, 1.0, 0.9, 0.4, 0.0, 0.0, 0.0, 0.0],
+        [0.3, 0.9, 1.0, 0.7, 0.0, 0.0, 0.0, 0.0],
+        [0.1, 0.4, 0.7, 1.0, 0.0, 0.0, 0.0, 0.0],
+        [0.0, 0.0, 0.0, 0.0, 1.0, 0.6, 0.0, 0.0],
+        [0.0, 0.0, 0.0, 0.0, 0.6, 1.0, 0.5, 0.0],
+        [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.4],
+        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]
+    ])
+
+    links = overlay.compute_coherence_links(coupling, coherence=0.9)
+
+    links_ok = len(links) > 0
+    all_ok = all_ok and links_ok
+
+    print(f"   Coherence links: {len(links)} connections")
+    for link in links[:3]:
+        print(f"      ch{link['from']} <-> ch{link['to']}: strength={link['strength']:.2f}")
+    print(f"   [{'OK' if links_ok else 'FAIL'}] Coherence links (FR-004)")
+
+    # Test symmetry ring
+    ring = overlay.compute_symmetry_ring(ici=0.5)
+    ring_ok = ring['symmetry_score'] == 1.0  # Perfect symmetry at ICI=0.5
+    all_ok = all_ok and ring_ok
+
+    print(f"   Symmetry ring (ICI=0.5): score={ring['symmetry_score']:.2f}")
+    print(f"   [{'OK' if ring_ok else 'FAIL'}] Symmetry ring (User Story 3)")
+    print()
+
+    # Test 4: ChromaticVisualizer integration
+    print("4. Testing ChromaticVisualizer...")
+    visualizer = ChromaticVisualizer(config)
+
+    # Create test data
+    frequencies = [100, 200, 300, 400, 500, 600, 700, 800]  # Hz
+    amplitudes = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
+
+    # Update state
+    visualizer.update_state(
+        channel_frequencies=frequencies,
+        channel_amplitudes=amplitudes,
+        phi_phase=np.pi / 2,
+        phi_depth=1.0,
+        ici=0.5,
+        coherence=0.9,
+        criticality=1.0,
+        coupling_matrix=coupling
+    )
+
+    # Get state
+    state = visualizer.get_current_state()
+    state_ok = state is not None and len(state['channels']) == 8
+    all_ok = all_ok and state_ok
+
+    print(f"   State channels: {len(state['channels']) if state else 0}")
+    print(f"   [{'OK' if state_ok else 'FAIL'}] Chromatic state generation")
+
+    if state:
+        # Verify color mapping (SC-001)
+        ch0 = state['channels'][0]
+        freq_accuracy = abs(ch0['frequency'] - 100.0) <= 3.0  # SC-001: ±3 Hz
         all_ok = all_ok and freq_accuracy
 
-        logger.info("   Channel 0, hue=%s°, L=%s", ch0['frequency'], ch0['hue'], ch0['lightness'])
-        logger.error("   [%s] Color accuracy (SC-001)", 'OK' if freq_accuracy else 'FAIL')
+        print(f"   Channel 0: freq={ch0['frequency']:.1f}Hz, hue={ch0['hue']:.1f}°, L={ch0['lightness']:.2f}")
+        print(f"   [{'OK' if freq_accuracy else 'FAIL'}] Color accuracy (SC-001)")
 
     # Performance stats
     perf = visualizer.get_performance_stats()
-    logger.info("   Avg frame time, perf['avg_frame_time_ms'])
-    logger.info("   [OK] Performance tracking (SC-003)")
-    logger.info(str())
+    print(f"   Avg frame time: {perf['avg_frame_time_ms']:.2f} ms")
+    print(f"   [OK] Performance tracking (SC-003)")
+    print()
 
-    logger.info("=" * 60)
-    if all_ok)
-    else)
-    logger.info("=" * 60)
+    print("=" * 60)
+    if all_ok:
+        print("Self-Test PASSED")
+    else:
+        print("Self-Test FAILED - Review failures above")
+    print("=" * 60)
 
     return all_ok
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/cluster_monitor.py b/server/cluster_monitor.py
index b6759aa2541648e8074b5df46cb179b75d5dd52e..37cede7e5d17a5e8c948e03cc35341335e403439 100644
--- a/server/cluster_monitor.py
+++ b/server/cluster_monitor.py
@@ -1,557 +1,893 @@
 """
 Cluster Monitor - Feature 022
 Web dashboard and APIs to observe and manage multiple Soundlab/PhaseNet nodes.
 
+Features:
+- Real-time status aggregation from all nodes
+- Historical metrics with ring buffers (10 min @ 1Hz)
 - Health rules and status calculation
-
+- Management actions (promote, quarantine, restart)
 - RBAC for security
 - Audit logging
 - Topology persistence
 - WebSocket updates at 1-2Hz
 
 Requirements:
-- FR-001, history buffer, action controller
+- FR-001: Status aggregator, history buffer, action controller
 - FR-002: Collect per-node metrics
 - FR-003: REST API endpoints
 - FR-004: WebSocket /ws/cluster at 1-2Hz
-
+- FR-006: Ring buffers (600 samples @ 1Hz)
 - FR-007: RBAC for management actions
 - FR-008: Audit logging
 - FR-009: Health rules
 - FR-010: Topology persistence
 
 Success Criteria:
 - SC-001: Updates within 2s
 - SC-002: Actions complete with audit
 - SC-003: History with no gaps >5s
 - SC-004: CPU overhead <5%
+- SC-005: RBAC prevents unauthorized actions
+"""
 
 import json
 import time
 import os
 import hashlib
 import threading
 from typing import Optional, Dict, List, Any
 from dataclasses import dataclass, asdict
 from collections import deque
 from enum import Enum
 import psutil
 
 
-class NodeHealth(Enum))"""
+class NodeHealth(Enum):
+    """Node health status (FR-009)"""
     HEALTHY = "healthy"
     WARNING = "warning"
     CRITICAL = "critical"
     OFFLINE = "offline"
 
 
 class NodeRole(Enum):
     """Node role in cluster"""
     MASTER = "master"
     FOLLOWER = "follower"
     CANDIDATE = "candidate"
     STANDALONE = "standalone"
 
 
 @dataclass
 class ClusterMonitorConfig:
     """Configuration for cluster monitor"""
-    update_interval)
-    history_samples)
+    update_interval: float = 1.0  # Update frequency in Hz (FR-004)
+    history_samples: int = 600  # 10 min @ 1Hz (FR-006)
     health_check_interval: float = 2.0  # Health check frequency
     topology_file: str = "state/cluster_topology.json"  # FR-010
-    audit_log_dir)
+    audit_log_dir: str = "logs/cluster"  # FR-008
+    # Health thresholds (FR-009)
     rtt_warning_ms: float = 25.0
     drift_warning_ms: float = 1.0
     pkt_loss_warning_pct: float = 1.0
     pkt_loss_critical_pct: float = 10.0
     stale_timeout_s: float = 5.0
     debounce_interval_s: float = 3.0  # Status debounce
     enable_rbac: bool = True  # FR-007
     enable_logging: bool = True
 
 
 @dataclass
-class NodeMetrics)"""
+class NodeMetrics:
+    """Node metrics snapshot (FR-002)"""
     timestamp: float
     node_id: str
     role: str
     host: str
     port: int
     rtt_ms: float
     drift_ms: float
     phi_phase: float
     phi_depth: float
     coherence: float
     criticality: float
     pkt_loss_pct: float
     cpu_pct: float
     mem_pct: float
     uptime_s: float
     last_seen: float
 
 
 @dataclass
-class NodeStatus)"""
+class NodeStatus:
+    """Node status with health (FR-009)"""
     node_id: str
     role: str
     host: str
     port: int
     health: str
     health_reason: str
     last_seen: float
     uptime_s: float
     # Current metrics
     rtt_ms: float
     drift_ms: float
     phi_phase: float
     phi_depth: float
     coherence: float
     criticality: float
     pkt_loss_pct: float
     cpu_pct: float
     mem_pct: float
     # Flags
     is_stale: bool
     is_leader: bool
 
 
-class ClusterMonitor)
+class ClusterMonitor:
+    """
+    Cluster Monitor for Soundlab/PhaseNet nodes (Feature 022)
 
     Aggregates status, maintains history, provides management actions,
     and broadcasts updates via WebSocket.
     """
 
-    def __init__(self, config: Optional[ClusterMonitorConfig]) :
+    def __init__(self, config: Optional[ClusterMonitorConfig] = None):
         """
         Initialize Cluster Monitor
 
         Args:
-            config)
+            config: Monitor configuration
+        """
+        self.config = config or ClusterMonitorConfig()
 
         # Node tracking
-        self.nodes, NodeStatus] = {}
-        self.node_history, deque] = {}  # node_id :
-                logger.info("[ClusterMonitor] Admin token, self.admin_token)
+        self.nodes: Dict[str, NodeStatus] = {}
+        self.node_history: Dict[str, deque] = {}  # node_id -> deque of NodeMetrics (FR-006)
+        self.node_lock = threading.Lock()
+
+        # Health tracking
+        self.last_health_check = 0.0
+        self.status_changes: Dict[str, float] = {}  # node_id -> last status change time (debounce)
+
+        # Audit log (FR-008)
+        self.audit_log_path = None
+        if self.config.enable_logging:
+            os.makedirs(self.config.audit_log_dir, exist_ok=True)
+            self.audit_log_path = os.path.join(
+                self.config.audit_log_dir,
+                f"cluster_audit_{int(time.time())}.log"
+            )
+
+        # RBAC (FR-007)
+        self.access_tokens: set = set()  # Simple token-based auth
+        if self.config.enable_rbac:
+            # Generate default admin token
+            self.admin_token = self._generate_token("admin")
+            self.access_tokens.add(self.admin_token)
+
+        # WebSocket clients (FR-004)
+        self.ws_clients: List = []
+        self.ws_client_lock = threading.Lock()
+
+        # Background threads
+        self.is_running = False
+        self.aggregator_thread = None
+        self.health_check_thread = None
+
+        # Node synchronizer and PhaseNet references (set externally)
+        self.node_sync = None
+        self.phasenet = None
+
+        # Hardware interface reference (Feature 023, FR-008)
+        self.hw_interface = None
+
+        # Hybrid node bridge reference (Feature 024, FR-009)
+        self.hybrid_bridge = None
+
+        # Load persisted topology (FR-010)
+        self._load_topology()
+
+        if self.config.enable_logging:
+            print("[ClusterMonitor] Initialized")
+
+    def _generate_token(self, user: str) -> str:
+        """Generate access token for RBAC (FR-007)"""
+        import secrets
+        token_data = f"{user}:{time.time()}:{secrets.token_hex(16)}"
+        return hashlib.sha256(token_data.encode()).hexdigest()
+
+    def start(self):
+        """Start cluster monitor (FR-001)"""
+        if self.is_running:
+            return
 
-    def stop(self) :
+        self.is_running = True
+
+        # Start aggregator thread
+        self.aggregator_thread = threading.Thread(target=self._aggregator_loop, daemon=True)
+        self.aggregator_thread.start()
+
+        # Start health check thread
+        self.health_check_thread = threading.Thread(target=self._health_check_loop, daemon=True)
+        self.health_check_thread.start()
+
+        if self.config.enable_logging:
+            print("[ClusterMonitor] Started")
+            if self.config.enable_rbac:
+                print(f"[ClusterMonitor] Admin token: {self.admin_token}")
+
+    def stop(self):
         """Stop cluster monitor"""
-        if not self.is_running)
+        if not self.is_running:
+            return
+
+        self.is_running = False
+
+        # Save topology (FR-010)
         self._save_topology()
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[ClusterMonitor] Stopped")
 
-    def _aggregator_loop(self) :
+    def _aggregator_loop(self):
         """
-        Aggregation loop, FR-002, SC-001)
+        Aggregation loop: collect metrics from nodes (FR-001, FR-002, SC-001)
 
         Runs at configured update_interval (1-2 Hz).
         """
         while self.is_running:
-            try)
+            try:
+                current_time = time.time()
 
                 # Collect from Node Synchronizer (Feature 020)
-                if self.node_sync and self.node_sync.is_running)
+                if self.node_sync and self.node_sync.is_running:
+                    self._collect_from_node_sync(current_time)
 
                 # Collect from PhaseNet (Feature 021)
-                if self.phasenet and self.phasenet.is_running)
+                if self.phasenet and self.phasenet.is_running:
+                    self._collect_from_phasenet(current_time)
 
                 # Collect from Hardware Interface (Feature 023, FR-008)
-                if self.hw_interface and self.hw_interface.is_connected)
+                if self.hw_interface and self.hw_interface.is_connected:
+                    self._collect_from_hardware(current_time)
 
                 # Collect from Hybrid Analog-DSP Node (Feature 024, FR-009)
-                if self.hybrid_bridge and self.hybrid_bridge.is_connected)
+                if self.hybrid_bridge and self.hybrid_bridge.is_connected:
+                    self._collect_from_hybrid(current_time)
 
                 # Broadcast updates via WebSocket (FR-004, SC-001)
                 self._broadcast_updates()
 
                 # Wait for next interval
                 time.sleep(self.config.update_interval)
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[ClusterMonitor] Aggregator error, e)
+                    print(f"[ClusterMonitor] Aggregator error: {e}")
 
-    def _collect_from_node_sync(self, current_time: float) :
-                for client in status['clients'], 'unknown')
+    def _collect_from_node_sync(self, current_time: float):
+        """Collect metrics from Node Synchronizer (FR-002)"""
+        try:
+            status = self.node_sync.get_status()
+            stats = self.node_sync.get_statistics()
+
+            # Get local node info
+            node_id = status.get('node_id', 'unknown')
+            role = status.get('state', 'unknown')
+            is_leader = status.get('is_leader', False)
+
+            # Get system metrics
+            cpu_pct = psutil.cpu_percent(interval=None)
+            mem = psutil.virtual_memory()
+            uptime_s = time.time() - psutil.boot_time()
+
+            # Create metrics snapshot
+            metrics = NodeMetrics(
+                timestamp=current_time,
+                node_id=node_id,
+                role=role,
+                host='localhost',  # Local node
+                port=0,  # Not applicable for local
+                rtt_ms=0.0,  # Local node has no RTT
+                drift_ms=0.0,
+                phi_phase=0.0,  # Would need to get from auto_phi_learner
+                phi_depth=0.0,
+                coherence=stats.get('coherence', {}).get('mean', 0.0) if 'coherence' in stats else 0.0,
+                criticality=1.0,
+                pkt_loss_pct=0.0,
+                cpu_pct=cpu_pct,
+                mem_pct=mem.percent,
+                uptime_s=uptime_s,
+                last_seen=current_time
+            )
+
+            self._update_node_metrics(metrics, is_leader)
+
+            # Collect from connected clients
+            if 'clients' in status:
+                for client in status['clients']:
+                    client_id = client.get('id', 'unknown')
                     # Would need more detailed metrics from clients
                     # For now, create basic entry
                     client_metrics = NodeMetrics(
                         timestamp=current_time,
                         node_id=client_id,
                         role='client',
                         host=client.get('address', 'unknown'),
                         port=0,
                         rtt_ms=0.0,
                         drift_ms=0.0,
                         phi_phase=0.0,
                         phi_depth=0.0,
                         coherence=0.0,
                         criticality=1.0,
                         pkt_loss_pct=0.0,
                         cpu_pct=0.0,
                         mem_pct=0.0,
                         uptime_s=0.0,
                         last_seen=client.get('last_seen', current_time)
-
+                    )
                     self._update_node_metrics(client_metrics, False)
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Error collecting from NodeSync, e)
+                print(f"[ClusterMonitor] Error collecting from NodeSync: {e}")
 
-    def _collect_from_phasenet(self, current_time: float) :
-                for peer in status['peers'], 'unknown')
+    def _collect_from_phasenet(self, current_time: float):
+        """Collect metrics from PhaseNet (FR-002)"""
+        try:
+            status = self.phasenet.get_status()
+            stats = self.phasenet.get_statistics()
+
+            # Get local node info
+            node_id = status.get('node_id', 'unknown')
+            role = status.get('state', 'unknown')
+            is_leader = status.get('is_leader', False)
+
+            # Get system metrics
+            cpu_pct = psutil.cpu_percent(interval=None)
+            mem = psutil.virtual_memory()
+            uptime_s = time.time() - psutil.boot_time()
+
+            # Create metrics snapshot
+            metrics = NodeMetrics(
+                timestamp=current_time,
+                node_id=node_id,
+                role=role,
+                host='localhost',
+                port=0,
+                rtt_ms=stats.get('latency', {}).get('mean_ms', 0.0) if 'latency' in stats else 0.0,
+                drift_ms=0.0,  # PhaseNet doesn't track drift same way
+                phi_phase=0.0,  # Would need from local state
+                phi_depth=0.0,
+                coherence=stats.get('coherence', {}).get('mean', 0.0) if 'coherence' in stats else 0.0,
+                criticality=1.0,
+                pkt_loss_pct=stats.get('packets', {}).get('drop_rate', 0.0) * 100 if 'packets' in stats else 0.0,
+                cpu_pct=cpu_pct,
+                mem_pct=mem.percent,
+                uptime_s=uptime_s,
+                last_seen=current_time
+            )
+
+            self._update_node_metrics(metrics, is_leader)
+
+            # Collect from peers
+            if 'peers' in status:
+                for peer in status['peers']:
+                    peer_id = peer.get('node_id', 'unknown')
                     peer_metrics = NodeMetrics(
                         timestamp=current_time,
                         node_id=peer_id,
                         role='follower' if not peer.get('is_leader', False) else 'leader',
                         host=peer.get('address', 'unknown'),
                         port=0,
                         rtt_ms=peer.get('latency_ms', 0.0),
                         drift_ms=0.0,
                         phi_phase=0.0,
                         phi_depth=0.0,
                         coherence=1.0 - peer.get('phase_diff', 0.0),  # Convert diff to coherence
                         criticality=1.0,
                         pkt_loss_pct=0.0,
                         cpu_pct=0.0,
                         mem_pct=0.0,
                         uptime_s=0.0,
                         last_seen=peer.get('last_seen', current_time)
-
+                    )
                     self._update_node_metrics(peer_metrics, peer.get('is_leader', False))
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Error collecting from PhaseNet, e)
+                print(f"[ClusterMonitor] Error collecting from PhaseNet: {e}")
+
+    def _collect_from_hardware(self, current_time: float):
+        """Collect metrics from Hardware Interface (Feature 023, FR-008)"""
+        try:
+            stats = self.hw_interface.get_statistics()
+
+            # Create hardware node metrics entry
+            hw_metrics = NodeMetrics(
+                timestamp=current_time,
+                node_id="hardware_bridge",
+                role="hardware",
+                host=self.hw_interface.port or "auto",
+                port=self.hw_interface.baudrate,
+                rtt_ms=stats.get('latency_us', 0) / 1000.0,  # Convert µs to ms
+                drift_ms=stats.get('clock_drift_ppm', 0.0) / 1000.0,  # Approximate conversion
+                phi_phase=0.0,  # Hardware doesn't provide these directly
+                phi_depth=0.0,
+                coherence=1.0 - stats.get('loss_rate', 0.0),  # Convert loss to coherence
+                criticality=1.0,
+                pkt_loss_pct=stats.get('loss_rate', 0.0) * 100,
+                cpu_pct=0.0,  # Not applicable for hardware
+                mem_pct=0.0,
+                uptime_s=stats.get('uptime_ms', 0) / 1000.0,  # Convert ms to s
+                last_seen=current_time
+            )
+
+            # Track as hardware node (not a leader)
+            self._update_node_metrics(hw_metrics, is_leader=False)
 
-    def _collect_from_hardware(self, current_time: float) :
+        except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Error collecting from Hardware, e)
+                print(f"[ClusterMonitor] Error collecting from Hardware: {e}")
+
+    def _collect_from_hybrid(self, current_time: float):
+        """Collect metrics from Hybrid Analog-DSP Node (Feature 024, FR-009)"""
+        try:
+            # Get DSP metrics (ICI, coherence, spectral analysis)
+            dsp = self.hybrid_bridge.get_dsp_metrics()
+
+            # Get safety telemetry
+            safety = self.hybrid_bridge.get_safety()
+
+            # Get node statistics
+            stats = self.hybrid_bridge.get_statistics()
+
+            # Create hybrid node metrics entry
+            hybrid_metrics = NodeMetrics(
+                timestamp=current_time,
+                node_id="hybrid_analog_dsp",
+                role="hybrid",
+                host=self.hybrid_bridge.port or "auto",
+                port=self.hybrid_bridge.baudrate,
+                rtt_ms=stats.get('total_latency_us', 0) / 1000.0,  # Convert µs to ms (SC-001)
+                drift_ms=stats.get('drift_ppm', 0.0) / 1000.0,  # Clock drift from calibration
+                phi_phase=0.0,  # Hybrid node doesn't track Φ-phase directly yet
+                phi_depth=0.0,
+                coherence=dsp.get('coherence', 0.0),  # Phase coherence from DSP analysis
+                criticality=dsp.get('criticality', 1.0),  # Criticality metric
+                pkt_loss_pct=0.0,  # Not applicable for direct serial
+                cpu_pct=stats.get('cpu_load', 0.0),  # CPU load from embedded processor
+                mem_pct=stats.get('buffer_utilization', 0.0),  # DMA buffer utilization
+                uptime_s=stats.get('uptime_ms', 0) / 1000.0,  # Convert ms to s
+                last_seen=current_time
+            )
+
+            # Track as hybrid node (not a leader)
+            self._update_node_metrics(hybrid_metrics, is_leader=False)
 
-    def _collect_from_hybrid(self, current_time: float) :
+        except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Error collecting from Hybrid, e)
+                print(f"[ClusterMonitor] Error collecting from Hybrid: {e}")
+
+    def _update_node_metrics(self, metrics: NodeMetrics, is_leader: bool):
+        """
+        Update node metrics and history (FR-006, SC-003)
 
-    @lru_cache(maxsize=128)
-    def _update_node_metrics(self, metrics: NodeMetrics, is_leader: bool) :
+        Args:
             metrics: Node metrics snapshot
             is_leader: Whether node is leader
         """
-        with self.node_lock)
-            if node_id not in self.node_history)
+        with self.node_lock:
+            node_id = metrics.node_id
+
+            # Initialize history buffer if needed (FR-006)
+            if node_id not in self.node_history:
+                self.node_history[node_id] = deque(maxlen=self.config.history_samples)
 
             # Add to history
             self.node_history[node_id].append(metrics)
 
             # Calculate health (FR-009)
             health, health_reason = self._calculate_health(metrics)
 
             # Check if stale
             is_stale = (time.time() - metrics.last_seen) > self.config.stale_timeout_s
 
             # Update node status
             self.nodes[node_id] = NodeStatus(
                 node_id=node_id,
                 role=metrics.role,
                 host=metrics.host,
                 port=metrics.port,
                 health=health.value,
                 health_reason=health_reason,
                 last_seen=metrics.last_seen,
                 uptime_s=metrics.uptime_s,
                 rtt_ms=metrics.rtt_ms,
                 drift_ms=metrics.drift_ms,
                 phi_phase=metrics.phi_phase,
                 phi_depth=metrics.phi_depth,
                 coherence=metrics.coherence,
                 criticality=metrics.criticality,
                 pkt_loss_pct=metrics.pkt_loss_pct,
                 cpu_pct=metrics.cpu_pct,
                 mem_pct=metrics.mem_pct,
                 is_stale=is_stale,
                 is_leader=is_leader
+            )
+
+    def _calculate_health(self, metrics: NodeMetrics) -> tuple[NodeHealth, str]:
+        """
+        Calculate node health based on rules (FR-009)
 
-    @lru_cache(maxsize=128)
-    def _calculate_health(self, metrics) :
+        Args:
             metrics: Node metrics
 
-        Returns, reason)
+        Returns:
+            Tuple of (health_status, reason)
         """
         current_time = time.time()
         age = current_time - metrics.last_seen
 
         # Critical: offline or high packet loss
-        if age > self.config.stale_timeout_s, f"Offline (last seen {age)")
+        if age > self.config.stale_timeout_s:
+            return (NodeHealth.CRITICAL, f"Offline (last seen {age:.1f}s ago)")
 
-        if metrics.pkt_loss_pct > self.config.pkt_loss_critical_pct, f"Packet loss {metrics.pkt_loss_pct)
+        if metrics.pkt_loss_pct > self.config.pkt_loss_critical_pct:
+            return (NodeHealth.CRITICAL, f"Packet loss {metrics.pkt_loss_pct:.1f}%")
 
         # Warning: thresholds exceeded
         warnings = []
         if metrics.rtt_ms > self.config.rtt_warning_ms:
-            warnings.append(f"RTT {metrics.rtt_ms)
+            warnings.append(f"RTT {metrics.rtt_ms:.1f}ms")
         if metrics.drift_ms > self.config.drift_warning_ms:
-            warnings.append(f"Drift {metrics.drift_ms)
+            warnings.append(f"Drift {metrics.drift_ms:.1f}ms")
         if metrics.pkt_loss_pct > self.config.pkt_loss_warning_pct:
-            warnings.append(f"Pkt loss {metrics.pkt_loss_pct)
+            warnings.append(f"Pkt loss {metrics.pkt_loss_pct:.1f}%")
 
-        if warnings, ", ".join(warnings))
+        if warnings:
+            return (NodeHealth.WARNING, ", ".join(warnings))
 
         # Healthy
         return (NodeHealth.HEALTHY, "All metrics nominal")
 
-    def _health_check_loop(self) :
-        """Health check loop)"""
+    def _health_check_loop(self):
+        """Health check loop: mark stale nodes (FR-009)"""
         while self.is_running:
-            try)
+            try:
+                current_time = time.time()
 
-                with self.node_lock, status in list(self.nodes.items():
+                with self.node_lock:
+                    for node_id, status in list(self.nodes.items()):
                         age = current_time - status.last_seen
 
                         # Mark as stale if timeout exceeded
                         if age > self.config.stale_timeout_s:
                             status.is_stale = True
                             status.health = NodeHealth.OFFLINE.value
-                            status.health_reason = f"Offline ({age)"
+                            status.health_reason = f"Offline ({age:.1f}s)"
 
                             # Optionally remove very old nodes
-                            if age > 60.0)
+                            if age > 60.0:  # Remove after 1 minute offline
+                                del self.nodes[node_id]
+
+                time.sleep(self.config.health_check_interval)
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[ClusterMonitor] Health check error, e)
+                    print(f"[ClusterMonitor] Health check error: {e}")
 
-    def _broadcast_updates(self) :
+    def _broadcast_updates(self):
+        """Broadcast updates to WebSocket clients (FR-004, SC-001)"""
+        with self.ws_client_lock:
             if not self.ws_clients:
                 return
 
             # Create update message
             update = {
-                "type",
-                "timestamp"),
-                "nodes")
+                "type": "cluster_update",
+                "timestamp": time.time(),
+                "nodes": self.get_nodes_list()
             }
 
             # Broadcast to all clients
-            # Note) :
+            # Note: Actual WebSocket send would be in main.py
+            # This is a placeholder for the data structure
+
+    def get_nodes_list(self) -> List[Dict]:
+        """
+        Get list of all nodes (FR-003)
+
+        Returns:
             List of node status dictionaries
         """
-        with self.node_lock) for status in self.nodes.values()]
+        with self.node_lock:
+            return [asdict(status) for status in self.nodes.values()]
+
+    def get_node_detail(self, node_id: str) -> Optional[Dict]:
+        """
+        Get detailed node info with history (FR-003, SC-003)
 
-    def get_node_detail(self, node_id) :
+        Args:
             node_id: Node identifier
 
-        Returns, or None if not found
+        Returns:
+            Node detail dictionary with history, or None if not found
         """
         with self.node_lock:
-            if node_id not in self.nodes, []))
+            if node_id not in self.nodes:
+                return None
+
+            status = self.nodes[node_id]
+            history = list(self.node_history.get(node_id, []))
 
             return {
-                "status"),
-                "history") for m in history],
-                "history_count"),
-                "history_duration_s") if len(history) > 1 else 0
+                "status": asdict(status),
+                "history": [asdict(m) for m in history],
+                "history_count": len(history),
+                "history_duration_s": (history[-1].timestamp - history[0].timestamp) if len(history) > 1 else 0
             }
 
-    def promote_node(self, node_id, token) :
+    def promote_node(self, node_id: str, token: Optional[str] = None) -> Dict:
+        """
+        Promote node to master (FR-003, SC-002)
+
+        Args:
             node_id: Node to promote
-            token)
+            token: Authorization token (FR-007)
 
-        Returns, SC-005)
-        if self.config.enable_rbac and token not in self.access_tokens, node_id, False, "Unauthorized")
-            return {"ok", "message": "Unauthorized"}
+        Returns:
+            Result dictionary
+        """
+        # Check RBAC (FR-007, SC-005)
+        if self.config.enable_rbac and token not in self.access_tokens:
+            self._audit_log("promote", node_id, False, "Unauthorized")
+            return {"ok": False, "message": "Unauthorized"}
 
         # Check node exists
         if node_id not in self.nodes:
-            return {"ok", "message", SC-002)
+            return {"ok": False, "message": "Node not found"}
+
+        # Audit log (FR-008, SC-002)
         self._audit_log("promote", node_id, True, "Success")
 
         # Implementation would depend on NodeSync/PhaseNet APIs
         # For now, return success
         return {
-            "ok",
-            "message",
-            "action",
-            "timestamp")
+            "ok": True,
+            "message": f"Node {node_id} promoted",
+            "action": "promote",
+            "timestamp": time.time()
         }
 
-    def quarantine_node(self, node_id, token) :
+    def quarantine_node(self, node_id: str, token: Optional[str] = None) -> Dict:
+        """
+        Quarantine node (FR-003, SC-002)
+
+        Args:
             node_id: Node to quarantine
-            token)
+            token: Authorization token (FR-007)
 
-        Returns, SC-005)
-        if self.config.enable_rbac and token not in self.access_tokens, node_id, False, "Unauthorized")
-            return {"ok", "message": "Unauthorized"}
+        Returns:
+            Result dictionary
+        """
+        # Check RBAC (FR-007, SC-005)
+        if self.config.enable_rbac and token not in self.access_tokens:
+            self._audit_log("quarantine", node_id, False, "Unauthorized")
+            return {"ok": False, "message": "Unauthorized"}
 
         # Check node exists
         if node_id not in self.nodes:
-            return {"ok", "message", SC-002)
+            return {"ok": False, "message": "Node not found"}
+
+        # Audit log (FR-008, SC-002)
         self._audit_log("quarantine", node_id, True, "Success")
 
         return {
-            "ok",
-            "message",
-            "action",
-            "timestamp")
+            "ok": True,
+            "message": f"Node {node_id} quarantined",
+            "action": "quarantine",
+            "timestamp": time.time()
         }
 
-    def restart_node(self, node_id, token) :
+    def restart_node(self, node_id: str, token: Optional[str] = None) -> Dict:
+        """
+        Restart node (FR-003, SC-002)
+
+        Args:
             node_id: Node to restart
-            token)
+            token: Authorization token (FR-007)
 
-        Returns, SC-005)
-        if self.config.enable_rbac and token not in self.access_tokens, node_id, False, "Unauthorized")
-            return {"ok", "message": "Unauthorized"}
+        Returns:
+            Result dictionary
+        """
+        # Check RBAC (FR-007, SC-005)
+        if self.config.enable_rbac and token not in self.access_tokens:
+            self._audit_log("restart", node_id, False, "Unauthorized")
+            return {"ok": False, "message": "Unauthorized"}
 
         # Check node exists
         if node_id not in self.nodes:
-            return {"ok", "message", SC-002)
+            return {"ok": False, "message": "Node not found"}
+
+        # Audit log (FR-008, SC-002)
         self._audit_log("restart", node_id, True, "Success")
 
         return {
-            "ok",
-            "message",
-            "action",
-            "timestamp")
+            "ok": True,
+            "message": f"Node {node_id} restart requested",
+            "action": "restart",
+            "timestamp": time.time()
         }
 
-    def _audit_log(self, action: str, node_id: str, success: bool, message: str) :
+    def _audit_log(self, action: str, node_id: str, success: bool, message: str):
+        """
+        Write audit log entry (FR-008, SC-002)
+
+        Args:
             action: Action performed
             node_id: Target node
             success: Whether action succeeded
             message: Result message
         """
         if not self.config.enable_logging or not self.audit_log_path:
             return
 
         try:
             entry = {
-                "timestamp"),
-                "action",
-                "node_id",
-                "success",
-                "message", 'a') as f) + '\n')
+                "timestamp": time.time(),
+                "action": action,
+                "node_id": node_id,
+                "success": success,
+                "message": message
+            }
+
+            with open(self.audit_log_path, 'a') as f:
+                f.write(json.dumps(entry) + '\n')
 
         except Exception as e:
-            logger.error("[ClusterMonitor] Audit log error, e)
+            print(f"[ClusterMonitor] Audit log error: {e}")
+
+    def _save_topology(self):
+        """Save cluster topology to file (FR-010)"""
+        try:
+            os.makedirs(os.path.dirname(self.config.topology_file), exist_ok=True)
 
-    def _save_topology(self) :
+            with self.node_lock:
                 topology = {
-                    "timestamp"),
-                    "nodes")
+                    "timestamp": time.time(),
+                    "nodes": self.get_nodes_list()
                 }
 
-            with open(self.config.topology_file, 'w') as f, f, indent=2)
+            with open(self.config.topology_file, 'w') as f:
+                json.dump(topology, f, indent=2)
 
-            if self.config.enable_logging, self.config.topology_file)
+            if self.config.enable_logging:
+                print(f"[ClusterMonitor] Topology saved to {self.config.topology_file}")
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Topology save error, e)
+                print(f"[ClusterMonitor] Topology save error: {e}")
+
+    def _load_topology(self):
+        """Load cluster topology from file (FR-010)"""
+        try:
+            if not os.path.exists(self.config.topology_file):
+                return
+
+            with open(self.config.topology_file, 'r') as f:
+                topology = json.load(f)
+
+            # Note: We don't restore full state, just log it was loaded
+            if self.config.enable_logging:
+                node_count = len(topology.get('nodes', []))
+                print(f"[ClusterMonitor] Loaded topology ({node_count} nodes)")
 
-    def _load_topology(self) :
+        except Exception as e:
             if self.config.enable_logging:
-                logger.error("[ClusterMonitor] Topology load error, e)
+                print(f"[ClusterMonitor] Topology load error: {e}")
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict[str, Any]:
         """
         Get cluster statistics
 
         Returns:
             Statistics dictionary
         """
-        with self.node_lock) if n.health == NodeHealth.HEALTHY.value)
+        with self.node_lock:
+            healthy_count = sum(1 for n in self.nodes.values() if n.health == NodeHealth.HEALTHY.value)
             warning_count = sum(1 for n in self.nodes.values() if n.health == NodeHealth.WARNING.value)
             critical_count = sum(1 for n in self.nodes.values() if n.health == NodeHealth.CRITICAL.value)
             leader_count = sum(1 for n in self.nodes.values() if n.is_leader)
 
             return {
-                "total_nodes"),
-                "healthy_nodes",
-                "warning_nodes",
-                "critical_nodes",
-                "offline_nodes") if n.is_stale),
-                "leader_count",
-                "history_samples") for h in self.node_history.values()),
+                "total_nodes": len(self.nodes),
+                "healthy_nodes": healthy_count,
+                "warning_nodes": warning_count,
+                "critical_nodes": critical_count,
+                "offline_nodes": sum(1 for n in self.nodes.values() if n.is_stale),
+                "leader_count": leader_count,
+                "history_samples": sum(len(h) for h in self.node_history.values()),
                 "update_interval": self.config.update_interval
             }
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("Cluster Monitor Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("Cluster Monitor Self-Test")
+    print("=" * 60)
 
     # Test initialization
-    logger.info("\n1. Testing initialization...")
+    print("\n1. Testing initialization...")
     config = ClusterMonitorConfig(
         enable_logging=True,
         enable_rbac=True
-
+    )
     monitor = ClusterMonitor(config)
-    logger.info("   OK)
-    logger.info("   Admin token, monitor.admin_token)
+    print("   OK: Initialization")
+    print(f"   Admin token: {monitor.admin_token}")
 
     # Test metrics update
-    logger.info("\n2. Testing metrics update...")
+    print("\n2. Testing metrics update...")
     # Skip all psutil calls for test to avoid Windows blocking issues
     metrics = NodeMetrics(
         timestamp=time.time(),
         node_id="test_node_1",
         role="master",
         host="localhost",
         port=9000,
         rtt_ms=2.5,
         drift_ms=0.5,
         phi_phase=0.5,
         phi_depth=0.8,
         coherence=0.95,
         criticality=1.0,
         pkt_loss_pct=0.5,
         cpu_pct=15.0,
         mem_pct=45.0,
         uptime_s=1000.0,
         last_seen=time.time()
-
+    )
     monitor._update_node_metrics(metrics, is_leader=True)
-    logger.info("   OK)
+    print("   OK: Metrics updated")
 
     # Test node list
-    logger.info("\n3. Testing node list...")
+    print("\n3. Testing node list...")
     nodes = monitor.get_nodes_list()
-    logger.info("   OK, len(nodes))
+    print(f"   OK: {len(nodes)} nodes listed")
 
     # Test node detail
-    logger.info("\n4. Testing node detail...")
+    print("\n4. Testing node detail...")
     detail = monitor.get_node_detail("test_node_1")
-    logger.info("   OK: Detail retrieved (history)", detail['history_count'])
+    print(f"   OK: Detail retrieved (history: {detail['history_count']} samples)")
 
     # Test health rules
-    logger.info("\n5. Testing health rules...")
+    print("\n5. Testing health rules...")
     health, reason = monitor._calculate_health(metrics)
-    logger.info("   OK, Reason=%s", health.value, reason)
+    print(f"   OK: Health={health.value}, Reason={reason}")
 
     # Test RBAC
-    logger.info("\n6. Testing RBAC...")
+    print("\n6. Testing RBAC...")
     result = monitor.promote_node("test_node_1", token="invalid_token")
-    logger.info("   OK)", result['ok'])
+    print(f"   OK: Unauthorized blocked (ok={result['ok']})")
 
     result = monitor.promote_node("test_node_1", token=monitor.admin_token)
-    logger.info("   OK)", result['ok'])
+    print(f"   OK: Authorized succeeded (ok={result['ok']})")
 
     # Test statistics
-    logger.info("\n7. Testing statistics...")
+    print("\n7. Testing statistics...")
     stats = monitor.get_statistics()
-    logger.info("   OK, healthy=%s)", stats['total_nodes'], stats['healthy_nodes'])
+    print(f"   OK: Stats (nodes={stats['total_nodes']}, healthy={stats['healthy_nodes']})")
 
     # Test topology persistence
-    logger.info("\n8. Testing topology persistence...")
+    print("\n8. Testing topology persistence...")
     monitor._save_topology()
-    logger.info("   OK)
-
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("   OK: Topology saved")
 
-"""  # auto-closed missing docstring
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
diff --git a/server/config/performance_profile.json b/server/config/performance_profile.json
new file mode 100644
index 0000000000000000000000000000000000000000..1a7eac1a207297d8548cff09e2e65a5f6cc58193
--- /dev/null
+++ b/server/config/performance_profile.json
@@ -0,0 +1,9 @@
+{
+  "target_fps": 45,
+  "audio_buffer_ms": 10,
+  "visual_complexity_level": 5,
+  "enable_phi_breathing": true,
+  "enable_topology_links": true,
+  "enable_gradients": true,
+  "render_resolution": 1.0
+}
diff --git a/server/core/__pycache__/__init__.cpython-311.pyc b/server/core/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index 891b2e4041561d2825449a1d2f0608741326082e..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 315
zcmYjMy-EZz5KdNQC9)Paf>qip^Z`UHv=y7PIu3^1CYKGHO_n4pXyrRt_y}U_<JkHF
z3kw!kIYAGdfthb+zWIBtDn?M+uc>)Q|CxiW&|feeB6uN^EQlrxD(RfeipOVm?StY`
zg?klv?YrJP<x*tqt}&7KqHcv&Sa3-g7kO$FUk^@-4q3wT)nKeNPV<%ZbsNvwDW7j=
zxwX+l5XpFl=_?Wc_E-@k=5D;!xf#d*<KE7Sj}k!o8XyB<Ex;R@@*|t$pHr7Lp1lMx
t&Ln`d{$aC!(Rn#obs0+J<*(xjnsJ9xIu<3}9Z5Dv<e+*gvxYq^e*>z*UDW^p

diff --git a/server/core/config.py b/server/core/config.py
index f949f06e02783b6db4abc64f7ff7a89a441b5146..91dd5f14492976b3486b60445310849cf9982317 100644
--- a/server/core/config.py
+++ b/server/core/config.py
@@ -1,132 +1,222 @@
 """
 Configuration Management for Soundlab Server
 
 This module defines all configuration dataclasses used throughout the server.
 Separates configuration from implementation logic.
 """
 
 import logging
 from dataclasses import dataclass, field
 from typing import Optional, List
 from pathlib import Path
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class ServerConfig:
     """Main server configuration"""
     host: str = "0.0.0.0"
     port: int = 8000
     enable_cors: bool = True
-    enable_logging) :
-        logger.info("Server config: %s)", self.host, self.port, self.enable_cors)
+    enable_logging: bool = True
+
+    def __post_init__(self) -> None:
+        logger.info("Server config: %s:%d (CORS=%s)", self.host, self.port, self.enable_cors)
 
 
 @dataclass
 class AudioConfig:
     """Audio processing configuration"""
     input_device: Optional[int] = None
     output_device: Optional[int] = None
     sample_rate: int = 48000
-    buffer_size) :
-        logger.info("Audio config, self.sample_rate, self.buffer_size)
+    buffer_size: int = 512
+
+    def __post_init__(self) -> None:
+        logger.info("Audio config: %dHz @ %d samples", self.sample_rate, self.buffer_size)
 
 
 @dataclass
 class FeatureFlags:
     """Feature enablement flags"""
 
     # Adaptive features
     auto_phi: bool = False
     criticality_balancer: bool = False
     state_memory: bool = False
     state_classifier: bool = False
     predictive_model: bool = False
 
     # Recording/playback
     session_recorder: bool = True
     timeline_player: bool = True
     data_exporter: bool = True
 
     # Networking
     node_sync: bool = False
     phasenet: bool = False
     cluster_monitor: bool = False
 
     # Hardware
     hardware_bridge: bool = False
     hybrid_bridge: bool = False
-    hybrid_node) :
+    hybrid_node: bool = False
+
+    def __post_init__(self) -> None:
+        enabled = [k for k, v in self.__dict__.items() if v]
+        logger.info("Enabled features: %s", ", ".join(enabled) if enabled else "none")
+
+
+@dataclass
+class NodeSyncConfig:
     """Node synchronization configuration"""
     enabled: bool = False
     role: str = "master"  # 'master' or 'client'
-    master_url) :
+    master_url: Optional[str] = None
+
+    def __post_init__(self) -> None:
         if self.enabled:
-            logger.info("Node sync, master=%s", self.role, self.master_url)
+            logger.info("Node sync: role=%s, master=%s", self.role, self.master_url)
 
 
 @dataclass
 class PhaseNetConfig:
     """PhaseNet protocol configuration"""
     enabled: bool = False
     port: int = 9000
-    encryption_key) :
+    encryption_key: Optional[str] = None
+
+    def __post_init__(self) -> None:
         if self.enabled:
             encrypted = "encrypted" if self.encryption_key else "unencrypted"
-            logger.info("PhaseNet)", self.port, encrypted)
+            logger.info("PhaseNet: port=%d (%s)", self.port, encrypted)
 
 
 @dataclass
 class HardwareConfig:
     """Hardware interface configuration"""
 
     # Hardware bridge
     bridge_enabled: bool = False
     bridge_port: Optional[str] = None
-    bridge_baudrate)
+    bridge_baudrate: int = 115200
+
+    # Hybrid bridge (analog-DSP)
     hybrid_bridge_enabled: bool = False
     hybrid_bridge_port: Optional[str] = None
     hybrid_bridge_baudrate: int = 115200
 
     # Hybrid node
     hybrid_node_enabled: bool = False
     hybrid_node_input_device: Optional[int] = None
-    hybrid_node_output_device) :
+    hybrid_node_output_device: Optional[int] = None
+
+    def __post_init__(self) -> None:
+        if any([self.bridge_enabled, self.hybrid_bridge_enabled, self.hybrid_node_enabled]):
+            logger.info("Hardware interfaces enabled")
+
+
+@dataclass
+class SoundlabConfig:
     """Complete Soundlab server configuration"""
 
-    server)
-    audio)
-    features)
-    node_sync)
-    phasenet)
-    hardware)
+    server: ServerConfig = field(default_factory=ServerConfig)
+    audio: AudioConfig = field(default_factory=AudioConfig)
+    features: FeatureFlags = field(default_factory=FeatureFlags)
+    node_sync: NodeSyncConfig = field(default_factory=NodeSyncConfig)
+    phasenet: PhaseNetConfig = field(default_factory=PhaseNetConfig)
+    hardware: HardwareConfig = field(default_factory=HardwareConfig)
 
     @classmethod
-    def from_args(cls, args) :
-            errors.append(f"Invalid port)")
+    def from_args(cls, args) -> 'SoundlabConfig':
+        """Create configuration from command-line arguments"""
+        return cls(
+            server=ServerConfig(
+                host=args.host,
+                port=args.port,
+                enable_cors=True,
+                enable_logging=not args.no_logging if hasattr(args, 'no_logging') else True
+            ),
+            audio=AudioConfig(
+                input_device=args.input_device if hasattr(args, 'input_device') else None,
+                output_device=args.output_device if hasattr(args, 'output_device') else None
+            ),
+            features=FeatureFlags(
+                auto_phi=args.enable_auto_phi if hasattr(args, 'enable_auto_phi') else False,
+                criticality_balancer=args.enable_criticality_balancer if hasattr(args, 'enable_criticality_balancer') else False,
+                state_memory=args.enable_state_memory if hasattr(args, 'enable_state_memory') else False,
+                state_classifier=args.enable_state_classifier if hasattr(args, 'enable_state_classifier') else False,
+                predictive_model=args.enable_predictive_model if hasattr(args, 'enable_predictive_model') else False,
+                session_recorder=not args.disable_session_recorder if hasattr(args, 'disable_session_recorder') else True,
+                timeline_player=not args.disable_timeline_player if hasattr(args, 'disable_timeline_player') else True,
+                data_exporter=not args.disable_data_exporter if hasattr(args, 'disable_data_exporter') else True,
+                node_sync=args.enable_node_sync if hasattr(args, 'enable_node_sync') else False,
+                phasenet=args.enable_phasenet if hasattr(args, 'enable_phasenet') else False,
+                cluster_monitor=args.enable_cluster_monitor if hasattr(args, 'enable_cluster_monitor') else False,
+                hardware_bridge=args.enable_hardware_bridge if hasattr(args, 'enable_hardware_bridge') else False,
+                hybrid_bridge=args.enable_hybrid_bridge if hasattr(args, 'enable_hybrid_bridge') else False,
+                hybrid_node=args.enable_hybrid_node if hasattr(args, 'enable_hybrid_node') else False
+            ),
+            node_sync=NodeSyncConfig(
+                enabled=args.enable_node_sync if hasattr(args, 'enable_node_sync') else False,
+                role=args.node_sync_role if hasattr(args, 'node_sync_role') else "master",
+                master_url=args.node_sync_master_url if hasattr(args, 'node_sync_master_url') else None
+            ),
+            phasenet=PhaseNetConfig(
+                enabled=args.enable_phasenet if hasattr(args, 'enable_phasenet') else False,
+                port=args.phasenet_port if hasattr(args, 'phasenet_port') else 9000,
+                encryption_key=args.phasenet_key if hasattr(args, 'phasenet_key') else None
+            ),
+            hardware=HardwareConfig(
+                bridge_enabled=args.enable_hardware_bridge if hasattr(args, 'enable_hardware_bridge') else False,
+                bridge_port=args.hardware_port if hasattr(args, 'hardware_port') else None,
+                bridge_baudrate=args.hardware_baudrate if hasattr(args, 'hardware_baudrate') else 115200,
+                hybrid_bridge_enabled=args.enable_hybrid_bridge if hasattr(args, 'enable_hybrid_bridge') else False,
+                hybrid_bridge_port=args.hybrid_port if hasattr(args, 'hybrid_port') else None,
+                hybrid_bridge_baudrate=args.hybrid_baudrate if hasattr(args, 'hybrid_baudrate') else 115200,
+                hybrid_node_enabled=args.enable_hybrid_node if hasattr(args, 'enable_hybrid_node') else False,
+                hybrid_node_input_device=args.hybrid_node_input_device if hasattr(args, 'hybrid_node_input_device') else None,
+                hybrid_node_output_device=args.hybrid_node_output_device if hasattr(args, 'hybrid_node_output_device') else None
+            )
+        )
+
+    def validate(self) -> List[str]:
+        """Validate configuration and return list of errors"""
+        errors = []
+
+        # Validate port range
+        if not (1024 <= self.server.port <= 65535):
+            errors.append(f"Invalid port: {self.server.port} (must be 1024-65535)")
 
         # Validate node sync
         if self.node_sync.enabled and self.node_sync.role == "client":
-            if not self.node_sync.master_url)
+            if not self.node_sync.master_url:
+                errors.append("Node sync client requires master_url")
 
         # Validate phasenet
-        if self.phasenet.enabled):
-                errors.append(f"Invalid PhaseNet port)
+        if self.phasenet.enabled:
+            if not (1024 <= self.phasenet.port <= 65535):
+                errors.append(f"Invalid PhaseNet port: {self.phasenet.port}")
 
         return errors
 
-    def log_summary(self) : %s, self.server.host, self.server.port)
-        logger.info("Audio, self.audio.sample_rate, self.audio.buffer_size)
+    def log_summary(self) -> None:
+        """Log configuration summary"""
+        logger.info("=" * 60)
+        logger.info("Soundlab Server Configuration")
+        logger.info("=" * 60)
+        logger.info("Server: %s:%d", self.server.host, self.server.port)
+        logger.info("Audio: %dHz @ %d samples", self.audio.sample_rate, self.audio.buffer_size)
 
         enabled_features = [k for k, v in self.features.__dict__.items() if v]
         if enabled_features:
-            logger.info("Features, ", ".join(enabled_features))
+            logger.info("Features: %s", ", ".join(enabled_features))
 
         if self.node_sync.enabled:
-            logger.info("Node Sync, self.node_sync.role)
+            logger.info("Node Sync: %s", self.node_sync.role)
 
         if self.phasenet.enabled:
-            logger.info("PhaseNet, self.phasenet.port)
+            logger.info("PhaseNet: port %d", self.phasenet.port)
 
         logger.info("=" * 60)
diff --git a/server/correlation_analyzer.py b/server/correlation_analyzer.py
index 1066a0108c1a7e74dd36bec70599d96a8f874510..b6268c1a19a3313cfa4f6d63a0fadebc1287a84d 100644
--- a/server/correlation_analyzer.py
+++ b/server/correlation_analyzer.py
@@ -1,286 +1,363 @@
 """
 CorrelationAnalyzer - Feature 015: Multi-Session Comparative Analytics
 
 Computes cross-session correlation matrices and heatmaps.
 
 Features:
 - FR-004: Generate correlation heatmaps and summary tables
 - SC-003: Correlation accuracy >= 0.95 against NumPy baseline
 - User Story 3: Cross-session correlation analysis
 
 Requirements:
 - FR-004: System MUST generate correlation heatmaps
+- SC-003: Correlation accuracy >= 0.95
+"""
 
 import numpy as np
 from typing import List, Dict, Optional, Tuple
 from dataclasses import dataclass
 import json
 
 
 @dataclass
 class CorrelationMatrix:
     """Correlation matrix result"""
-    metric_name, "coherence", "criticality", or "phi"
+    metric_name: str              # "ici", "coherence", "criticality", or "phi"
     session_ids: List[str]        # List of session IDs
     matrix: List[List[float]]     # NxN correlation matrix
     is_symmetric: bool            # Verification flag
     diagonal_ones: bool           # Verification flag
 
 
 class CorrelationAnalyzer:
     """
     CorrelationAnalyzer - Cross-session correlation analysis
 
-
-
+    Handles:
+    - Computing correlation matrices (FR-004)
+    - Generating heatmap data (FR-004)
+    - Accuracy validation (SC-003)
     """
 
-    def __init__(self) :
+    def __init__(self):
         """Initialize CorrelationAnalyzer"""
-        self.sessions_data, Dict] = {}
+        self.sessions_data: Dict[str, Dict] = {}
 
-    @lru_cache(maxsize=128)
-    def load_session(self, session_id: str, session_data: Dict) :
+    def load_session(self, session_id: str, session_data: Dict):
         """
         Load a session for correlation analysis
 
         Args:
             session_id: Session identifier
-            session_data)
-    def compute_correlation_matrix(self, metric) :
-            metric, "coherence", "criticality", "phi")
+            session_data: Session data from StateRecorder
+        """
+        self.sessions_data[session_id] = session_data
+
+    def compute_correlation_matrix(self, metric: str = "ici") -> Optional[CorrelationMatrix]:
+        """
+        Compute correlation matrix for a metric across all sessions (FR-004)
+
+        Args:
+            metric: Metric to correlate ("ici", "coherence", "criticality", "phi")
 
-        Returns) < 2:
+        Returns:
+            CorrelationMatrix or None if failed
+        """
+        if len(self.sessions_data) < 2:
             return None
 
-        try))
+        try:
+            session_ids = list(self.sessions_data.keys())
             n_sessions = len(session_ids)
 
             # Extract metric time series for each session
             metric_key_map = {
-                "ici",
-                "coherence",
-                "criticality",
-                "phi", "ici")
+                "ici": "ici",
+                "coherence": "coherence",
+                "criticality": "criticality",
+                "phi": "phi_value"
+            }
+
+            metric_key = metric_key_map.get(metric, "ici")
 
             time_series = {}
-            for session_id in session_ids, [])
+            for session_id in session_ids:
+                samples = self.sessions_data[session_id].get("samples", [])
                 values = np.array([s.get(metric_key, 0.5) for s in samples])
                 time_series[session_id] = values
 
             # Find minimum length for alignment
             min_length = min(len(ts) for ts in time_series.values())
 
             # Truncate all series to minimum length
             aligned_series = {
-                sid: ts[, ts in time_series.items()
+                sid: ts[:min_length]
+                for sid, ts in time_series.items()
             }
 
             # Build correlation matrix (SC-003)
             corr_matrix = np.zeros((n_sessions, n_sessions))
 
-            for i, sid_i in enumerate(session_ids), sid_j in enumerate(session_ids):
-                    if i == j, j] = 1.0  # Diagonal is always 1.0
-                    else,
+            for i, sid_i in enumerate(session_ids):
+                for j, sid_j in enumerate(session_ids):
+                    if i == j:
+                        corr_matrix[i, j] = 1.0  # Diagonal is always 1.0
+                    else:
+                        # Compute Pearson correlation
+                        corr = np.corrcoef(
+                            aligned_series[sid_i],
                             aligned_series[sid_j]
                         )[0, 1]
                         corr_matrix[i, j] = float(corr)
 
             # Verify matrix properties
             is_symmetric = np.allclose(corr_matrix, corr_matrix.T, atol=1e-6)
             diagonal_ones = np.allclose(np.diag(corr_matrix), 1.0, atol=1e-6)
 
             # Verify bounds [-1, 1]
             all_bounded = np.all((corr_matrix >= -1.0) & (corr_matrix <= 1.0))
 
             if not all_bounded:
-                logger.warning("[CorrelationAnalyzer] Warning)
+                print("[CorrelationAnalyzer] Warning: Correlation values out of bounds")
 
             return CorrelationMatrix(
                 metric_name=metric,
                 session_ids=session_ids,
                 matrix=corr_matrix.tolist(),
                 is_symmetric=is_symmetric,
                 diagonal_ones=diagonal_ones
+            )
 
         except Exception as e:
-            logger.error("[CorrelationAnalyzer] Error computing correlation, e)
+            print(f"[CorrelationAnalyzer] Error computing correlation: {e}")
             return None
 
-    @lru_cache(maxsize=128)
-    def compute_all_correlations(self) :
+    def compute_all_correlations(self) -> Dict[str, CorrelationMatrix]:
         """
         Compute correlation matrices for all metrics
 
-        Returns, "coherence", "criticality", "phi"]
+        Returns:
+            Dictionary mapping metric names to correlation matrices
+        """
+        metrics = ["ici", "coherence", "criticality", "phi"]
         results = {}
 
-        for metric in metrics)
-            if matrix)
-    def get_heatmap_data(self, metric) :
+        for metric in metrics:
+            matrix = self.compute_correlation_matrix(metric)
+            if matrix:
+                results[metric] = matrix
+
+        return results
+
+    def get_heatmap_data(self, metric: str = "ici") -> Optional[Dict]:
+        """
+        Get heatmap visualization data (FR-004)
+
+        Args:
             metric: Metric to generate heatmap for
 
+        Returns:
+            Dictionary with heatmap data for visualization
+        """
+        corr_matrix = self.compute_correlation_matrix(metric)
+
         if not corr_matrix:
             return None
 
         return {
-            "metric",
-            "session_ids",
-            "matrix",
-            "is_symmetric",
-            "diagonal_ones",
-            "min_value")),
-            "max_value")),
-            "mean_off_diagonal"))
+            "metric": metric,
+            "session_ids": corr_matrix.session_ids,
+            "matrix": corr_matrix.matrix,
+            "is_symmetric": corr_matrix.is_symmetric,
+            "diagonal_ones": corr_matrix.diagonal_ones,
+            "min_value": float(np.min(corr_matrix.matrix)),
+            "max_value": float(np.max(corr_matrix.matrix)),
+            "mean_off_diagonal": float(np.mean([
+                corr_matrix.matrix[i][j]
+                for i in range(len(corr_matrix.session_ids))
                 for j in range(len(corr_matrix.session_ids))
                 if i != j
             ])) if len(corr_matrix.session_ids) > 1 else 0.0
         }
 
-    @lru_cache(maxsize=128)
-    def validate_accuracy(self, baseline_matrix, computed_matrix) :
+    def validate_accuracy(self, baseline_matrix: np.ndarray, computed_matrix: CorrelationMatrix) -> float:
+        """
+        Validate correlation accuracy against baseline (SC-003)
+
+        Args:
             baseline_matrix: Ground truth correlation matrix
             computed_matrix: Computed correlation matrix
 
+        Returns:
+            Accuracy score (0-1)
         """
         computed = np.array(computed_matrix.matrix)
 
         # Compute mean absolute error
         mae = np.mean(np.abs(baseline_matrix - computed))
 
         # Convert to accuracy (1.0 = perfect match)
         accuracy = 1.0 - mae
 
         return accuracy
 
-    @lru_cache(maxsize=128)
-    def get_summary_table(self) :
-                if i < j, [])
+    def get_summary_table(self) -> List[Dict]:
+        """
+        Generate summary statistics table (FR-004)
+
+        Returns:
+            List of summary statistics for each session pair
+        """
+        summary = []
+
+        session_ids = list(self.sessions_data.keys())
+
+        for i, sid_a in enumerate(session_ids):
+            for j, sid_b in enumerate(session_ids):
+                if i < j:  # Only compute once per pair
+                    # Get metrics for both sessions
+                    samples_a = self.sessions_data[sid_a].get("samples", [])
                     samples_b = self.sessions_data[sid_b].get("samples", [])
 
                     # Compute pairwise statistics
                     min_len = min(len(samples_a), len(samples_b))
 
-                    icis_a = np.array([s.get("ici", 0.5) for s in samples_a[)
-                    icis_b = np.array([s.get("ici", 0.5) for s in samples_b[)
+                    icis_a = np.array([s.get("ici", 0.5) for s in samples_a[:min_len]])
+                    icis_b = np.array([s.get("ici", 0.5) for s in samples_b[:min_len]])
 
                     ici_corr = np.corrcoef(icis_a, icis_b)[0, 1] if min_len > 1 else 0.0
 
                     summary.append({
-                        "session_a",
-                        "session_b",
-                        "ici_correlation"),
-                        "sample_count")
+                        "session_a": sid_a,
+                        "session_b": sid_b,
+                        "ici_correlation": float(ici_corr),
+                        "sample_count": min_len
+                    })
 
         return summary
 
 
 # Self-test
-@lru_cache(maxsize=128)
-def _self_test() : str, phase_offset: float) :
+def _self_test():
+    """Run basic self-test of CorrelationAnalyzer"""
+    print("=" * 60)
+    print("CorrelationAnalyzer Self-Test")
+    print("=" * 60)
+    print()
+
+    import time
+
+    # Create synthetic sessions with known correlations
+    print("1. Creating synthetic sessions...")
+
+    def create_session(session_id: str, phase_offset: float):
+        samples = []
+        for i in range(100):
             # Create sinusoidal pattern with phase offset
             samples.append({
-                "timestamp") + i * 0.1,
-                "ici"),
-                "coherence",
-                "criticality",
-                "phi_value"),
-                "phi_phase",
-                "phi_depth",
-                "active_source")
+                "timestamp": time.time() + i * 0.1,
+                "ici": 0.5 + 0.1 * np.sin(i * 0.1 + phase_offset),
+                "coherence": 0.6,
+                "criticality": 0.4,
+                "phi_value": 1.0 + 0.2 * np.sin(i * 0.1 + phase_offset),
+                "phi_phase": 0.0,
+                "phi_depth": 0.5,
+                "active_source": "test"
+            })
         return {
-            "metadata": {"session_id",
+            "metadata": {"session_id": session_id},
             "samples": samples
         }
 
     # Create 3 sessions:
-    # - session_1 and session_2)
-    # - session_3)
+    # - session_1 and session_2: identical (correlation = 1.0)
+    # - session_3: phase shifted (correlation < 1.0)
     session_1 = create_session("session_1", 0.0)
     session_2 = create_session("session_2", 0.0)  # Identical to session_1
     session_3 = create_session("session_3", np.pi / 2)  # 90 degree phase shift
 
-    logger.info("   [OK] Created 3 synthetic sessions")
-    logger.info(str())
+    print("   [OK] Created 3 synthetic sessions")
+    print()
 
     # Create analyzer
-    logger.info("2. Creating CorrelationAnalyzer...")
+    print("2. Creating CorrelationAnalyzer...")
     analyzer = CorrelationAnalyzer()
-    logger.info("   [OK] CorrelationAnalyzer created")
-    logger.info(str())
+    print("   [OK] CorrelationAnalyzer created")
+    print()
 
     # Load sessions
-    logger.info("3. Loading sessions...")
+    print("3. Loading sessions...")
     analyzer.load_session("session_1", session_1)
     analyzer.load_session("session_2", session_2)
     analyzer.load_session("session_3", session_3)
-    logger.info("   [OK] Loaded 3 sessions")
-    logger.info(str())
+    print("   [OK] Loaded 3 sessions")
+    print()
 
     # Compute correlation matrix
-    logger.info("4. Computing correlation matrix for ICI...")
+    print("4. Computing correlation matrix for ICI...")
     corr_matrix = analyzer.compute_correlation_matrix("ici")
 
     if corr_matrix:
-        logger.info("   Correlation Matrix)
+        print("   Correlation Matrix:")
         for i, sid in enumerate(corr_matrix.session_ids):
             row_str = "   " + sid + ": "
-            row_str += " ".join(f"{corr_matrix.matrix[i][j])))
-            logger.info(str(row_str))
+            row_str += " ".join(f"{corr_matrix.matrix[i][j]:6.3f}" for j in range(len(corr_matrix.session_ids)))
+            print(row_str)
 
         # Verify properties
-        logger.info("   Symmetric, corr_matrix.is_symmetric)
-        logger.info("   Diagonal ones, corr_matrix.diagonal_ones)
+        print(f"   Symmetric: {corr_matrix.is_symmetric}")
+        print(f"   Diagonal ones: {corr_matrix.diagonal_ones}")
 
         # Check expected correlations
         # session_1 vs session_2 should be ~1.0
         corr_1_2 = corr_matrix.matrix[0][1]
         identical_ok = abs(corr_1_2 - 1.0) < 0.01
 
-        logger.error("   [%s] Identical sessions correlation)", 'OK' if identical_ok else 'FAIL', corr_1_2)
+        print(f"   [{'OK' if identical_ok else 'FAIL'}] Identical sessions correlation: {corr_1_2:.3f} (expected ~1.0)")
 
         # Check symmetry
         symmetric_ok = corr_matrix.is_symmetric
         diagonal_ok = corr_matrix.diagonal_ones
 
-        logger.error("   [%s] Matrix is symmetric", 'OK' if symmetric_ok else 'FAIL')
-        logger.error("   [%s] Diagonal is ones", 'OK' if diagonal_ok else 'FAIL')
-    logger.info(str())
+        print(f"   [{'OK' if symmetric_ok else 'FAIL'}] Matrix is symmetric")
+        print(f"   [{'OK' if diagonal_ok else 'FAIL'}] Diagonal is ones")
+    print()
 
     # Get heatmap data
-    logger.info("5. Generating heatmap data...")
+    print("5. Generating heatmap data...")
     heatmap = analyzer.get_heatmap_data("ici")
     if heatmap:
-        logger.info("   Min value, heatmap['min_value'])
-        logger.info("   Max value, heatmap['max_value'])
-        logger.info("   Mean off-diagonal, heatmap['mean_off_diagonal'])
-        logger.info("   [OK] Heatmap data generated")
-    logger.info(str())
+        print(f"   Min value: {heatmap['min_value']:.3f}")
+        print(f"   Max value: {heatmap['max_value']:.3f}")
+        print(f"   Mean off-diagonal: {heatmap['mean_off_diagonal']:.3f}")
+        print("   [OK] Heatmap data generated")
+    print()
 
     # Validate accuracy (SC-003)
-    logger.info("6. Validating accuracy (SC-003)...")
+    print("6. Validating accuracy (SC-003)...")
     # Create baseline (should match computed for synthetic data)
     baseline = np.array(corr_matrix.matrix)
     accuracy = analyzer.validate_accuracy(baseline, corr_matrix)
     accuracy_ok = accuracy >= 0.95
 
-    logger.info("   Accuracy, accuracy)
-    logger.error("   [%s] Accuracy >= 0.95 (SC-003)", 'OK' if accuracy_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Accuracy: {accuracy:.4f}")
+    print(f"   [{'OK' if accuracy_ok else 'FAIL'}] Accuracy >= 0.95 (SC-003)")
+    print()
 
     # Get summary table
-    logger.info("7. Generating summary table...")
+    print("7. Generating summary table...")
     summary = analyzer.get_summary_table()
-    logger.info("   [OK] Summary table with %s entries", len(summary))
+    print(f"   [OK] Summary table with {len(summary)} entries")
     for entry in summary:
-        logger.info("      %s vs %s, entry['session_a'], entry['session_b'], entry['ici_correlation'])
-    logger.info(str())
+        print(f"      {entry['session_a']} vs {entry['session_b']}: r={entry['ici_correlation']:.3f}")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/criticality_balancer.py b/server/criticality_balancer.py
index 005ac9355ed3f090524ecb5b9aa28b4a37ad72b5..5d118fdc35fe506223a0cc9ba050ce8fd61d4c9c 100644
--- a/server/criticality_balancer.py
+++ b/server/criticality_balancer.py
@@ -1,426 +1,597 @@
 """
 Criticality Balancer - Adaptive Coupling and Amplitude Control
-Feature 012)
+Feature 012: Maintains system stability at edge of chaos (criticality ~= 1.0)
 
-Extends Auto-Phi Learner with, FR-004)
+Extends Auto-Phi Learner with:
+- Adaptive coupling matrix adjustment (FR-003, FR-004)
+- Per-channel amplitude balancing (FR-005)
+- Batch WebSocket updates (FR-006)
+- Enable/disable control (FR-007)
 
-
-
-Control Law) × sign(d(coherence)/dt)
+Control Law:
+  Δc = β × (1 – criticality) × sign(d(coherence)/dt)
   Aᵢ ← Aᵢ × (1 ± δ) based on local energy
 
 Success Criteria:
 - SC-001: Criticality 0.95-1.05 for ≥90% runtime
 - SC-002: Recovery from imbalance <10s
 - SC-003: CPU overhead <5%
+- SC-004: Instant disable
+"""
 
 import numpy as np
 import time
 from typing import Optional, Callable, Dict, List
 from dataclasses import dataclass
 from collections import deque
 
 
 @dataclass
 class CriticalityBalancerConfig:
     """Configuration for Criticality Balancer"""
     enabled: bool = False
 
     # Control gains
-    beta_coupling)
-    delta_amplitude)
+    beta_coupling: float = 0.1      # Coupling adjustment gain (FR-003)
+    delta_amplitude: float = 0.05   # Amplitude adjustment rate (FR-005)
 
     # Target range
     target_criticality: float = 1.0
     criticality_min: float = 0.95   # SC-001
-    criticality_max)
+    criticality_max: float = 1.05   # SC-001
+
+    # Safety bounds (edge cases)
     coupling_min: float = 0.0
     coupling_max: float = 1.0
     amplitude_min: float = 0.0
-    amplitude_max)
+    amplitude_max: float = 1.0
+
+    # Extreme protection (User Story 2)
     hypersync_threshold: float = 1.1   # Prevent runaway synchronization
     coma_threshold: float = 0.4        # Prevent collapse
 
     # Update rate
     update_interval: float = 0.1  # 10 Hz
 
     # Smoothing
     smoothing_window: int = 30  # 3s @ 10Hz
 
     # Logging
     enable_logging: bool = True
     log_interval: float = 1.0
 
 
 @dataclass
 class CriticalityBalancerState:
     """Current state of Criticality Balancer"""
     enabled: bool = False
 
     # Current metrics
     criticality: float = 1.0
     coherence: float = 0.0
 
     # Control state
     criticality_error: float = 0.0
     coherence_derivative: float = 0.0
-    coupling_adjustment)
+    coupling_adjustment: float = 0.0
+
+    # Channel state (8 channels)
     amplitudes: np.ndarray = None
     coupling_matrix: np.ndarray = None
 
     # Performance tracking
     in_range_count: int = 0
     total_count: int = 0
     last_recovery_start: float = 0.0
     recovery_time: float = 0.0
     recovering: bool = False
     settled: bool = True  # For compatibility with tests
 
     # Extreme condition tracking
     hypersync_count: int = 0
     coma_count: int = 0
 
     # Timing
-    last_update)
+    last_update: float = 0.0
+
     def __post_init__(self):
-        if self.amplitudes is None) * 0.5
-        if self.coupling_matrix is None, 8)) * 0.1
+        if self.amplitudes is None:
+            self.amplitudes = np.ones(8) * 0.5
+        if self.coupling_matrix is None:
+            # Initialize with uniform coupling
+            self.coupling_matrix = np.ones((8, 8)) * 0.1
             np.fill_diagonal(self.coupling_matrix, 0.0)  # No self-coupling
 
 
-class CriticalityBalancer) by)
+class CriticalityBalancer:
+    """
+    Adaptive coupling and amplitude balancer
+
+    Maintains system at edge of chaos (criticality ≈ 1.0) by:
+    - Redistributing coupling between channels (User Story 1)
+    - Balancing amplitudes based on local energy (FR-005)
+    - Preventing runaway states (User Story 2)
 
+    Features:
+    - Adaptive coupling matrix adjustment
+    - Per-channel amplitude balancing
+    - Batch WebSocket updates
+    - Extreme condition protection
+    - Enable/disable toggle
+    """
 
-    Features, config: Optional[CriticalityBalancerConfig]) :
+    def __init__(self, config: Optional[CriticalityBalancerConfig] = None):
         """
         Initialize Criticality Balancer
 
         Args:
-            config)
+            config: CriticalityBalancerConfig (uses defaults if None)
         """
         self.config = config or CriticalityBalancerConfig()
         self.state = CriticalityBalancerState()
 
         # Metrics history for smoothing (FR-002)
         self.criticality_history = deque(maxlen=self.config.smoothing_window)
         self.coherence_history = deque(maxlen=self.config.smoothing_window)
 
         # Performance tracking
         self.criticality_log: List[float] = []
         self.recovery_times: List[float] = []
-        self.timestamps)
-        self.update_callback, None]] = None
+        self.timestamps: List[float] = []
 
-        logger.error("[CriticalityBalancer] Initialized")
-        logger.error("[CriticalityBalancer]   beta_coupling=%s, delta_amplitude=%s", self.config.beta_coupling, self.config.delta_amplitude)
-        logger.error("[CriticalityBalancer]   target_criticality=%s", self.config.target_criticality)
-        logger.error("[CriticalityBalancer]   enabled=%s", self.config.enabled)
+        # Last logged time
+        self.last_log_time = 0.0
 
-    def set_enabled(self, enabled: bool) :
-            enabled, False to disable
+        # Callback for parameter updates (FR-006)
+        self.update_callback: Optional[Callable[[Dict], None]] = None
+
+        print("[CriticalityBalancer] Initialized")
+        print(f"[CriticalityBalancer]   beta_coupling={self.config.beta_coupling}, delta_amplitude={self.config.delta_amplitude}")
+        print(f"[CriticalityBalancer]   target_criticality={self.config.target_criticality}")
+        print(f"[CriticalityBalancer]   enabled={self.config.enabled}")
+
+    def set_enabled(self, enabled: bool):
+        """
+        Enable or disable balancer (FR-007, SC-004)
+
+        Args:
+            enabled: True to enable, False to disable
         """
         if self.config.enabled == enabled:
             return
 
         self.config.enabled = enabled
         self.state.enabled = enabled
 
-        if enabled)
+        if enabled:
+            print("[CriticalityBalancer] ENABLED - starting adaptive balancing")
             # Reset state
             self.state.last_update = time.time()
             self.state.in_range_count = 0
             self.state.total_count = 0
             self.state.recovering = False
-        else)
+        else:
+            print("[CriticalityBalancer] DISABLED - manual control restored")
+
+    def process_metrics(self, metrics_frame) -> bool:
+        """
+        Process incoming metrics frame (FR-002)
 
-    @lru_cache(maxsize=128)
-    def process_metrics(self, metrics_frame) :
-            metrics_frame, phase_coherence, etc.
+        Args:
+            metrics_frame: MetricsFrame with criticality, phase_coherence, etc.
 
         Returns:
             True if update was applied
         """
-        if not self.config.enabled)
+        if not self.config.enabled:
+            return False
+
+        current_time = time.time()
 
         # Check if we should update (rate limiting)
-        if current_time - self.state.last_update < self.config.update_interval, 'criticality', 1.0)
+        if current_time - self.state.last_update < self.config.update_interval:
+            return False
+
+        # Extract metrics
+        criticality = getattr(metrics_frame, 'criticality', 1.0)
         coherence = getattr(metrics_frame, 'phase_coherence', 0.0)
 
         # Edge case: Invalid metrics → skip
         if criticality == 0.0 and coherence == 0.0:
-            logger.error("[CriticalityBalancer] WARNING, skipping update")
+            print("[CriticalityBalancer] WARNING: Invalid metrics, skipping update")
             return False
 
         # Add to history
         self.criticality_history.append(criticality)
         self.coherence_history.append(coherence)
 
         # Need enough history for derivative
-        if len(self.criticality_history) < 2)
+        if len(self.criticality_history) < 2:
+            return False
+
+        # Compute smoothed values
+        criticality_smooth = np.mean(self.criticality_history)
         coherence_smooth = np.mean(self.coherence_history)
 
         # Store in state
         self.state.criticality = criticality_smooth
         self.state.coherence = coherence_smooth
 
         # Compute time delta
         dt = current_time - self.state.last_update
         self.state.last_update = current_time
 
         # Apply balancing algorithms
         updated = self._apply_balancing(criticality_smooth, coherence_smooth, dt)
 
         # Track performance
         self._track_performance(criticality_smooth, current_time)
 
         # Log if needed
-        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval)
+        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval:
+            self._log_stats()
             self.last_log_time = current_time
 
         return updated
 
-    def _apply_balancing(self, criticality, coherence, dt) :
+    def _apply_balancing(self, criticality: float, coherence: float, dt: float) -> bool:
+        """
+        Apply adaptive balancing algorithms (FR-003, FR-004, FR-005)
+
+        Args:
             criticality: Current criticality value
             coherence: Current phase coherence
             dt: Time delta since last update
 
-        Returns) >= 2) / dt
+        Returns:
+            True if parameters were updated
+        """
+        updated = False
+
+        # Compute error and derivative
+        criticality_error = self.config.target_criticality - criticality
+        self.state.criticality_error = criticality_error
+
+        # Compute coherence derivative
+        if len(self.coherence_history) >= 2:
+            coherence_prev = self.coherence_history[-2]
+            coherence_derivative = (coherence - coherence_prev) / dt
             self.state.coherence_derivative = coherence_derivative
-        else, FR-004) ---
+        else:
+            coherence_derivative = 0.0
+
+        # --- Coupling Matrix Adjustment (FR-003, FR-004) ---
         # Δc = β × (1 – criticality) × sign(d(coherence)/dt)
         coherence_sign = np.sign(coherence_derivative) if coherence_derivative != 0 else 0
         delta_coupling = self.config.beta_coupling * criticality_error * coherence_sign
 
         self.state.coupling_adjustment = delta_coupling
 
         # Apply to off-diagonal entries
-        if abs(delta_coupling) > 0.001)
+        if abs(delta_coupling) > 0.001:
+            # Get current coupling matrix
+            coupling_matrix = self.state.coupling_matrix.copy()
 
-            # Adjust off-diagonal entries (User Story 1)
+            # Adjust off-diagonal entries (User Story 1: redistribute coupling)
             mask = ~np.eye(8, dtype=bool)  # Off-diagonal mask
             coupling_matrix[mask] += delta_coupling
 
-            # Clamp to bounds (Edge case)
+            # Clamp to bounds (Edge case: hard clamp)
             coupling_matrix = np.clip(coupling_matrix, self.config.coupling_min, self.config.coupling_max)
 
             # Preserve zero diagonal
             np.fill_diagonal(coupling_matrix, 0.0)
 
             # Normalize rows to maintain total coupling strength
             row_sums = coupling_matrix.sum(axis=1, keepdims=True)
             row_sums[row_sums == 0] = 1.0  # Avoid division by zero
             coupling_matrix = coupling_matrix / row_sums * row_sums.mean()
 
             self.state.coupling_matrix = coupling_matrix
             updated = True
 
         # --- Amplitude Balancing (FR-005) ---
         # Adjust per-channel amplitude based on local energy
-        # Simple heuristic, boost low-energy
+        # Simple heuristic: reduce high-energy channels, boost low-energy
         amplitudes = self.state.amplitudes.copy()
 
         # Compute relative energy (using coupling row sums as proxy)
         energy_proxy = self.state.coupling_matrix.sum(axis=1)
         mean_energy = energy_proxy.mean()
 
         # Adjust amplitudes inversely proportional to energy
         for i in range(8):
-            if mean_energy > 0, low energy → increase amplitude
+            if mean_energy > 0:
+                energy_ratio = energy_proxy[i] / mean_energy
+                # High energy → reduce amplitude, low energy → increase amplitude
                 adjustment = self.config.delta_amplitude * (1.0 - energy_ratio) * dt
                 amplitudes[i] += adjustment
 
         # Clamp to bounds
         amplitudes = np.clip(amplitudes, self.config.amplitude_min, self.config.amplitude_max)
 
         # Check if changed significantly
-        if np.max(np.abs(amplitudes - self.state.amplitudes)) > 0.001) ---
+        if np.max(np.abs(amplitudes - self.state.amplitudes)) > 0.001:
+            self.state.amplitudes = amplitudes
+            updated = True
+
+        # --- Extreme Protection (User Story 2) ---
         # Prevent hypersync (criticality > 1.1)
-        if criticality > self.config.hypersync_threshold, self.config.coupling_min, self.config.coupling_max)
+        if criticality > self.config.hypersync_threshold:
+            # Reduce coupling to prevent runaway
+            self.state.coupling_matrix *= 0.95
+            self.state.coupling_matrix = np.clip(self.state.coupling_matrix, self.config.coupling_min, self.config.coupling_max)
             np.fill_diagonal(self.state.coupling_matrix, 0.0)
             self.state.hypersync_count += 1
-            logger.error("[CriticalityBalancer] Hypersync detected, reducing coupling", criticality)
+            print(f"[CriticalityBalancer] Hypersync detected: criticality={criticality:.3f}, reducing coupling")
             updated = True
 
         # Prevent coma (criticality < 0.4)
-        if criticality < self.config.coma_threshold, self.config.coupling_min, self.config.coupling_max)
+        if criticality < self.config.coma_threshold:
+            # Increase coupling to boost activity
+            self.state.coupling_matrix *= 1.05
+            self.state.coupling_matrix = np.clip(self.state.coupling_matrix, self.config.coupling_min, self.config.coupling_max)
             np.fill_diagonal(self.state.coupling_matrix, 0.0)
             self.state.coma_count += 1
-            logger.error("[CriticalityBalancer] Coma detected, increasing coupling", criticality)
+            print(f"[CriticalityBalancer] Coma detected: criticality={criticality:.3f}, increasing coupling")
             updated = True
 
         # Send batch update (FR-006)
         if updated and self.update_callback:
             update_data = {
-                'type',
-                'coupling_matrix'),
-                'amplitudes')
+                'type': 'update_coupling',
+                'coupling_matrix': self.state.coupling_matrix.tolist(),
+                'amplitudes': self.state.amplitudes.tolist()
             }
             self.update_callback(update_data)
 
         return updated
 
-    def _track_performance(self, criticality: float, current_time: float) :
+    def _track_performance(self, criticality: float, current_time: float):
+        """
+        Track performance metrics (SC-001, SC-002)
+
+        Args:
             criticality: Current criticality value
             current_time: Current timestamp
         """
-        # Check if in range (SC-001)
+        # Check if in range (SC-001: 0.95-1.05)
         in_range = self.config.criticality_min <= criticality <= self.config.criticality_max
 
-        if in_range)
+        if in_range:
+            self.state.in_range_count += 1
+
+        self.state.total_count += 1
+
+        # Detect imbalance and recovery (SC-002)
         out_of_range = not in_range and abs(self.state.criticality_error) > 0.1
 
         if out_of_range and not self.state.recovering:
             # Start recovery
             self.state.recovering = True
             self.state.last_recovery_start = current_time
-            logger.error("[CriticalityBalancer] Imbalance detected, criticality)
+            print(f"[CriticalityBalancer] Imbalance detected: criticality={criticality:.3f}")
 
-        elif self.state.recovering and in_range)
-            logger.error("[CriticalityBalancer] Recovered in %ss (SC-002)", recovery_time)
+        elif self.state.recovering and in_range:
+            # Recovery complete
+            recovery_time = current_time - self.state.last_recovery_start
+            self.state.recovery_time = recovery_time
+            self.state.recovering = False
+            self.recovery_times.append(recovery_time)
+            print(f"[CriticalityBalancer] Recovered in {recovery_time:.2f}s (SC-002: target <10s)")
 
         # Append to log
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            self.criticality_log.append(criticality)
             self.timestamps.append(current_time)
 
-    @lru_cache(maxsize=128)
-    def _log_stats(self) :
+    def _log_stats(self):
         """Log performance statistics"""
-        if self.state.total_count == 0)
+        if self.state.total_count == 0:
+            return
+
+        # Calculate in-range percentage (SC-001)
         in_range_percent = (self.state.in_range_count / self.state.total_count) * 100
 
         # Calculate criticality statistics
         if len(self.criticality_log) > 0:
-            crit_mean = np.mean(self.criticality_log[-30)  # Last 30 samples
-            crit_std = np.std(self.criticality_log[-30)
-        else) > 0)
+            crit_mean = np.mean(self.criticality_log[-30:])  # Last 30 samples
+            crit_std = np.std(self.criticality_log[-30:])
+        else:
+            crit_mean = 0.0
+            crit_std = 0.0
+
+        # Recovery statistics
+        if len(self.recovery_times) > 0:
+            avg_recovery = np.mean(self.recovery_times)
             max_recovery = np.max(self.recovery_times)
         else:
             avg_recovery = 0.0
             max_recovery = 0.0
 
-        print(f"[CriticalityBalancer] Stats: criticality={crit_mean:.3f}±{crit_std, "
-              f"in_range={in_range_percent, "
-              f"avg_recovery={avg_recovery)
+        print(f"[CriticalityBalancer] Stats: criticality={crit_mean:.3f}±{crit_std:.3f}, "
+              f"in_range={in_range_percent:.1f}%, "
+              f"avg_recovery={avg_recovery:.2f}s")
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
         """
         Get performance statistics
 
         Returns:
             Dictionary with performance metrics
         """
         if self.state.total_count == 0:
             return {
-                'enabled',
-                'criticality_mean',
-                'criticality_std',
-                'in_range_percent',
-                'avg_recovery_time',
-                'max_recovery_time') * 100
-
-        if len(self.criticality_log) > 0))
+                'enabled': self.config.enabled,
+                'criticality_mean': 0.0,
+                'criticality_std': 0.0,
+                'in_range_percent': 0.0,
+                'avg_recovery_time': 0.0,
+                'max_recovery_time': 0.0
+            }
+
+        in_range_percent = (self.state.in_range_count / self.state.total_count) * 100
+
+        if len(self.criticality_log) > 0:
+            crit_mean = float(np.mean(self.criticality_log))
             crit_std = float(np.std(self.criticality_log))
-        else) > 0))
+        else:
+            crit_mean = 0.0
+            crit_std = 0.0
+
+        if len(self.recovery_times) > 0:
+            avg_recovery = float(np.mean(self.recovery_times))
             max_recovery = float(np.max(self.recovery_times))
         else:
             avg_recovery = 0.0
             max_recovery = 0.0
 
         return {
-            'enabled',
-            'criticality_mean',
-            'criticality_std',
-            'in_range_percent',
-            'in_range_count',
-            'total_count',
-            'recovering',
-            'recovery_count'),
-            'avg_recovery_time',
-            'max_recovery_time',
-            'criticality_error',
-            'coupling_adjustment') :
+            'enabled': self.config.enabled,
+            'criticality_mean': crit_mean,
+            'criticality_std': crit_std,
+            'in_range_percent': in_range_percent,
+            'in_range_count': self.state.in_range_count,
+            'total_count': self.state.total_count,
+            'recovering': self.state.recovering,
+            'recovery_count': len(self.recovery_times),
+            'avg_recovery_time': avg_recovery,
+            'max_recovery_time': max_recovery,
+            'criticality_error': self.state.criticality_error,
+            'coupling_adjustment': self.state.coupling_adjustment
+        }
+
+    def reset_statistics(self):
+        """Reset performance statistics"""
+        self.state.in_range_count = 0
+        self.state.total_count = 0
+        self.state.recovering = False
+        self.state.recovery_time = 0.0
+
+        self.criticality_log.clear()
+        self.recovery_times.clear()
+        self.timestamps.clear()
+
+        print("[CriticalityBalancer] Statistics reset")
+
+    def get_current_state(self) -> Dict:
         """
         Get current balancer state
 
         Returns:
             Dictionary with current state
         """
         return {
-            'enabled',
-            'criticality'),
-            'coherence'),
-            'criticality_error'),
-            'coupling_adjustment'),
-            'coupling_matrix'),
-            'amplitudes'),
-            'recovering') :
+            'enabled': self.state.enabled,
+            'criticality': float(self.state.criticality),
+            'coherence': float(self.state.coherence),
+            'criticality_error': float(self.state.criticality_error),
+            'coupling_adjustment': float(self.state.coupling_adjustment),
+            'coupling_matrix': self.state.coupling_matrix.tolist(),
+            'amplitudes': self.state.amplitudes.tolist(),
+            'recovering': self.state.recovering
+        }
+
+    def export_logs(self) -> Dict:
         """
         Export performance logs for analysis
 
         Returns:
             Dictionary with time-series data
         """
         return {
-            'timestamps',
-            'criticality',
-            'recovery_times',
+            'timestamps': self.timestamps,
+            'criticality': self.criticality_log,
+            'recovery_times': self.recovery_times,
             'config': {
-                'beta_coupling',
-                'delta_amplitude',
-                'target_criticality')
-def _self_test() :
+                'beta_coupling': self.config.beta_coupling,
+                'delta_amplitude': self.config.delta_amplitude,
+                'target_criticality': self.config.target_criticality
+            }
+        }
+
+
+# Self-test function
+def _self_test():
+    """Test CriticalityBalancer"""
+    print("=" * 60)
+    print("CriticalityBalancer Self-Test")
+    print("=" * 60)
+
+    # Create mock metrics frame
+    class MockMetrics:
+        def __init__(self, criticality, phase_coherence):
             self.criticality = criticality
             self.phase_coherence = phase_coherence
 
-    # Test 1)
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = CriticalityBalancerConfig(enabled=True, beta_coupling=0.1, delta_amplitude=0.05)
     balancer = CriticalityBalancer(config)
 
     assert balancer.config.enabled == True
     assert balancer.config.beta_coupling == 0.1
     assert balancer.state.coupling_matrix.shape == (8, 8)
     assert balancer.state.amplitudes.shape == (8,)
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Test 2)
-    logger.info("\n2. Testing enable/disable toggle...")
+    # Test 2: Toggle enable/disable (SC-004)
+    print("\n2. Testing enable/disable toggle...")
     balancer.set_enabled(False)
     assert balancer.config.enabled == False
     balancer.set_enabled(True)
     assert balancer.config.enabled == True
-    logger.info("   OK)
+    print("   OK: Toggle")
 
-    # Test 3)
+    # Test 3: Process metrics and balancing
+    print("\n3. Testing balancing algorithm...")
 
     # Track updates
     updates = []
-    @lru_cache(maxsize=128)
-    def callback(update_data) :
-        logger.info("   Coupling matrix shape, np.array(updates[0]['coupling_matrix']).shape)
-        logger.info("   Amplitudes shape, np.array(updates[0]['amplitudes']).shape)
+    def callback(update_data):
+        updates.append(update_data)
+
+    balancer.update_callback = callback
+
+    # Send metrics with low criticality
+    time.sleep(0.15)
+    metrics1 = MockMetrics(criticality=0.8, phase_coherence=0.7)
+    balancer.process_metrics(metrics1)
+
+    time.sleep(0.15)
+    metrics2 = MockMetrics(criticality=0.8, phase_coherence=0.75)
+    balancer.process_metrics(metrics2)
+
+    print(f"   Updates sent: {len(updates)}")
+    if len(updates) > 0:
+        print(f"   Coupling matrix shape: {np.array(updates[0]['coupling_matrix']).shape}")
+        print(f"   Amplitudes shape: {np.array(updates[0]['amplitudes']).shape}")
 
     assert len(updates) > 0, "Should send updates"
-    logger.info("   OK)
+    print("   OK: Balancing algorithm")
 
-    # Test 4)
+    # Test 4: Statistics tracking
+    print("\n4. Testing statistics...")
     stats = balancer.get_statistics()
     assert 'criticality_mean' in stats
     assert 'in_range_percent' in stats
     assert stats['enabled'] == True
-    logger.info("   OK: Statistics, stats['in_range_percent'])
+    print(f"   OK: Statistics: in_range={stats['in_range_percent']:.1f}%")
 
-    # Test 5)
+    # Test 5: Get current state
+    print("\n5. Testing state export...")
     state = balancer.get_current_state()
     assert 'coupling_matrix' in state
     assert 'amplitudes' in state
     assert len(state['amplitudes']) == 8
-    logger.info("   OK)
+    print("   OK: State export")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/data_exporter.py b/server/data_exporter.py
index aa8df5b960c256e118191f5f35ed240b81b5ff34..eca821ec30f5a5f48d04bead81da429389d443b4 100644
--- a/server/data_exporter.py
+++ b/server/data_exporter.py
@@ -1,473 +1,646 @@
 """
 Data Exporter - Feature 019
 Export and convert recorded sessions to various formats for research, visualization, or publication.
 
 Supported Formats:
+- CSV: Metrics only (simple spreadsheet format)
+- JSON: Raw logs (all data streams)
+- HDF5: Hierarchical compressed dataset (NumPy/MATLAB compatible)
+- MP4: Audio + visual overlay (video format)
 
-
-
-
-Features, MATLAB, Audacity)
+Features:
+- Time-range clipping
+- ZIP compression for download packages
+- Checksum verification
+- Compatible with external tools (NumPy, MATLAB, Audacity)
 
 Requirements:
 - FR-001: DataExporter class
-
-- FR-003, JSON, HDF5, MP4 export formats
+- FR-002: Session folder input (Feature 017 format)
+- FR-003: CSV, JSON, HDF5, MP4 export formats
 - FR-004: Time-range clipping
 - FR-005: REST API endpoint /api/export
 - FR-007: ZIP compression
 
 Success Criteria:
 - SC-001: ±1 frame alignment
 - SC-002: Export time <2× real-time
 - SC-003: File integrity checksum
+- SC-004: External tool compatibility
+"""
 
 import json
 import csv
 import wave
 import os
 import time
 import hashlib
 import zipfile
 from pathlib import Path
 from typing import Optional, Dict, List, Tuple, Any
 from dataclasses import dataclass
 from enum import Enum
 import numpy as np
 
 
-class ExportFormat(Enum))"""
+class ExportFormat(Enum):
+    """Export format options (FR-003)"""
     CSV = "csv"
     JSON = "json"
     HDF5 = "hdf5"
     MP4 = "mp4"
 
 
 @dataclass
 class ExportConfig:
     """Configuration for data exporter"""
     output_dir: str = "exports"
-    enable_compression)
-    enable_checksum)
+    enable_compression: bool = True  # ZIP compression (FR-007)
+    enable_checksum: bool = True  # File integrity verification (SC-003)
     enable_logging: bool = True
 
 
 @dataclass
-class ExportRequest)"""
+class ExportRequest:
+    """Export request parameters (FR-005)"""
     session_path: str
     format: ExportFormat
     output_name: Optional[str] = None
-    time_range, float]] = None  # (start, end) in seconds (FR-004)
+    time_range: Optional[Tuple[float, float]] = None  # (start, end) in seconds (FR-004)
     compress: bool = True  # ZIP compression
 
 
-class DataExporter)
+class DataExporter:
+    """
+    Data Exporter for recorded sessions (Feature 019)
 
     Exports session data to various formats for analysis and publication.
     Supports time-range clipping, compression, and checksum verification.
     """
 
-    def __init__(self, config: Optional[ExportConfig]) :
+    def __init__(self, config: Optional[ExportConfig] = None):
         """
         Initialize Data Exporter
 
         Args:
-            config)
+            config: Export configuration
+        """
+        self.config = config or ExportConfig()
 
         # Ensure output directory exists
         os.makedirs(self.config.output_dir, exist_ok=True)
 
         # Export statistics
         self.total_exports = 0
         self.last_export_time = 0.0
         self.last_error: Optional[str] = None
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[DataExporter] Initialized")
 
-    def export_session(self, request) :
-            request, format, and options
+    def export_session(self, request: ExportRequest) -> Dict[str, Any]:
+        """
+        Export session to specified format (FR-001, FR-002)
 
-        Returns, size, checksum, and metadata
+        Args:
+            request: Export request with session path, format, and options
+
+        Returns:
+            Export result with file path, size, checksum, and metadata
         """
         start_time = time.perf_counter()
 
-        try):
-                raise ValueError(f"Session not found)
+        try:
+            # Validate session exists
+            if not os.path.exists(request.session_path):
+                raise ValueError(f"Session not found: {request.session_path}")
 
             # Load session metadata
             session_info = self._load_session_info(request.session_path)
 
             # Generate output filename
             if request.output_name:
                 output_name = request.output_name
-            else)
+            else:
+                session_name = os.path.basename(request.session_path)
                 output_name = f"{session_name}_{request.format.value}"
 
             # Export based on format (FR-003)
-            if request.format == ExportFormat.CSV, output_name, session_info)
-            elif request.format == ExportFormat.JSON, output_name, session_info)
-            elif request.format == ExportFormat.HDF5, output_name, session_info)
-            elif request.format == ExportFormat.MP4, output_name, session_info)
+            if request.format == ExportFormat.CSV:
+                output_file = self._export_csv(request, output_name, session_info)
+            elif request.format == ExportFormat.JSON:
+                output_file = self._export_json(request, output_name, session_info)
+            elif request.format == ExportFormat.HDF5:
+                output_file = self._export_hdf5(request, output_name, session_info)
+            elif request.format == ExportFormat.MP4:
+                output_file = self._export_mp4(request, output_name, session_info)
             else:
-                raise ValueError(f"Unsupported format)
+                raise ValueError(f"Unsupported format: {request.format}")
 
             # Compress to ZIP if requested (FR-007)
-            if request.compress, output_name)
+            if request.compress:
+                output_file = self._compress_to_zip(output_file, output_name)
 
             # Calculate checksum (SC-003)
             checksum = None
-            if self.config.enable_checksum)
+            if self.config.enable_checksum:
+                checksum = self._calculate_checksum(output_file)
 
             # Get file size
             file_size = os.path.getsize(output_file)
 
             # Record statistics
             export_time = time.perf_counter() - start_time
             self.last_export_time = export_time
             self.total_exports += 1
 
             # Calculate real-time ratio (SC-002)
             session_duration = session_info.get('duration', 0)
             realtime_ratio = export_time / session_duration if session_duration > 0 else 0
 
             if self.config.enable_logging:
                 print(f"[DataExporter] Exported {request.format.value} in {export_time:.2f}s "
-                      f"({realtime_ratio), size={file_size / 1024)
+                      f"({realtime_ratio:.2f}x real-time), size={file_size / 1024:.1f} KB")
 
             return {
-                "ok",
-                "output_file",
-                "file_size",
-                "checksum",
-                "export_time",
-                "realtime_ratio",
-                "session_duration",
-                "time_range",
+                "ok": True,
+                "output_file": output_file,
+                "file_size": file_size,
+                "checksum": checksum,
+                "export_time": export_time,
+                "realtime_ratio": realtime_ratio,
+                "session_duration": session_duration,
+                "time_range": request.time_range,
                 "format": request.format.value
             }
 
-        except Exception as e)
+        except Exception as e:
+            self.last_error = str(e)
             if self.config.enable_logging:
-                logger.error("[DataExporter] ERROR, e)
+                print(f"[DataExporter] ERROR: {e}")
             return {
-                "ok",
-                "error")
+                "ok": False,
+                "error": str(e)
             }
 
-    def _load_session_info(self, session_path) :
+    def _load_session_info(self, session_path: str) -> Dict[str, Any]:
+        """
+        Load session metadata (FR-002)
+
+        Args:
             session_path: Path to session folder
 
-        Returns, "session.json")
+        Returns:
+            Session metadata dictionary
+        """
+        session_json = os.path.join(session_path, "session.json")
 
-        if os.path.exists(session_json), 'r') as f)
-        else, create basic info
+        if os.path.exists(session_json):
+            with open(session_json, 'r') as f:
+                return json.load(f)
+        else:
+            # No metadata file, create basic info
             return {
-                "session_name"),
-                "duration",
-                "start_time", jsonl_path, time_range, float]] = None) :
+                "session_name": os.path.basename(session_path),
+                "duration": 0,
+                "start_time": 0
+            }
+
+    def _load_jsonl_data(self, jsonl_path: str, time_range: Optional[Tuple[float, float]] = None) -> List[Dict]:
+        """
+        Load JSONL data file with optional time-range clipping (FR-004, SC-001)
+
+        Args:
             jsonl_path: Path to JSONL file
-            time_range, end) time range in seconds
+            time_range: Optional (start, end) time range in seconds
+
+        Returns:
+            List of data frames
+        """
+        data = []
 
-        Returns), 'r') as f:
-            for line in f))
+        if not os.path.exists(jsonl_path):
+            return data
+
+        with open(jsonl_path, 'r') as f:
+            for line in f:
+                if line.strip():
+                    frame = json.loads(line)
 
                     # Apply time range filter (FR-004)
-                    if time_range, 0)
+                    if time_range:
+                        timestamp = frame.get('timestamp', 0)
                         start, end = time_range
-                        if timestamp < start or timestamp > end)
+                        if timestamp < start or timestamp > end:
+                            continue
+
+                    data.append(frame)
 
         return data
 
-    def _export_csv(self, request, output_name, session_info) :
+    def _export_csv(self, request: ExportRequest, output_name: str, session_info: Dict) -> str:
+        """
+        Export session to CSV format (metrics only) (FR-003)
+
+        Args:
             request: Export request
             output_name: Output filename base
             session_info: Session metadata
 
-        Returns, f"{output_name}.csv")
+        Returns:
+            Path to exported CSV file
+        """
+        output_file = os.path.join(self.config.output_dir, f"{output_name}.csv")
 
         # Load metrics data
         metrics_path = os.path.join(request.session_path, "metrics.jsonl")
         metrics_data = self._load_jsonl_data(metrics_path, request.time_range)
 
-        if not metrics_data)
+        if not metrics_data:
+            raise ValueError("No metrics data found in session")
 
         # Extract all unique field names
         fieldnames = set()
-        for frame in metrics_data))
+        for frame in metrics_data:
+            fieldnames.update(frame.keys())
         fieldnames = sorted(list(fieldnames))
 
-        # Write CSV (SC-004, NumPy, etc.)
-        with open(output_file, 'w', newline='') as f, fieldnames=fieldnames)
+        # Write CSV (SC-004: compatible with Excel, NumPy, etc.)
+        with open(output_file, 'w', newline='') as f:
+            writer = csv.DictWriter(f, fieldnames=fieldnames)
             writer.writeheader()
             writer.writerows(metrics_data)
 
         return output_file
 
-    def _export_json(self, request, output_name, session_info) :
+    def _export_json(self, request: ExportRequest, output_name: str, session_info: Dict) -> str:
+        """
+        Export session to JSON format (raw logs, all streams) (FR-003)
+
+        Args:
             request: Export request
             output_name: Output filename base
             session_info: Session metadata
 
-        Returns, f"{output_name}.json")
+        Returns:
+            Path to exported JSON file
+        """
+        output_file = os.path.join(self.config.output_dir, f"{output_name}.json")
 
         # Load all data streams
         metrics_path = os.path.join(request.session_path, "metrics.jsonl")
         phi_path = os.path.join(request.session_path, "phi.jsonl")
         controls_path = os.path.join(request.session_path, "controls.jsonl")
 
         metrics_data = self._load_jsonl_data(metrics_path, request.time_range)
         phi_data = self._load_jsonl_data(phi_path, request.time_range)
         controls_data = self._load_jsonl_data(controls_path, request.time_range)
 
         # Create unified JSON structure
         export_data = {
-            "session_info",
-            "time_range",
-            "export_timestamp"),
+            "session_info": session_info,
+            "time_range": request.time_range,
+            "export_timestamp": time.time(),
             "data": {
-                "metrics",
-                "phi",
-                "controls", 'w') as f, f, indent=2)
+                "metrics": metrics_data,
+                "phi": phi_data,
+                "controls": controls_data
+            }
+        }
+
+        # Write JSON
+        with open(output_file, 'w') as f:
+            json.dump(export_data, f, indent=2)
 
         return output_file
 
-    def _export_hdf5(self, request, output_name, session_info) :
+    def _export_hdf5(self, request: ExportRequest, output_name: str, session_info: Dict) -> str:
+        """
+        Export session to HDF5 format (hierarchical compressed) (FR-003, SC-004)
+
+        Compatible with NumPy, MATLAB, and other scientific tools.
+
+        Args:
             request: Export request
             output_name: Output filename base
             session_info: Session metadata
 
-        Returns, f"{output_name}.h5")
+        Returns:
+            Path to exported HDF5 file
+        """
+        output_file = os.path.join(self.config.output_dir, f"{output_name}.h5")
 
         try:
             import h5py
         except ImportError:
-            raise ImportError("h5py not installed. Install with)
+            raise ImportError("h5py not installed. Install with: pip install h5py")
 
         # Load all data streams
         metrics_path = os.path.join(request.session_path, "metrics.jsonl")
         phi_path = os.path.join(request.session_path, "phi.jsonl")
         controls_path = os.path.join(request.session_path, "controls.jsonl")
         audio_path = os.path.join(request.session_path, "audio.wav")
 
         metrics_data = self._load_jsonl_data(metrics_path, request.time_range)
         phi_data = self._load_jsonl_data(phi_path, request.time_range)
         controls_data = self._load_jsonl_data(controls_path, request.time_range)
 
-        # Create HDF5 file (SC-004)
-        with h5py.File(output_file, 'w') as f)
+        # Create HDF5 file (SC-004: MATLAB/NumPy compatible)
+        with h5py.File(output_file, 'w') as f:
+            # Store session metadata
+            meta_group = f.create_group('metadata')
             for key, value in session_info.items():
                 meta_group.attrs[key] = value
             if request.time_range:
                 meta_group.attrs['time_range_start'] = request.time_range[0]
                 meta_group.attrs['time_range_end'] = request.time_range[1]
 
             # Store metrics as datasets
-            if metrics_data)
+            if metrics_data:
+                metrics_group = f.create_group('metrics')
                 # Convert to columnar format for efficient access
                 metric_arrays = self._jsonl_to_arrays(metrics_data)
-                for key, values in metric_arrays.items(), data=values, compression='gzip')
+                for key, values in metric_arrays.items():
+                    metrics_group.create_dataset(key, data=values, compression='gzip')
 
             # Store phi parameters
-            if phi_data)
+            if phi_data:
+                phi_group = f.create_group('phi')
                 phi_arrays = self._jsonl_to_arrays(phi_data)
-                for key, values in phi_arrays.items(), data=values, compression='gzip')
+                for key, values in phi_arrays.items():
+                    phi_group.create_dataset(key, data=values, compression='gzip')
 
             # Store controls
-            if controls_data)
+            if controls_data:
+                controls_group = f.create_group('controls')
                 controls_arrays = self._jsonl_to_arrays(controls_data)
-                for key, values in controls_arrays.items(), data=values, compression='gzip')
+                for key, values in controls_arrays.items():
+                    controls_group.create_dataset(key, data=values, compression='gzip')
 
             # Store audio data (if time range specified, clip it)
-            if os.path.exists(audio_path), sample_rate = self._load_audio_data(audio_path, request.time_range)
+            if os.path.exists(audio_path):
+                audio_data, sample_rate = self._load_audio_data(audio_path, request.time_range)
                 audio_group = f.create_group('audio')
                 audio_group.create_dataset('data', data=audio_data, compression='gzip')
                 audio_group.attrs['sample_rate'] = sample_rate
                 audio_group.attrs['channels'] = audio_data.shape[1] if len(audio_data.shape) > 1 else 1
 
         return output_file
 
-    def _export_mp4(self, request, output_name, session_info) :
+    def _export_mp4(self, request: ExportRequest, output_name: str, session_info: Dict) -> str:
+        """
+        Export session to MP4 format (audio + visual overlay) (FR-003)
+
+        Requires ffmpeg to be installed on the system.
+
+        Args:
             request: Export request
             output_name: Output filename base
             session_info: Session metadata
 
-        Returns, f"{output_name}.mp4")
+        Returns:
+            Path to exported MP4 file
+        """
+        output_file = os.path.join(self.config.output_dir, f"{output_name}.mp4")
 
         # Check if ffmpeg is available
         import shutil
-        if not shutil.which('ffmpeg'))
+        if not shutil.which('ffmpeg'):
+            raise RuntimeError("ffmpeg not found. Please install ffmpeg to export MP4.")
 
         # For now, create a placeholder implementation
         # Full implementation would require:
         # 1. Generate visualization frames from metrics data
         # 2. Combine with audio using ffmpeg
         # 3. Apply time range clipping
 
-        # Simplified, "audio.wav")
+        # Simplified: Just copy audio and add basic metadata
+        audio_path = os.path.join(request.session_path, "audio.wav")
 
-        if not os.path.exists(audio_path))
+        if not os.path.exists(audio_path):
+            raise ValueError("No audio data found in session")
 
         # Use ffmpeg to convert WAV to MP4 with AAC audio
         import subprocess
 
         # Apply time range if specified (FR-004)
-        if request.time_range, end = request.time_range
+        if request.time_range:
+            start, end = request.time_range
             duration = end - start
             cmd = [
                 'ffmpeg', '-y',
                 '-ss', str(start),
                 '-t', str(duration),
                 '-i', audio_path,
-                '-c, 'aac',
-                '-b, '192k',
+                '-c:a', 'aac',
+                '-b:a', '192k',
                 output_file
             ]
-        else, '-y',
+        else:
+            cmd = [
+                'ffmpeg', '-y',
                 '-i', audio_path,
-                '-c, 'aac',
-                '-b, '192k',
+                '-c:a', 'aac',
+                '-b:a', '192k',
                 output_file
             ]
 
         subprocess.run(cmd, capture_output=True, check=True)
 
         return output_file
 
-    @lru_cache(maxsize=128)
-    def _load_audio_data(self, audio_path, time_range, float]] = None) :
+    def _load_audio_data(self, audio_path: str, time_range: Optional[Tuple[float, float]] = None) -> Tuple[np.ndarray, int]:
+        """
+        Load audio data with optional time-range clipping (FR-004)
+
+        Args:
             audio_path: Path to WAV file
-            time_range, end) time range in seconds
+            time_range: Optional (start, end) time range in seconds
 
-        Returns, sample_rate)
+        Returns:
+            Tuple of (audio_data, sample_rate)
         """
-        with wave.open(audio_path, 'rb') as wav)
+        with wave.open(audio_path, 'rb') as wav:
+            sample_rate = wav.getframerate()
             n_channels = wav.getnchannels()
             n_frames = wav.getnframes()
 
             # Apply time range clipping (FR-004, SC-001)
-            if time_range, end = time_range
+            if time_range:
+                start, end = time_range
                 start_frame = int(start * sample_rate)
                 end_frame = int(end * sample_rate)
 
                 # Clamp to valid range
                 start_frame = max(0, min(start_frame, n_frames))
                 end_frame = max(start_frame, min(end_frame, n_frames))
 
                 # Seek to start position
                 wav.setpos(start_frame)
 
                 # Read frames
                 n_frames_to_read = end_frame - start_frame
                 audio_bytes = wav.readframes(n_frames_to_read)
-            else)
+            else:
+                audio_bytes = wav.readframes(n_frames)
 
             # Convert to numpy array
             audio_data = np.frombuffer(audio_bytes, dtype=np.int16)
 
             # Reshape to (frames, channels)
-            if n_channels > 1, n_channels)
+            if n_channels > 1:
+                audio_data = audio_data.reshape(-1, n_channels)
 
             return audio_data, sample_rate
 
-    @lru_cache(maxsize=128)
-    def _jsonl_to_arrays(self, data) :
+    def _jsonl_to_arrays(self, data: List[Dict]) -> Dict[str, np.ndarray]:
         """
-        Convert JSONL data to columnar numpy arrays (SC-004)
+        Convert JSONL data to columnar numpy arrays (SC-004: NumPy compatible)
 
         Args:
             data: List of data frames
 
         Returns:
-            Dictionary of field name :
+            Dictionary of field name -> numpy array
+        """
+        if not data:
+            return {}
+
+        # Collect all fields
+        fields = set()
+        for frame in data:
+            fields.update(frame.keys())
+
+        # Convert to arrays
+        arrays = {}
+        for field in fields:
             values = []
-            for frame in data)
+            for frame in data:
+                value = frame.get(field)
                 # Convert to numeric if possible
-                if isinstance(value, (int, float)))
-                elif isinstance(value, str))
+                if isinstance(value, (int, float)):
+                    values.append(value)
+                elif isinstance(value, str):
+                    # Keep strings as is (will be stored as variable-length strings in HDF5)
                     values.append(value)
-                else) if value is not None else '')
+                else:
+                    # Convert to string representation
+                    values.append(str(value) if value is not None else '')
 
             # Create numpy array
-            if all(isinstance(v, (int, float)) for v in values), dtype=np.float64)
-            else, dtype=object)
+            if all(isinstance(v, (int, float)) for v in values):
+                arrays[field] = np.array(values, dtype=np.float64)
+            else:
+                # Variable-length strings
+                arrays[field] = np.array(values, dtype=object)
 
         return arrays
 
-    def _compress_to_zip(self, file_path, archive_name) :
+    def _compress_to_zip(self, file_path: str, archive_name: str) -> str:
+        """
+        Compress file to ZIP archive (FR-007)
+
+        Args:
             file_path: Path to file to compress
             archive_name: Base name for archive
 
-        Returns, f"{archive_name}.zip")
+        Returns:
+            Path to ZIP file
+        """
+        zip_path = os.path.join(self.config.output_dir, f"{archive_name}.zip")
 
-        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf, os.path.basename(file_path))
+        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
+            zipf.write(file_path, os.path.basename(file_path))
 
         # Remove original file
         os.remove(file_path)
 
         return zip_path
 
-    @lru_cache(maxsize=128)
-    def _calculate_checksum(self, file_path) :
+    def _calculate_checksum(self, file_path: str) -> str:
+        """
+        Calculate SHA256 checksum for file integrity (SC-003)
+
+        Args:
             file_path: Path to file
 
+        Returns:
+            SHA256 checksum hex string
+        """
+        sha256 = hashlib.sha256()
+
         with open(file_path, 'rb') as f:
-            while chunk ))
+            while chunk := f.read(8192):
+                sha256.update(chunk)
 
         return sha256.hexdigest()
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict[str, Any]:
         """
         Get export statistics
 
         Returns:
             Statistics dictionary
         """
         return {
-            "total_exports",
-            "last_export_time",
-            "last_error") :
+            "total_exports": self.total_exports,
+            "last_export_time": self.last_export_time,
+            "last_error": self.last_error
+        }
+
+    def list_exports(self) -> List[Dict[str, Any]]:
         """
         List all exported files in output directory
 
-        Returns)), filename)
+        Returns:
+            List of export file info
+        """
+        exports = []
+
+        if not os.path.exists(self.config.output_dir):
+            return exports
+
+        for filename in os.listdir(self.config.output_dir):
+            file_path = os.path.join(self.config.output_dir, filename)
             if os.path.isfile(file_path):
                 exports.append({
-                    "filename",
-                    "size"),
-                    "modified"),
-                    "path")
+                    "filename": filename,
+                    "size": os.path.getsize(file_path),
+                    "modified": os.path.getmtime(file_path),
+                    "path": file_path
+                })
 
         return exports
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("Data Exporter Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("Data Exporter Self-Test")
+    print("=" * 60)
 
     # Create test configuration
     config = ExportConfig(
         output_dir="test_exports",
         enable_compression=True,
         enable_checksum=True,
         enable_logging=True
+    )
 
     exporter = DataExporter(config)
 
-    logger.info("\n1. Testing initialization...")
-    logger.info("   OK)
+    print("\n1. Testing initialization...")
+    print("   OK: Initialization")
 
-    logger.info("\n2. Testing statistics...")
+    print("\n2. Testing statistics...")
     stats = exporter.get_statistics()
-    logger.info("   OK)", stats['total_exports'])
+    print(f"   OK: Statistics (total_exports={stats['total_exports']})")
 
-    logger.info("\n3. Testing list exports...")
+    print("\n3. Testing list exports...")
     exports = exporter.list_exports()
-    logger.info("   OK)", len(exports))
-
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
-    logger.info("Note)
+    print(f"   OK: List exports ({len(exports)} files)")
 
-"""  # auto-closed missing docstring
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
+    print("Note: Full export testing requires recorded session")
diff --git a/server/deprecation.py b/server/deprecation.py
index b615de79f81ade2df5e0c3f087747689086ffbf4..37a3777a49f5f4608fd087f4d77ce4e35df15b3e 100644
--- a/server/deprecation.py
+++ b/server/deprecation.py
@@ -1,204 +1,362 @@
 """
 Deprecation utilities for Soundlab + D-ASE
-Feature 026 (FR-004),
+Feature 026 (FR-004): API deprecation warnings and tracking
+
+This module provides decorators and utilities for marking APIs as deprecated,
 issuing warnings, and tracking deprecated functionality for eventual removal.
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import functools
 import warnings
 from typing import Optional, Callable, Any
 
 
 class DeprecationInfo:
-        name,
-        deprecated_in,
-        remove_in,
-        reason,
-        alternative,
+    """Store information about a deprecated item"""
+
+    def __init__(
+        self,
+        name: str,
+        deprecated_in: str,
+        remove_in: str,
+        reason: Optional[str] = None,
+        alternative: Optional[str] = None,
     ):
+        self.name = name
+        self.deprecated_in = deprecated_in
+        self.remove_in = remove_in
+        self.reason = reason
+        self.alternative = alternative
+
+    def __str__(self) -> str:
         msg = f"{self.name} is deprecated since v{self.deprecated_in} and will be removed in v{self.remove_in}."
         if self.reason:
             msg += f" Reason: {self.reason}."
         if self.alternative:
             msg += f" Use {self.alternative} instead."
         return msg
 
 
 # Registry of all deprecated items
 _DEPRECATION_REGISTRY = {}
 
 
 def deprecated(
-    deprecated_in,
-    remove_in,
-    reason,
-    alternative,
-) :
-        deprecated_in, "1.1.0")
-        remove_in, "2.0.0")
+    deprecated_in: str,
+    remove_in: str,
+    reason: Optional[str] = None,
+    alternative: Optional[str] = None,
+) -> Callable:
+    """
+    Decorator to mark a function, method, or class as deprecated.
+
+    Args:
+        deprecated_in: Version when the item was deprecated (e.g., "1.1.0")
+        remove_in: Version when the item will be removed (e.g., "2.0.0")
         reason: Optional reason for deprecation
         alternative: Optional alternative API to use
 
-    Example,
+    Example:
+        @deprecated(
+            deprecated_in="1.1.0",
             remove_in="2.0.0",
             reason="Replaced by optimized implementation",
             alternative="new_function()"
-
-        def old_function() :
+        )
+        def old_function():
             pass
     """
 
-    def decorator(obj) :
+    def decorator(obj: Any) -> Any:
+        # Determine the name
+        if hasattr(obj, "__name__"):
+            name = obj.__name__
+        elif hasattr(obj, "__class__"):
             name = obj.__class__.__name__
-        else)
+        else:
+            name = str(obj)
 
         # Create deprecation info
         info = DeprecationInfo(
             name=name,
             deprecated_in=deprecated_in,
             remove_in=remove_in,
             reason=reason,
             alternative=alternative,
+        )
 
         # Register the deprecation
         _DEPRECATION_REGISTRY[name] = info
 
         # If it's a class, wrap its __init__
-        if isinstance(obj, type))
-            def new_init(self, *args, **kwargs) :
+        if isinstance(obj, type):
+            original_init = obj.__init__
+
+            @functools.wraps(original_init)
+            def new_init(self, *args, **kwargs):
+                warnings.warn(
+                    str(info),
+                    category=DeprecationWarning,
+                    stacklevel=2,
+                )
+                return original_init(self, *args, **kwargs)
+
+            obj.__init__ = new_init
+            return obj
+
+        # If it's a function or method, wrap it
+        @functools.wraps(obj)
+        def wrapper(*args, **kwargs):
+            warnings.warn(
+                str(info),
+                category=DeprecationWarning,
+                stacklevel=2,
+            )
+            return obj(*args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+def deprecated_parameter(
+    param_name: str,
+    deprecated_in: str,
+    remove_in: str,
+    reason: Optional[str] = None,
+    alternative: Optional[str] = None,
+) -> Callable:
     """
     Decorator to mark a function parameter as deprecated.
 
     Args:
         param_name: Name of the deprecated parameter
         deprecated_in: Version when the parameter was deprecated
         remove_in: Version when the parameter will be removed
         reason: Optional reason for deprecation
         alternative: Optional alternative parameter to use
 
-    Example,
+    Example:
+        @deprecated_parameter(
+            "old_param",
             deprecated_in="1.1.0",
             remove_in="2.0.0",
             alternative="new_param"
-
-        def my_function(new_param, old_param) :
+        )
+        def my_function(new_param=None, old_param=None):
             pass
     """
 
-    def decorator(func) :
-            if param_name in kwargs) {info}",
+    def decorator(func: Callable) -> Callable:
+        info = DeprecationInfo(
+            name=f"{func.__name__}.{param_name}",
+            deprecated_in=deprecated_in,
+            remove_in=remove_in,
+            reason=reason,
+            alternative=alternative,
+        )
+
+        _DEPRECATION_REGISTRY[f"{func.__name__}.{param_name}"] = info
+
+        @functools.wraps(func)
+        def wrapper(*args, **kwargs):
+            if param_name in kwargs:
+                warnings.warn(
+                    f"Parameter '{param_name}' of {func.__name__}() {info}",
                     category=DeprecationWarning,
                     stacklevel=2,
-
+                )
             return func(*args, **kwargs)
 
         return wrapper
 
     return decorator
 
 
 def deprecated_alias(
-    old_name,
-    new_name,
-    deprecated_in,
-    remove_in,
-) :
+    old_name: str,
+    new_name: str,
+    deprecated_in: str,
+    remove_in: str,
+) -> Callable:
     """
     Create a deprecated alias for a function or class.
 
     Args:
         old_name: Name of the deprecated alias
         new_name: Name of the new function/class
         deprecated_in: Version when the alias was deprecated
         remove_in: Version when the alias will be removed
 
     Returns:
         A wrapped version of the new function that issues a deprecation warning
 
-    Example) :
+    Example:
+        def new_function():
+            pass
+
+        old_function = deprecated_alias(
+            "old_function",
+            "new_function",
+            deprecated_in="1.1.0",
+            remove_in="2.0.0"
+        )(new_function)
+    """
+
+    def decorator(obj: Any) -> Any:
+        info = DeprecationInfo(
+            name=old_name,
+            deprecated_in=deprecated_in,
+            remove_in=remove_in,
+            reason=f"Use {new_name} instead",
+            alternative=new_name,
+        )
+
+        _DEPRECATION_REGISTRY[old_name] = info
+
+        if isinstance(obj, type):
+            # For classes, create a new class that warns on instantiation
+            class DeprecatedClass(obj):
+                def __init__(self, *args, **kwargs):
+                    warnings.warn(
+                        str(info),
+                        category=DeprecationWarning,
+                        stacklevel=2,
+                    )
+                    super().__init__(*args, **kwargs)
+
+            DeprecatedClass.__name__ = old_name
+            return DeprecatedClass
+
+        # For functions
+        @functools.wraps(obj)
+        def wrapper(*args, **kwargs):
+            warnings.warn(
+                str(info),
+                category=DeprecationWarning,
+                stacklevel=2,
+            )
+            return obj(*args, **kwargs)
+
+        wrapper.__name__ = old_name
+        return wrapper
+
+    return decorator
+
+
+def get_deprecations() -> dict:
     """
     Get all registered deprecations.
 
-def get_deprecations_by_version(remove_in) :
+    Returns:
+        Dictionary mapping deprecated item names to DeprecationInfo objects
+    """
+    return _DEPRECATION_REGISTRY.copy()
+
+
+def get_deprecations_by_version(remove_in: str) -> dict:
     """
     Get deprecations scheduled for removal in a specific version.
 
     Args:
-        remove_in, "2.0.0")
+        remove_in: Version to filter by (e.g., "2.0.0")
 
     Returns:
         Dictionary of deprecations scheduled for removal in that version
     """
     return {
-        name, info in _DEPRECATION_REGISTRY.items()
+        name: info
+        for name, info in _DEPRECATION_REGISTRY.items()
         if info.remove_in == remove_in
     }
 
 
-def print_deprecation_report() :
-        if info.remove_in not in by_version)
-
-    for version in sorted(by_version.keys():
-        logger.info("Scheduled for removal in v%s, version)
-        logger.info("-" * 80)
-        for info in by_version[version], info.name)
-            logger.warning("    Deprecated in, info.deprecated_in)
+def print_deprecation_report():
+    """Print a formatted report of all deprecations."""
+    print("=" * 80)
+    print("Soundlab + D-ASE Deprecation Report")
+    print("=" * 80)
+    print()
+
+    if not _DEPRECATION_REGISTRY:
+        print("No deprecated items.")
+        return
+
+    # Group by removal version
+    by_version = {}
+    for name, info in _DEPRECATION_REGISTRY.items():
+        if info.remove_in not in by_version:
+            by_version[info.remove_in] = []
+        by_version[info.remove_in].append(info)
+
+    for version in sorted(by_version.keys()):
+        print(f"Scheduled for removal in v{version}:")
+        print("-" * 80)
+        for info in by_version[version]:
+            print(f"  • {info.name}")
+            print(f"    Deprecated in: v{info.deprecated_in}")
             if info.reason:
-                logger.info("    Reason, info.reason)
+                print(f"    Reason: {info.reason}")
             if info.alternative:
-                logger.info("    Alternative, info.alternative)
-            logger.info(str())
-        logger.info(str())
+                print(f"    Alternative: {info.alternative}")
+            print()
+        print()
 
 
 # Example usage and tests
 if __name__ == "__main__":
-    # Example 1,
+    # Example 1: Deprecated function
+    @deprecated(
+        deprecated_in="1.1.0",
         remove_in="2.0.0",
         reason="Replaced by optimized implementation",
         alternative="fast_process()",
-
-    @lru_cache(maxsize=128)
-    def slow_process(data) :
+    )
+    def slow_process(data):
         """Old slow processing function."""
         return data
 
-    # Example 2,
+    # Example 2: Deprecated class
+    @deprecated(
+        deprecated_in="1.1.0",
         remove_in="2.0.0",
         reason="Use the new PhiProcessor class",
         alternative="PhiProcessor",
+    )
+    class OldProcessor:
+        """Old processor class."""
 
-    class OldProcessor) :
+        def __init__(self):
             pass
 
-    # Example 3,
+    # Example 3: Deprecated parameter
+    @deprecated_parameter(
+        "old_format",
         deprecated_in="1.1.0",
         remove_in="2.0.0",
         alternative="output_format",
-
-    @lru_cache(maxsize=128)
-    def export_data(data, output_format, old_format) :
+    )
+    def export_data(data, output_format="json", old_format=None):
         """Export data in various formats."""
-        if old_format)
+        if old_format:
+            output_format = old_format
+        return f"Exporting as {output_format}"
+
+    # Print report
+    print_deprecation_report()
 
     # Test the decorators (these will issue warnings)
-    logger.warning("Testing deprecated items (warnings expected))
-    logger.info("-" * 80)
+    print("Testing deprecated items (warnings expected):")
+    print("-" * 80)
 
     # This will warn
     result = slow_process([1, 2, 3])
-    logger.info("Result, result)
+    print(f"Result: {result}")
 
     # This will warn
     processor = OldProcessor()
-    logger.info("Processor, processor)
+    print(f"Processor: {processor}")
 
     # This will warn
     output = export_data([1, 2, 3], old_format="csv")
-    logger.info("Output, output)
-
-"""  # auto-closed missing docstring
+    print(f"Output: {output}")
diff --git a/server/downmix.py b/server/downmix.py
index cd8cee1afe3501ed80079cc0667ccfcdc87e7f0b..75a632b94d8803210045d3085bc3e51b95b0694c 100644
--- a/server/downmix.py
+++ b/server/downmix.py
@@ -1,249 +1,334 @@
 """
 Multi-Channel Downmix Utilities
 
 Converts 8-channel D-ASE output to stereo for browser playback with spatial preservation.
 Implements multiple downmix strategies optimized for different use cases.
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import numpy as np
 from typing import Optional, Literal
 
 
 class StereoDownmixer:
+    """
+    8-channel to stereo downmixer with spatial panning
+
+    Supports multiple downmix strategies to preserve spatial characteristics
+    of the multi-channel D-ASE output while preventing clipping.
+    """
+
+    def __init__(self, num_channels: int = 8, strategy: Literal['spatial', 'energy', 'linear', 'phi'] = 'spatial'):
         """
         Initialize downmixer
 
         Args:
-            num_channels: Number of input channels (default)
+            num_channels: Number of input channels (default: 8)
             strategy: Downmix strategy
                 - 'spatial': Spatial panning with weighted coefficients
-
+                - 'energy': Energy-preserving (RMS-based)
                 - 'linear': Simple averaging
+                - 'phi': Golden-ratio weighted distribution
+        """
+        self.num_channels = num_channels
+        self.strategy = strategy
+        self.gain = 1.0  # Master output gain
+        self._setup_coefficients()
 
-        logger.info("[StereoDownmixer] Initialized with '%s' strategy", strategy)
+        print(f"[StereoDownmixer] Initialized with '{strategy}' strategy")
 
-    @lru_cache(maxsize=128)
-    def _setup_coefficients(self) :
+    def _setup_coefficients(self):
         """Setup mixing coefficients based on selected strategy"""
 
         if self.strategy == 'spatial':
-            # Spatial panning, right side gets 4-7
+            # Spatial panning: channels distributed across stereo field
+            # Left side gets more of channels 0-3, right side gets 4-7
             # Using power-law panning for natural spatial distribution
             self.left_weights = np.array([0.8, 0.6, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)
             self.right_weights = np.array([0.0, 0.0, 0.0, 0.0, 0.2, 0.4, 0.6, 0.8], dtype=np.float32)
             self.normalization = 2.0  # Prevent clipping
 
         elif self.strategy == 'energy':
-            # Energy-preserving, dtype=np.float32) / np.sqrt(8)
+            # Energy-preserving: maintain RMS level
+            # Each channel contributes equally to total energy
+            self.left_weights = np.ones(8, dtype=np.float32) / np.sqrt(8)
             self.right_weights = np.ones(8, dtype=np.float32) / np.sqrt(8)
             self.normalization = 1.0
 
         elif self.strategy == 'linear':
-            # Simple averaging, dtype=np.float32) / 8
+            # Simple averaging: all channels contribute equally
+            self.left_weights = np.ones(8, dtype=np.float32) / 8
             self.right_weights = np.ones(8, dtype=np.float32) / 8
             self.normalization = 1.0
 
-        elif self.strategy == 'phi')
+        elif self.strategy == 'phi':
+            # Golden-ratio weighted distribution
+            # Creates natural harmonic relationships in the downmix
+            PHI_INV = 0.618033988749895
+            weights = np.array([
+                PHI_INV ** i for i in range(8)
             ], dtype=np.float32)
             weights /= np.sum(weights)  # Normalize
 
-            # Distribute, higher indices → right
+            # Distribute: lower indices → left, higher indices → right
             self.left_weights = weights * np.linspace(1.0, 0.0, 8, dtype=np.float32)
             self.right_weights = weights * np.linspace(0.0, 1.0, 8, dtype=np.float32)
             self.normalization = 1.5
 
         else:
-            raise ValueError(f"Unknown downmix strategy)
+            raise ValueError(f"Unknown downmix strategy: {self.strategy}")
 
-    @lru_cache(maxsize=128)
-    def downmix(self, multi_channel) :
+    def downmix(self, multi_channel: np.ndarray) -> np.ndarray:
         """
         Downmix 8 channels to stereo
 
         Args:
-            multi_channel, num_samples] or [num_samples, 8]
+            multi_channel: float32[8, num_samples] or [num_samples, 8]
                 Multi-channel input signal
 
-        Returns, num_samples] stereo output
+        Returns:
+            float32[2, num_samples] stereo output
 
         Raises:
             ValueError: If input doesn't have 8 channels
         """
         # Handle both channel orderings
-        if multi_channel.ndim != 2, got shape {multi_channel.shape}")
+        if multi_channel.ndim != 2:
+            raise ValueError(f"Expected 2D array, got shape {multi_channel.shape}")
 
-        if multi_channel.shape[0] == 8, samples] format
+        if multi_channel.shape[0] == 8:
+            # Already in [channels, samples] format
             channels = multi_channel
-        elif multi_channel.shape[1] == 8, channels]
+        elif multi_channel.shape[1] == 8:
+            # Transpose from [samples, channels]
             channels = multi_channel.T
-        else, got shape {multi_channel.shape}")
+        else:
+            raise ValueError(f"Expected 8 channels, got shape {multi_channel.shape}")
 
         num_samples = channels.shape[1]
 
         # Apply weighted mixing
         left = np.zeros(num_samples, dtype=np.float32)
         right = np.zeros(num_samples, dtype=np.float32)
 
-        for ch_idx in range(8), num_samples]
+        for ch_idx in range(8):
+            left += channels[ch_idx] * self.left_weights[ch_idx]
+            right += channels[ch_idx] * self.right_weights[ch_idx]
+
+        # Apply normalization to prevent clipping
+        left /= self.normalization
+        right /= self.normalization
+
+        # Apply master gain
+        left *= self.gain
+        right *= self.gain
+
+        # Stack to stereo format [2, num_samples]
         stereo = np.vstack([left, right])
 
         return stereo
 
-    @lru_cache(maxsize=128)
-    def downmix_with_monitoring(self, multi_channel) :
+    def downmix_with_monitoring(self, multi_channel: np.ndarray) -> tuple[np.ndarray, dict]:
         """
         Downmix with additional monitoring information
 
         Args:
-            multi_channel, num_samples] input
+            multi_channel: float32[8, num_samples] input
 
-        Returns, monitoring_dict)
+        Returns:
+            Tuple of (stereo_output, monitoring_dict)
                 monitoring_dict contains:
                     - peak_left: Peak level in left channel
                     - peak_right: Peak level in right channel
                     - rms_left: RMS level in left channel
                     - rms_right: RMS level in right channel
-                    - clipped, True if any clipping occurred
+                    - clipped: Boolean, True if any clipping occurred
         """
         stereo = self.downmix(multi_channel)
 
         # Calculate monitoring metrics
         left = stereo[0]
         right = stereo[1]
 
         monitoring = {
-            'peak_left'))),
-            'peak_right'))),
-            'rms_left'))),
-            'rms_right'))),
-            'clipped') > 1.0))
+            'peak_left': float(np.max(np.abs(left))),
+            'peak_right': float(np.max(np.abs(right))),
+            'rms_left': float(np.sqrt(np.mean(left ** 2))),
+            'rms_right': float(np.sqrt(np.mean(right ** 2))),
+            'clipped': bool(np.any(np.abs(stereo) > 1.0))
         }
 
         return stereo, monitoring
 
-    @lru_cache(maxsize=128)
-    def set_strategy(self, strategy: Literal['spatial', 'energy', 'linear', 'phi']) :
+    def set_strategy(self, strategy: Literal['spatial', 'energy', 'linear', 'phi']):
         """
         Change downmix strategy
 
         Args:
-            strategy)
+            strategy: New downmix strategy
+        """
+        self.strategy = strategy
+        self._setup_coefficients()
 
-    @lru_cache(maxsize=128)
-    def set_weights(self, channel: Literal['L', 'R'], weights: np.ndarray) :
+    def set_weights(self, channel: Literal['L', 'R'], weights: np.ndarray):
         """
         Set custom weights for left or right channel
 
         Args:
             channel: 'L' for left or 'R' for right
-            weights)
+            weights: Array of weights (length must match num_channels)
         """
-        if len(weights) != self.num_channels, got {len(weights)}")
+        if len(weights) != self.num_channels:
+            raise ValueError(f"Expected {self.num_channels} weights, got {len(weights)}")
 
-        if channel == 'L', dtype=np.float32)
-        elif channel == 'R', dtype=np.float32)
+        if channel == 'L':
+            self.left_weights = np.array(weights, dtype=np.float32)
+        elif channel == 'R':
+            self.right_weights = np.array(weights, dtype=np.float32)
         else:
-            raise ValueError(f"Invalid channel)
+            raise ValueError(f"Invalid channel: {channel}. Use 'L' or 'R'")
 
-    @lru_cache(maxsize=128)
-    def get_strategy_info(self) :
+    def get_strategy_info(self) -> dict:
         """
         Get information about current downmix strategy
 
         Returns:
             Dictionary with strategy details
         """
         return {
-            'strategy',
-            'left_weights'),
-            'right_weights'),
-            'normalization'),
-            'gain'),
-            'total_left_gain')),
-            'total_right_gain'))
+            'strategy': self.strategy,
+            'left_weights': self.left_weights.tolist(),
+            'right_weights': self.right_weights.tolist(),
+            'normalization': float(self.normalization),
+            'gain': float(self.gain),
+            'total_left_gain': float(np.sum(self.left_weights)),
+            'total_right_gain': float(np.sum(self.right_weights))
         }
 
 
 # Convenience functions
-@lru_cache(maxsize=128)
-def downmix_to_stereo(multi_channel,
-                     strategy) :
+def downmix_to_stereo(multi_channel: np.ndarray,
+                     strategy: str = 'spatial') -> np.ndarray:
     """
     Quick downmix function for simple use cases
 
     Args:
-        multi_channel, num_samples] input
-        strategy)
+        multi_channel: float32[8, num_samples] input
+        strategy: Downmix strategy (see StereoDownmixer)
 
-    Returns, num_samples] stereo output
+    Returns:
+        float32[2, num_samples] stereo output
     """
     downmixer = StereoDownmixer(strategy=strategy)
     return downmixer.downmix(multi_channel)
 
 
-@lru_cache(maxsize=128)
-def adaptive_downmix(multi_channel,
-                    target_lufs) :
+def adaptive_downmix(multi_channel: np.ndarray,
+                    target_lufs: Optional[float] = -18.0) -> np.ndarray:
     """
     Adaptive downmix with automatic loudness normalization
 
     Args:
-        multi_channel, num_samples] input
-        target_lufs)
+        multi_channel: float32[8, num_samples] input
+        target_lufs: Target LUFS level (None = no normalization)
 
-    Returns, num_samples] stereo output
+    Returns:
+        float32[2, num_samples] stereo output
     """
     # Start with spatial downmix
     downmixer = StereoDownmixer(strategy='spatial')
     stereo = downmixer.downmix(multi_channel)
 
-    if target_lufs is not None, but close enough)
+    if target_lufs is not None:
+        # Simplified loudness normalization (not true LUFS, but close enough)
         # True LUFS requires K-weighting filter, but RMS is sufficient for real-time
         rms = np.sqrt(np.mean(stereo ** 2))
-        if rms > 0)
+        if rms > 0:
+            # Convert target LUFS to linear gain (approximate)
             # -18 LUFS ≈ 0.126 RMS for full-scale sine
             target_rms = 0.126 * (10 ** (target_lufs / 20))
             gain = target_rms / rms
             # Limit gain to prevent excessive boost
             gain = np.clip(gain, 0.1, 2.0)
             stereo *= gain
 
     return stereo
 
 
 # Self-test function
-@lru_cache(maxsize=128)
-def _self_test() :
-            logger.info("     %s, key, value)
+def _self_test():
+    """Run basic self-test of downmix module"""
+    print("=" * 60)
+    print("StereoDownmixer Self-Test")
+    print("=" * 60)
+
+    try:
+        # Generate test multi-channel signal
+        print("\n1. Generating 8-channel test signal...")
+        num_samples = 512
+        sample_rate = 48000
+        t = np.linspace(0, num_samples/sample_rate, num_samples, endpoint=False)
+
+        # Each channel has a different frequency
+        test_signal = np.zeros((8, num_samples), dtype=np.float32)
+        for ch in range(8):
+            freq = 440 * (2 ** (ch / 12))  # Chromatic scale
+            test_signal[ch] = 0.3 * np.sin(2 * np.pi * freq * t)
+
+        print(f"   ✓ Generated test signal: {test_signal.shape}")
+        print(f"   RMS per channel: {np.sqrt(np.mean(test_signal**2, axis=1))}")
+
+        # Test all strategies
+        print("\n2. Testing all downmix strategies...")
+        strategies = ['spatial', 'energy', 'linear', 'phi']
+
+        for strategy in strategies:
+            downmixer = StereoDownmixer(strategy=strategy)
+            stereo = downmixer.downmix(test_signal)
+
+            info = downmixer.get_strategy_info()
+            peak = np.max(np.abs(stereo))
+            rms = np.sqrt(np.mean(stereo ** 2))
+
+            print(f"\n   {strategy.upper()} strategy:")
+            print(f"     Output shape: {stereo.shape}")
+            print(f"     Peak level: {peak:.4f}")
+            print(f"     RMS level: {rms:.4f}")
+            print(f"     Clipping: {'YES' if peak > 1.0 else 'NO'}")
+
+        # Test monitoring
+        print("\n3. Testing downmix with monitoring...")
+        downmixer = StereoDownmixer(strategy='spatial')
+        stereo, monitoring = downmixer.downmix_with_monitoring(test_signal)
+
+        print("   Monitoring metrics:")
+        for key, value in monitoring.items():
+            print(f"     {key}: {value}")
 
         # Test convenience functions
-        logger.info("\n4. Testing convenience functions...")
+        print("\n4. Testing convenience functions...")
         quick_stereo = downmix_to_stereo(test_signal, strategy='phi')
-        logger.info("   Quick downmix, quick_stereo.shape)
+        print(f"   Quick downmix: {quick_stereo.shape}")
 
         adaptive_stereo = adaptive_downmix(test_signal, target_lufs=-18.0)
-        logger.info("   Adaptive downmix, adaptive_stereo.shape)
+        print(f"   Adaptive downmix: {adaptive_stereo.shape}")
 
         # Test transposed input
-        logger.info("\n5. Testing transposed input [samples, channels]...")
+        print("\n5. Testing transposed input [samples, channels]...")
         test_transposed = test_signal.T  # [512, 8]
         stereo_transposed = downmix_to_stereo(test_transposed)
-        logger.info("   ✓ Transposed input handled, stereo_transposed.shape)
+        print(f"   ✓ Transposed input handled: {stereo_transposed.shape}")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/hw_interface.py b/server/hw_interface.py
index f86afa3377f5a2278ff7c73b7bc212f38a9fb5fc..5b7ed3e01d21c0fe27ed70f0f3c807978797020c 100644
--- a/server/hw_interface.py
+++ b/server/hw_interface.py
@@ -1,471 +1,606 @@
 """
 Hardware Interface - Feature 023
 Python bridge for I²S hardware synchronization
 
 Communicates with embedded I²S bridge via PySerial for:
 - Consciousness metrics transmission
 - Hardware link status monitoring
 - Clock drift calibration
 - Diagnostic logging
 
 Requirements:
 - FR-007: PySerial/PyUSB communication
 - FR-008: Cluster Monitor integration
 - SC-001: Round-trip latency ≤40 µs
 - SC-002: Metrics loss <0.1%
+- SC-003: Failover recovery <1s
+"""
 
 import serial
 import serial.tools.list_ports
 import struct
 import time
 import threading
 import json
 from typing import Optional, Dict, List, Tuple
 from dataclasses import dataclass, asdict
 from enum import Enum
 
 
-class I2SLinkStatus(Enum))"""
+class I2SLinkStatus(Enum):
+    """Hardware link status (matches C enum)"""
     DISCONNECTED = 0
     SYNCING = 1
     STABLE = 2
     DEGRADED = 3
     ERROR = 4
 
 
 @dataclass
-class ConsciousnessMetrics)"""
-    phi_phase, 2π]
-    phi_depth, 1]
-    coherence, 1]
+class ConsciousnessMetrics:
+    """Consciousness metrics structure (FR-003)"""
+    phi_phase: float           # Φ-phase [0, 2π]
+    phi_depth: float           # Φ-depth [0, 1]
+    coherence: float           # Phase coherence [0, 1]
     criticality: float         # Criticality metric
-    ici)
+    ici: float                 # Inter-Criticality Interval (ms)
     timestamp_us: int          # Microsecond timestamp
     sequence: int              # Sequence number
 
 
 @dataclass
 class I2SStatistics:
+    """Hardware statistics (SC-001, SC-002)"""
     frames_transmitted: int    # Total frames sent
     frames_received: int       # Total frames received
     frames_dropped: int        # Dropped frames
-    latency_us)
-    jitter_us)
-    clock_drift_ppm)
+    latency_us: int            # Round-trip latency (µs)
+    jitter_us: int             # Latency jitter (µs)
+    clock_drift_ppm: float     # Clock drift (parts per million)
     link_status: str           # Current link status
     uptime_ms: int             # Uptime since last reset
     loss_rate: float           # Calculated loss rate
 
 
 class HardwareInterface:
+    """
+    Hardware I²S Bridge Interface
+
+    Provides Python API for communicating with embedded I²S bridge
+    via serial/USB connection.
+    """
+
+    # Protocol constants
+    CMD_INIT = 0x01
+    CMD_START = 0x02
+    CMD_STOP = 0x03
+    CMD_TRANSMIT = 0x04
+    CMD_GET_STATS = 0x05
+    CMD_RESET_STATS = 0x06
+    CMD_SELF_TEST = 0x07
+    CMD_CALIBRATE = 0x08
+    CMD_GET_VERSION = 0x09
+
+    RESP_OK = 0x00
+    RESP_ERROR = 0xFF
+
+    def __init__(self, port: Optional[str] = None, baudrate: int = 115200, enable_logging: bool = True):
         """
         Initialize hardware interface
 
         Args:
-            port)
+            port: Serial port path (auto-detect if None)
             baudrate: Serial baud rate
             enable_logging: Enable diagnostic logging
         """
         self.port = port
         self.baudrate = baudrate
         self.enable_logging = enable_logging
 
         self.serial: Optional[serial.Serial] = None
         self.is_connected = False
         self.is_running = False
 
         # Background thread for receiving data
-        self.receive_thread)
+        self.receive_thread: Optional[threading.Thread] = None
+        self.receive_lock = threading.Lock()
 
         # Statistics
         self.stats = I2SStatistics(
             frames_transmitted=0,
             frames_received=0,
             frames_dropped=0,
             latency_us=0,
             jitter_us=0,
             clock_drift_ppm=0.0,
             link_status=I2SLinkStatus.DISCONNECTED.name,
             uptime_ms=0,
             loss_rate=0.0
+        )
 
         # Callback for metrics received from hardware
         self.metrics_callback = None
 
-        if self.enable_logging)
+        if self.enable_logging:
+            print("[HwInterface] Initialized")
 
-    def list_devices(self) :
+    def list_devices(self) -> List[Dict]:
         """
         List available serial devices
 
+        Returns:
+            List of device info dictionaries
+        """
+        ports = serial.tools.list_ports.comports()
         devices = []
 
         for port in ports:
             devices.append({
-                "port",
-                "description",
-                "hwid",
-                "vid",
-                "pid")
+                "port": port.device,
+                "description": port.description,
+                "hwid": port.hwid,
+                "vid": port.vid,
+                "pid": port.pid
+            })
 
         return devices
 
-    def connect(self) :
+    def connect(self) -> bool:
+        """
+        Connect to hardware device (FR-007)
+
+        Returns:
             True if connected successfully
         """
         if self.is_connected:
             return True
 
         try:
             # Auto-detect port if not specified
-            if self.port is None)
+            if self.port is None:
+                devices = self.list_devices()
 
                 # Look for Teensy or Arduino devices
                 for device in devices:
                     if 'Teensy' in device['description'] or 'Arduino' in device['description']:
                         self.port = device['port']
                         if self.enable_logging:
-                            logger.info("[HwInterface] Auto-detected device, device['description'], self.port)
+                            print(f"[HwInterface] Auto-detected device: {device['description']} on {self.port}")
                         break
 
                 if self.port is None:
-                    if self.enable_logging)
+                    if self.enable_logging:
+                        print("[HwInterface] No compatible device found")
                     return False
 
             # Open serial connection
             self.serial = serial.Serial(
                 port=self.port,
                 baudrate=self.baudrate,
                 timeout=1.0,
                 write_timeout=1.0
+            )
 
             time.sleep(2.0)  # Wait for device reset after serial open
 
             self.is_connected = True
             self.stats.link_status = I2SLinkStatus.SYNCING.name
 
-            if self.enable_logging, self.port, self.baudrate)
+            if self.enable_logging:
+                print(f"[HwInterface] Connected to {self.port} @ {self.baudrate} baud")
 
             return True
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Connection error, e)
+                print(f"[HwInterface] Connection error: {e}")
             return False
 
-    def disconnect(self) :
+    def disconnect(self):
         """Disconnect from hardware device"""
-        if not self.is_connected)
+        if not self.is_connected:
+            return
+
+        self.stop()
 
-        if self.serial)
+        if self.serial:
+            self.serial.close()
             self.serial = None
 
         self.is_connected = False
         self.stats.link_status = I2SLinkStatus.DISCONNECTED.name
 
-        if self.enable_logging)
+        if self.enable_logging:
+            print("[HwInterface] Disconnected")
+
+    def start(self) -> bool:
+        """
+        Start hardware bridge (FR-002)
 
-    def start(self) :
+        Returns:
             True if started successfully
         """
         if not self.is_connected or self.is_running:
             return False
 
-        try)
+        try:
+            # Send START command
+            self._send_command(self.CMD_START)
             response = self._receive_response()
 
-            if response == self.RESP_OK, daemon=True)
+            if response == self.RESP_OK:
+                self.is_running = True
+                self.stats.link_status = I2SLinkStatus.STABLE.name
+
+                # Start receive thread
+                self.receive_thread = threading.Thread(target=self._receive_loop, daemon=True)
                 self.receive_thread.start()
 
-                if self.enable_logging)
+                if self.enable_logging:
+                    print("[HwInterface] Hardware bridge started")
                 return True
             else:
-                if self.enable_logging)
+                if self.enable_logging:
+                    print("[HwInterface] Start command failed")
                 return False
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Start error, e)
+                print(f"[HwInterface] Start error: {e}")
             return False
 
-    def stop(self) :
+    def stop(self) -> bool:
         """
         Stop hardware bridge
 
         Returns:
             True if stopped successfully
         """
         if not self.is_running:
             return False
 
-        try)
+        try:
+            # Send STOP command
+            self._send_command(self.CMD_STOP)
             response = self._receive_response()
 
             self.is_running = False
             self.stats.link_status = I2SLinkStatus.DISCONNECTED.name
 
             # Wait for receive thread to finish
-            if self.receive_thread)
+            if self.receive_thread:
+                self.receive_thread.join(timeout=2.0)
                 self.receive_thread = None
 
-            if self.enable_logging)
+            if self.enable_logging:
+                print("[HwInterface] Hardware bridge stopped")
 
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Stop error, e)
+                print(f"[HwInterface] Stop error: {e}")
             return False
 
-    def transmit_metrics(self, metrics) :
+    def transmit_metrics(self, metrics: ConsciousnessMetrics) -> bool:
+        """
+        Transmit consciousness metrics to hardware (FR-003)
+
+        Args:
             metrics: Consciousness metrics to send
 
         Returns:
             True if transmitted successfully
         """
         if not self.is_running:
             return False
 
-        try)
-            # Format,
+        try:
+            # Pack metrics into binary format (32-byte struct)
+            # Format: 5 floats + 2 uint32
+            packed = struct.pack(
+                '<fffffII',
                 metrics.phi_phase,
                 metrics.phi_depth,
                 metrics.coherence,
                 metrics.criticality,
                 metrics.ici,
                 metrics.timestamp_us,
                 metrics.sequence
+            )
 
             # Send TRANSMIT command with metrics data
             self._send_command(self.CMD_TRANSMIT, packed)
 
             self.stats.frames_transmitted += 1
 
             return True
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Transmit error, e)
+                print(f"[HwInterface] Transmit error: {e}")
             self.stats.frames_dropped += 1
             return False
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
+        """
+        Get hardware statistics (SC-001, SC-002)
+
+        Returns:
             Statistics dictionary
         """
-        if not self.is_connected)
+        if not self.is_connected:
+            return asdict(self.stats)
 
-        try)
+        try:
+            # Send GET_STATS command
+            self._send_command(self.CMD_GET_STATS)
             response = self._receive_response()
 
-            if response == self.RESP_OK)
-                # Format)
-                if len(data) == 44, data)
+            if response == self.RESP_OK:
+                # Receive statistics packet (44 bytes)
+                # Format: 3 uint64 + 2 uint32 + 1 float + 1 uint8 + 1 uint32
+                data = self.serial.read(44)
+                if len(data) == 44:
+                    unpacked = struct.unpack('<QQQIIfBI', data)
 
                     self.stats.frames_transmitted = unpacked[0]
                     self.stats.frames_received = unpacked[1]
                     self.stats.frames_dropped = unpacked[2]
                     self.stats.latency_us = unpacked[3]
                     self.stats.jitter_us = unpacked[4]
                     self.stats.clock_drift_ppm = unpacked[5]
                     link_status_code = unpacked[6]
                     self.stats.uptime_ms = unpacked[7]
 
                     # Convert link status code to enum
-                    try).name
-                    except ValueError)
-                    if self.stats.frames_transmitted > 0)
+                    try:
+                        self.stats.link_status = I2SLinkStatus(link_status_code).name
+                    except ValueError:
+                        self.stats.link_status = I2SLinkStatus.ERROR.name
+
+                    # Calculate loss rate (SC-002)
+                    if self.stats.frames_transmitted > 0:
+                        self.stats.loss_rate = self.stats.frames_dropped / self.stats.frames_transmitted
+
+            return asdict(self.stats)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Get statistics error, e)
+                print(f"[HwInterface] Get statistics error: {e}")
             return asdict(self.stats)
 
-    def reset_statistics(self) :
+    def reset_statistics(self) -> bool:
         """
         Reset statistics counters
 
         Returns:
             True if reset successful
         """
         if not self.is_connected:
             return False
 
-        try)
+        try:
+            self._send_command(self.CMD_RESET_STATS)
             response = self._receive_response()
 
             if response == self.RESP_OK:
                 # Reset local stats
                 self.stats.frames_transmitted = 0
                 self.stats.frames_received = 0
                 self.stats.frames_dropped = 0
                 self.stats.uptime_ms = 0
                 self.stats.loss_rate = 0.0
 
                 return True
 
             return False
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Reset statistics error, e)
+                print(f"[HwInterface] Reset statistics error: {e}")
             return False
 
-    def self_test(self) :
-                # Receive test results (9 bytes)
+    def self_test(self) -> Tuple[bool, int, int]:
+        """
+        Run hardware self-test (FR-010, SC-001)
+
+        Returns:
+            Tuple of (passed, latency_us, jitter_us)
+        """
+        if not self.is_connected:
+            return (False, 0, 0)
+
+        try:
+            # Send SELF_TEST command
+            self._send_command(self.CMD_SELF_TEST)
+            response = self._receive_response()
+
+            if response == self.RESP_OK:
+                # Receive test results (9 bytes: 1 bool + 2 uint32)
                 data = self.serial.read(9)
-                if len(data) == 9, latency_us, jitter_us = struct.unpack('<BII', data)
+                if len(data) == 9:
+                    passed, latency_us, jitter_us = struct.unpack('<BII', data)
 
                     if self.enable_logging:
                         status = "PASSED" if passed else "FAILED"
-                        logger.info("[HwInterface] Self-test %s, jitter=%sµs", status, latency_us, jitter_us)
+                        print(f"[HwInterface] Self-test {status}: latency={latency_us}µs, jitter={jitter_us}µs")
 
                     return (bool(passed), latency_us, jitter_us)
 
             return (False, 0, 0)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Self-test error, e)
+                print(f"[HwInterface] Self-test error: {e}")
             return (False, 0, 0)
 
-    def calibrate_drift(self) :
+    def calibrate_drift(self) -> float:
+        """
+        Calibrate clock drift (FR-005)
+
+        Returns:
+            Measured drift in parts per million (ppm)
+        """
+        if not self.is_connected:
             return 0.0
 
-        try)
+        try:
+            # Send CALIBRATE command
+            self._send_command(self.CMD_CALIBRATE)
             response = self._receive_response()
 
             if response == self.RESP_OK:
-                # Receive drift measurement (4 bytes)
+                # Receive drift measurement (4 bytes: float)
                 data = self.serial.read(4)
-                if len(data) == 4, data)[0]
+                if len(data) == 4:
+                    drift_ppm = struct.unpack('<f', data)[0]
 
                     self.stats.clock_drift_ppm = drift_ppm
 
                     if self.enable_logging:
-                        logger.info("[HwInterface] Clock drift, drift_ppm)
+                        print(f"[HwInterface] Clock drift: {drift_ppm:.3f} ppm")
 
                     return drift_ppm
 
             return 0.0
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Calibrate error, e)
+                print(f"[HwInterface] Calibrate error: {e}")
             return 0.0
 
-    def get_version(self) :
+    def get_version(self) -> str:
         """
         Get firmware version string
 
         Returns:
             Firmware version
         """
         if not self.is_connected:
             return "Unknown"
 
-        try)
+        try:
+            # Send GET_VERSION command
+            self._send_command(self.CMD_GET_VERSION)
             response = self._receive_response()
 
-            if response == self.RESP_OK, max 32 bytes)
+            if response == self.RESP_OK:
+                # Receive version string (null-terminated, max 32 bytes)
                 version = self.serial.read_until(b'\x00', 32).decode('utf-8').strip('\x00')
                 return version
 
             return "Unknown"
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HwInterface] Get version error, e)
+                print(f"[HwInterface] Get version error: {e}")
             return "Unknown"
 
-    def _send_command(self, cmd: int, data: bytes) :
+    def _send_command(self, cmd: int, data: bytes = b''):
         """Send command to hardware"""
-        if not self.serial)
+        if not self.serial:
+            raise RuntimeError("Serial not connected")
 
-        # Command format)
+        # Command format: [0xAA] [CMD] [LEN_L] [LEN_H] [DATA...] [CHECKSUM]
+        length = len(data)
         packet = bytes([0xAA, cmd, length & 0xFF, (length >> 8) & 0xFF]) + data
 
         # Calculate checksum (simple XOR)
         checksum = 0
-        for b in packet)
+        for b in packet:
+            checksum ^= b
+
+        packet += bytes([checksum])
 
         self.serial.write(packet)
         self.serial.flush()
 
-    def _receive_response(self, timeout) :
+    def _receive_response(self, timeout: float = 1.0) -> int:
         """Receive response from hardware"""
-        if not self.serial)
+        if not self.serial:
+            raise RuntimeError("Serial not connected")
 
         start_time = time.time()
 
         while (time.time() - start_time) < timeout:
-            if self.serial.in_waiting >= 1))
+            if self.serial.in_waiting >= 1:
+                return ord(self.serial.read(1))
 
         return self.RESP_ERROR
 
-    def _receive_loop(self) :
+    def _receive_loop(self):
         """Background thread for receiving metrics from hardware"""
         while self.is_running:
             try:
-                if self.serial and self.serial.in_waiting >= 32)
-                    if len(data) == 32, data)
+                if self.serial and self.serial.in_waiting >= 32:  # Size of ConsciousnessMetrics
+                    # Read metrics packet
+                    data = self.serial.read(32)
+                    if len(data) == 32:
+                        # Unpack metrics
+                        unpacked = struct.unpack('<fffffII', data)
 
                         metrics = ConsciousnessMetrics(
                             phi_phase=unpacked[0],
                             phi_depth=unpacked[1],
                             coherence=unpacked[2],
                             criticality=unpacked[3],
                             ici=unpacked[4],
                             timestamp_us=unpacked[5],
                             sequence=unpacked[6]
+                        )
 
                         self.stats.frames_received += 1
 
                         # Call callback if set
-                        if self.metrics_callback)
+                        if self.metrics_callback:
+                            self.metrics_callback(metrics)
 
                 time.sleep(0.01)  # 100 Hz polling
 
             except Exception as e:
                 if self.enable_logging:
-                    logger.error("[HwInterface] Receive error, e)
+                    print(f"[HwInterface] Receive error: {e}")
                 time.sleep(0.1)
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("Hardware Interface Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("Hardware Interface Self-Test")
+    print("=" * 60)
 
     # List available devices
     hw = HardwareInterface()
     devices = hw.list_devices()
 
-    logger.info("\nAvailable devices, len(devices))
+    print(f"\nAvailable devices: {len(devices)}")
     for i, device in enumerate(devices):
-        logger.info("  [%s] %s, i, device['port'], device['description'])
-
-    if not devices)
-        logger.info("\nSkipping connection test (no hardware detected)")
-    else, devices[0]['port'])
+        print(f"  [{i}] {device['port']}: {device['description']}")
+
+    if not devices:
+        print("  No devices found")
+        print("\nSkipping connection test (no hardware detected)")
+    else:
+        # Try to connect to first device
+        print(f"\nAttempting connection to {devices[0]['port']}...")
         if hw.connect():
-            logger.info("  OK)
+            print("  OK: Connected")
 
             # Get version
             version = hw.get_version()
-            logger.info("  Firmware version, version)
+            print(f"  Firmware version: {version}")
 
             # Get statistics
             stats = hw.get_statistics()
-            logger.info("  Link status, stats['link_status'])
+            print(f"  Link status: {stats['link_status']}")
 
             # Disconnect
             hw.disconnect()
-            logger.info("  OK)
+            print("  OK: Disconnected")
         else:
-            logger.error("  FAILED)
-
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test Complete")
-    logger.info("=" * 60)
+            print("  FAILED: Could not connect")
 
-"""  # auto-closed missing docstring
+    print("\n" + "=" * 60)
+    print("Self-Test Complete")
+    print("=" * 60)
diff --git a/server/hybrid_bridge.py b/server/hybrid_bridge.py
index f1e08dd8f31ae367462cd30f627615fb3fbe5142..97ac5d356e63203cc0b0f49719fab4192d44c51b 100644
--- a/server/hybrid_bridge.py
+++ b/server/hybrid_bridge.py
@@ -1,598 +1,747 @@
 """
 Hybrid Analog-DSP Node Bridge - Feature 024
 Python interface for hybrid analog/digital processing node
 
-Communicates with hybrid node hardware via PySerial for, coherence, spectral analysis)
-
-
+Communicates with hybrid node hardware via PySerial for:
+- ADC/DAC configuration and control
+- Real-time DSP metrics (ICI, coherence, spectral analysis)
+- Analog modulation control (VCA, control voltages)
+- Safety monitoring (voltage clamp, thermal)
 - Calibration routines
 
 Requirements:
 - FR-005: Interface via PySerial or PyUSB
 - FR-006: Integrate with metrics bus /ws/metrics
 - FR-007: Safety monitoring and emergency shutdown
 - FR-008: Calibration routine
 - FR-009: Cluster Monitor integration
 
 Success Criteria:
 - SC-001: Total loop latency ≤2 ms
 - SC-002: Modulation fidelity >95%
-- SC-003, drift <0.5%
+- SC-003: Stable operation 1 h, drift <0.5%
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import serial
 import serial.tools.list_ports
 import struct
 import time
 import threading
 import json
 from typing import Optional, Dict, List
 from dataclasses import dataclass, asdict
 from enum import Enum
 
 
-class HybridInterfaceType(Enum)):
+class HybridInterfaceType(Enum):
+    """ADC/DAC interface type"""
+    I2S = 0
+    SPI = 1
+    USB = 2
+
+
+class HybridNodeMode(Enum):
+    """Node operational mode"""
+    ANALOG_ONLY = 0
+    DSP_ONLY = 1
+    HYBRID = 2
+    CALIBRATION = 3
+
+
+class HybridSafetyStatus(Enum):
     """Safety status"""
     OK = 0
     VOLTAGE_CLAMP = 1
     TEMP_WARNING = 2
     TEMP_CRITICAL = 3
     ADC_OVERLOAD = 4
     FAULT = 5
 
 
 @dataclass
-class AnalogMetrics)"""
+class AnalogMetrics:
+    """Analog signal metrics (FR-002)"""
     rms_level: float          # RMS signal level
     peak_level: float         # Peak signal level
     dc_offset: float          # DC offset
-    thd)
-    snr_db)
+    thd: float                # Total harmonic distortion (%)
+    snr_db: float             # Signal-to-noise ratio (dB)
     is_overloaded: bool       # Overload flag
 
 
 @dataclass
-class DSPMetrics)"""
-    ici)
-    coherence, 1]
+class DSPMetrics:
+    """DSP analysis results (FR-003)"""
+    ici: float                # Inter-Criticality Interval (ms)
+    coherence: float          # Phase coherence [0, 1]
     criticality: float        # Criticality metric
-    spectral_centroid)
+    spectral_centroid: float  # Spectral centroid (Hz)
     spectral_flux: float      # Spectral flux
     zero_crossing_rate: float # Zero-crossing rate
     timestamp_us: int         # Microsecond timestamp
 
 
 @dataclass
-class ControlVoltage)"""
-    cv1) - VCA depth
-    cv2) - VCA rate
-    phi_phase, 2π]
-    phi_depth, 1]
+class ControlVoltage:
+    """Control voltage output (FR-004)"""
+    cv1: float                # Control voltage 1 (0-5V) - VCA depth
+    cv2: float                # Control voltage 2 (0-5V) - VCA rate
+    phi_phase: float          # Φ-phase modulation [0, 2π]
+    phi_depth: float          # Φ-depth modulation [0, 1]
 
 
 @dataclass
-class SafetyTelemetry)"""
+class SafetyTelemetry:
+    """Safety monitoring telemetry (FR-007)"""
     status: str               # Safety status
-    temperature)
+    temperature: float        # Temperature (°C)
     voltage_out: List[float]  # Output voltages
     overload_count: int       # Overload event count
     clamp_count: int          # Voltage clamp count
     thermal_warning: bool     # Thermal warning flag
 
 
 @dataclass
-class CalibrationData)"""
+class CalibrationData:
+    """Calibration data (FR-008)"""
     adc_gain: List[float]     # ADC gain correction per channel
     adc_offset: List[float]   # ADC offset correction per channel
     dac_gain: List[float]     # DAC gain correction per channel
     dac_offset: List[float]   # DAC offset correction per channel
-    adc_latency_us)
-    dsp_latency_us)
-    dac_latency_us)
-    total_latency_us) (SC-001)
+    adc_latency_us: int       # ADC latency (µs)
+    dsp_latency_us: int       # DSP latency (µs)
+    dac_latency_us: int       # DAC latency (µs)
+    total_latency_us: int     # Total loop latency (µs) (SC-001)
     calibration_timestamp: int # Calibration timestamp
     is_calibrated: bool       # Calibration valid
 
 
 @dataclass
-class NodeStatistics)"""
+class NodeStatistics:
+    """Node statistics (SC-003)"""
     frames_processed: int     # Total frames processed
     frames_dropped: int       # Dropped frames
-    cpu_load)
-    buffer_utilization)
-    uptime_ms)
-    drift_ppm)
-    modulation_fidelity) (SC-002)
+    cpu_load: float           # CPU load (%)
+    buffer_utilization: float # Buffer utilization (%)
+    uptime_ms: int            # Uptime (ms)
+    drift_ppm: float          # Clock drift (ppm)
+    modulation_fidelity: float # Modulation fidelity (%) (SC-002)
 
 
 class HybridBridge:
+    """
+    Hybrid Analog-DSP Node Bridge
+
+    Provides Python API for communicating with hybrid processing nodes
+    via serial/USB connection.
+    """
+
+    # Protocol commands
+    CMD_INIT = 0x10
+    CMD_START = 0x11
+    CMD_STOP = 0x12
+    CMD_GET_STATUS = 0x13
+    CMD_GET_DSP_METRICS = 0x14
+    CMD_GET_SAFETY = 0x15
+    CMD_SET_PREAMP_GAIN = 0x16
+    CMD_SET_CONTROL_VOLTAGE = 0x17
+    CMD_CALIBRATE = 0x18
+    CMD_LOAD_CALIBRATION = 0x19
+    CMD_SAVE_CALIBRATION = 0x1A
+    CMD_RESET_STATS = 0x1B
+    CMD_SET_MODE = 0x1C
+    CMD_EMERGENCY_SHUTDOWN = 0x1D
+    CMD_GET_VERSION = 0x1E
+
+    RESP_OK = 0x00
+    RESP_ERROR = 0xFF
+
+    def __init__(self, port: Optional[str] = None, baudrate: int = 115200, enable_logging: bool = True):
         """
         Initialize hybrid bridge
 
         Args:
-            port)
+            port: Serial port path (auto-detect if None)
             baudrate: Serial baud rate
             enable_logging: Enable diagnostic logging
         """
         self.port = port
         self.baudrate = baudrate
         self.enable_logging = enable_logging
 
         self.serial: Optional[serial.Serial] = None
         self.is_connected = False
         self.is_running = False
 
         # Background thread for metrics streaming
-        self.metrics_thread)
+        self.metrics_thread: Optional[threading.Thread] = None
+        self.metrics_lock = threading.Lock()
         self.metrics_callback = None
 
         # Current state
         self.current_mode = HybridNodeMode.HYBRID
         self.analog_metrics = None
         self.dsp_metrics = None
         self.safety_telemetry = None
         self.calibration = None
         self.statistics = None
 
-        if self.enable_logging)
+        if self.enable_logging:
+            print("[HybridBridge] Initialized")
 
-    def list_devices(self) :
+    def list_devices(self) -> List[Dict]:
         """
         List available serial devices
 
+        Returns:
+            List of device info dictionaries
+        """
+        ports = serial.tools.list_ports.comports()
         devices = []
 
         for port in ports:
             devices.append({
-                "port",
-                "description",
-                "hwid",
-                "vid",
-                "pid")
+                "port": port.device,
+                "description": port.description,
+                "hwid": port.hwid,
+                "vid": port.vid,
+                "pid": port.pid
+            })
 
         return devices
 
-    def connect(self) :
+    def connect(self) -> bool:
+        """
+        Connect to hybrid node device (FR-005)
+
+        Returns:
             True if connected successfully
         """
         if self.is_connected:
             return True
 
         try:
             # Auto-detect port if not specified
-            if self.port is None)
+            if self.port is None:
+                devices = self.list_devices()
 
                 # Look for Teensy, Arduino, or Raspberry Pi devices
-                for device in devices)
+                for device in devices:
+                    desc = device['description'].lower()
                     if 'teensy' in desc or 'arduino' in desc or 'raspberry' in desc:
                         self.port = device['port']
                         if self.enable_logging:
-                            logger.info("[HybridBridge] Auto-detected, device['description'], self.port)
+                            print(f"[HybridBridge] Auto-detected: {device['description']} on {self.port}")
                         break
 
                 if self.port is None:
-                    if self.enable_logging)
+                    if self.enable_logging:
+                        print("[HybridBridge] No compatible device found")
                     return False
 
             # Open serial connection
             self.serial = serial.Serial(
                 port=self.port,
                 baudrate=self.baudrate,
                 timeout=1.0,
                 write_timeout=1.0
+            )
 
             time.sleep(2.0)  # Wait for device reset
 
             self.is_connected = True
 
-            if self.enable_logging, self.port, self.baudrate)
+            if self.enable_logging:
+                print(f"[HybridBridge] Connected to {self.port} @ {self.baudrate} baud")
 
             return True
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Connection error, e)
+                print(f"[HybridBridge] Connection error: {e}")
             return False
 
-    @lru_cache(maxsize=128)
-    def disconnect(self) :
+    def disconnect(self):
         """Disconnect from hybrid node"""
-        if not self.is_connected)
+        if not self.is_connected:
+            return
 
-        if self.serial)
+        self.stop()
+
+        if self.serial:
+            self.serial.close()
             self.serial = None
 
         self.is_connected = False
 
-        if self.enable_logging)
+        if self.enable_logging:
+            print("[HybridBridge] Disconnected")
 
-    @lru_cache(maxsize=128)
-    def start(self) :
+    def start(self) -> bool:
         """
         Start hybrid node processing
 
         Returns:
             True if started successfully
         """
         if not self.is_connected or self.is_running:
             return False
 
-        try)
+        try:
+            self._send_command(self.CMD_START)
             response = self._receive_response()
 
-            if response == self.RESP_OK, daemon=True)
+            if response == self.RESP_OK:
+                self.is_running = True
+
+                # Start metrics streaming thread
+                self.metrics_thread = threading.Thread(target=self._metrics_loop, daemon=True)
                 self.metrics_thread.start()
 
-                if self.enable_logging)
+                if self.enable_logging:
+                    print("[HybridBridge] Node started")
                 return True
             else:
-                if self.enable_logging)
+                if self.enable_logging:
+                    print("[HybridBridge] Start command failed")
                 return False
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Start error, e)
+                print(f"[HybridBridge] Start error: {e}")
             return False
 
-    @lru_cache(maxsize=128)
-    def stop(self) :
+    def stop(self) -> bool:
         """
         Stop hybrid node processing
 
         Returns:
             True if stopped successfully
         """
         if not self.is_running:
             return False
 
-        try)
+        try:
+            self._send_command(self.CMD_STOP)
             response = self._receive_response()
 
             self.is_running = False
 
             # Wait for metrics thread to finish
-            if self.metrics_thread)
+            if self.metrics_thread:
+                self.metrics_thread.join(timeout=2.0)
                 self.metrics_thread = None
 
-            if self.enable_logging)
+            if self.enable_logging:
+                print("[HybridBridge] Node stopped")
 
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Stop error, e)
+                print(f"[HybridBridge] Stop error: {e}")
             return False
 
-    def get_dsp_metrics(self) :
+    def get_dsp_metrics(self) -> Dict:
+        """
+        Get DSP metrics (FR-003)
+
+        Returns:
             DSP metrics dictionary
         """
         if not self.is_connected:
             return {}
 
-        try)
+        try:
+            self._send_command(self.CMD_GET_DSP_METRICS)
             response = self._receive_response()
 
-            if response == self.RESP_OK)
-                # Format)
-                if len(data) == 28, data)
+            if response == self.RESP_OK:
+                # Receive DSP metrics packet (36 bytes)
+                # Format: 6 floats + 1 uint32
+                data = self.serial.read(28)
+                if len(data) == 28:
+                    unpacked = struct.unpack('<ffffffI', data)
 
                     self.dsp_metrics = DSPMetrics(
                         ici=unpacked[0],
                         coherence=unpacked[1],
                         criticality=unpacked[2],
                         spectral_centroid=unpacked[3],
                         spectral_flux=unpacked[4],
                         zero_crossing_rate=unpacked[5],
                         timestamp_us=unpacked[6]
+                    )
 
                     return asdict(self.dsp_metrics)
 
             return {}
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Get DSP metrics error, e)
+                print(f"[HybridBridge] Get DSP metrics error: {e}")
             return {}
 
-    def get_safety(self) :
+    def get_safety(self) -> Dict:
+        """
+        Get safety telemetry (FR-007)
+
+        Returns:
             Safety telemetry dictionary
         """
         if not self.is_connected:
             return {}
 
-        try)
+        try:
+            self._send_command(self.CMD_GET_SAFETY)
             response = self._receive_response()
 
-            if response == self.RESP_OK)
-                # Format)
-                if len(data) == 26, data)
+            if response == self.RESP_OK:
+                # Receive safety telemetry (26 bytes)
+                # Format: 1 uint8 + 1 float + 4 floats + 2 uint32 + 1 bool
+                data = self.serial.read(26)
+                if len(data) == 26:
+                    unpacked = struct.unpack('<BfffffIIB', data)
 
                     self.safety_telemetry = SafetyTelemetry(
                         status=HybridSafetyStatus(unpacked[0]).name,
                         temperature=unpacked[1],
                         voltage_out=[unpacked[2], unpacked[3], unpacked[4], unpacked[5]],
                         overload_count=unpacked[6],
                         clamp_count=unpacked[7],
                         thermal_warning=bool(unpacked[8])
+                    )
 
                     return asdict(self.safety_telemetry)
 
             return {}
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Get safety error, e)
+                print(f"[HybridBridge] Get safety error: {e}")
             return {}
 
-    def set_preamp_gain(self, gain) :
-            gain, 1.0 = 0dB)
+    def set_preamp_gain(self, gain: float) -> bool:
+        """
+        Set analog preamp gain (FR-002)
+
+        Args:
+            gain: Gain factor (linear, 1.0 = 0dB)
 
         Returns:
             True if set successfully
         """
         if not self.is_connected:
             return False
 
-        try, gain)
+        try:
+            # Pack gain as float
+            packed = struct.pack('<f', gain)
             self._send_command(self.CMD_SET_PREAMP_GAIN, packed)
             response = self._receive_response()
 
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Set preamp gain error, e)
+                print(f"[HybridBridge] Set preamp gain error: {e}")
             return False
 
-    def set_control_voltage(self, cv) :
+    def set_control_voltage(self, cv: ControlVoltage) -> bool:
+        """
+        Set control voltage output (FR-004)
+
+        Args:
             cv: Control voltage structure
 
         Returns:
             True if set successfully
         """
         if not self.is_connected:
             return False
 
-        try)
+        try:
+            # Pack control voltage (4 floats)
             packed = struct.pack('<ffff', cv.cv1, cv.cv2, cv.phi_phase, cv.phi_depth)
             self._send_command(self.CMD_SET_CONTROL_VOLTAGE, packed)
             response = self._receive_response()
 
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Set control voltage error, e)
+                print(f"[HybridBridge] Set control voltage error: {e}")
             return False
 
-    def calibrate(self) :
+    def calibrate(self) -> Optional[Dict]:
+        """
+        Run calibration routine (FR-008, SC-001)
+
+        Returns:
+            Calibration data dictionary if successful, None otherwise
+        """
+        if not self.is_connected:
             return None
 
         try:
-            if self.enable_logging)...")
+            if self.enable_logging:
+                print("[HybridBridge] Starting calibration (this may take 10-20 seconds)...")
 
             self._send_command(self.CMD_CALIBRATE)
             response = self._receive_response()
 
-            if response == self.RESP_OK)
+            if response == self.RESP_OK:
+                # Receive calibration data (large packet)
                 # Wait longer for calibration to complete
                 time.sleep(5.0)
 
                 # Read calibration data
                 # This is a simplified version; actual implementation would
                 # receive the full CalibrationData structure
                 data = self.serial.read(64)  # Approximate size
-                if len(data) >= 16, data[0)[0]
-                    is_calibrated = struct.unpack('<B', data[4)[0]
+                if len(data) >= 16:
+                    # Parse basic calibration info
+                    total_latency_us = struct.unpack('<I', data[0:4])[0]
+                    is_calibrated = struct.unpack('<B', data[4:5])[0]
 
                     self.calibration = CalibrationData(
                         adc_gain=[1.0, 1.0],
                         adc_offset=[0.0, 0.0],
                         dac_gain=[1.0, 1.0, 1.0, 1.0],
                         dac_offset=[0.0, 0.0, 0.0, 0.0],
                         adc_latency_us=total_latency_us // 3,
                         dsp_latency_us=total_latency_us // 3,
                         dac_latency_us=total_latency_us // 3,
                         total_latency_us=total_latency_us,
                         calibration_timestamp=int(time.time()),
                         is_calibrated=bool(is_calibrated)
+                    )
 
-                    if self.enable_logging)
-                        logger.info("  Total latency, total_latency_us)
-                        logger.error("  SC-001 (≤2000 µs), 'PASS' if total_latency_us <= 2000 else 'FAIL')
+                    if self.enable_logging:
+                        print(f"[HybridBridge] Calibration complete")
+                        print(f"  Total latency: {total_latency_us} µs")
+                        print(f"  SC-001 (≤2000 µs): {'PASS' if total_latency_us <= 2000 else 'FAIL'}")
 
                     return asdict(self.calibration)
 
             return None
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Calibration error, e)
+                print(f"[HybridBridge] Calibration error: {e}")
             return None
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
+        """
+        Get node statistics (SC-003)
+
+        Returns:
             Statistics dictionary
         """
         if not self.is_connected:
             return {}
 
-        try)
+        try:
+            self._send_command(self.CMD_GET_STATUS)
             response = self._receive_response()
 
-            if response == self.RESP_OK)
+            if response == self.RESP_OK:
+                # Receive statistics (28 bytes)
                 data = self.serial.read(28)
-                if len(data) == 28, data)
+                if len(data) == 28:
+                    unpacked = struct.unpack('<QQffffI', data)
 
                     self.statistics = NodeStatistics(
                         frames_processed=unpacked[0],
                         frames_dropped=unpacked[1],
                         cpu_load=unpacked[2],
                         buffer_utilization=unpacked[3],
                         uptime_ms=unpacked[6],
                         drift_ppm=unpacked[4],
                         modulation_fidelity=unpacked[5]
+                    )
 
                     return asdict(self.statistics)
 
             return {}
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Get statistics error, e)
+                print(f"[HybridBridge] Get statistics error: {e}")
             return {}
 
-    def reset_statistics(self) :
+    def reset_statistics(self) -> bool:
         """Reset statistics counters"""
         if not self.is_connected:
             return False
 
-        try)
+        try:
+            self._send_command(self.CMD_RESET_STATS)
             response = self._receive_response()
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Reset statistics error, e)
+                print(f"[HybridBridge] Reset statistics error: {e}")
             return False
 
-    def set_mode(self, mode) :
+    def set_mode(self, mode: HybridNodeMode) -> bool:
         """Set operational mode"""
         if not self.is_connected:
             return False
 
-        try, mode.value)
+        try:
+            packed = struct.pack('<B', mode.value)
             self._send_command(self.CMD_SET_MODE, packed)
             response = self._receive_response()
 
             if response == self.RESP_OK:
                 self.current_mode = mode
                 return True
 
             return False
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Set mode error, e)
+                print(f"[HybridBridge] Set mode error: {e}")
             return False
 
-    def emergency_shutdown(self, reason) :
+    def emergency_shutdown(self, reason: str) -> bool:
+        """
+        Emergency shutdown (FR-007)
+
+        Args:
             reason: Reason for shutdown
 
         Returns:
             True if shutdown successful
         """
         if not self.is_connected:
             return False
 
         try:
             if self.enable_logging:
-                logger.info("[HybridBridge] EMERGENCY SHUTDOWN, reason)
+                print(f"[HybridBridge] EMERGENCY SHUTDOWN: {reason}")
 
             self._send_command(self.CMD_EMERGENCY_SHUTDOWN)
             response = self._receive_response()
 
             self.is_running = False
 
             return (response == self.RESP_OK)
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Emergency shutdown error, e)
+                print(f"[HybridBridge] Emergency shutdown error: {e}")
             return False
 
-    def get_version(self) :
+    def get_version(self) -> str:
         """Get firmware version"""
         if not self.is_connected:
             return "Unknown"
 
-        try)
+        try:
+            self._send_command(self.CMD_GET_VERSION)
             response = self._receive_response()
 
-            if response == self.RESP_OK, 32).decode('utf-8').strip('\x00')
+            if response == self.RESP_OK:
+                version = self.serial.read_until(b'\x00', 32).decode('utf-8').strip('\x00')
                 return version
 
             return "Unknown"
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[HybridBridge] Get version error, e)
+                print(f"[HybridBridge] Get version error: {e}")
             return "Unknown"
 
-    def _send_command(self, cmd: int, data: bytes) :
+    def _send_command(self, cmd: int, data: bytes = b''):
         """Send command to hybrid node"""
-        if not self.serial)
+        if not self.serial:
+            raise RuntimeError("Serial not connected")
 
-        # Command format)
+        # Command format: [0xAA] [CMD] [LEN_L] [LEN_H] [DATA...] [CHECKSUM]
+        length = len(data)
         packet = bytes([0xAA, cmd, length & 0xFF, (length >> 8) & 0xFF]) + data
 
         # Calculate checksum (XOR)
         checksum = 0
-        for b in packet)
+        for b in packet:
+            checksum ^= b
+
+        packet += bytes([checksum])
 
         self.serial.write(packet)
         self.serial.flush()
 
-    def _receive_response(self, timeout) :
+    def _receive_response(self, timeout: float = 1.0) -> int:
         """Receive response from hybrid node"""
-        if not self.serial)
+        if not self.serial:
+            raise RuntimeError("Serial not connected")
 
         start_time = time.time()
 
         while (time.time() - start_time) < timeout:
-            if self.serial.in_waiting >= 1))
+            if self.serial.in_waiting >= 1:
+                return ord(self.serial.read(1))
 
         return self.RESP_ERROR
 
-    def _metrics_loop(self) :
-            try)
+    def _metrics_loop(self):
+        """Background thread for metrics streaming (FR-006)"""
+        while self.is_running:
+            try:
+                # Poll DSP metrics
+                metrics = self.get_dsp_metrics()
 
-                if metrics and self.metrics_callback)
+                if metrics and self.metrics_callback:
+                    self.metrics_callback(metrics)
 
                 time.sleep(0.033)  # ~30 Hz update rate
 
             except Exception as e:
                 if self.enable_logging:
-                    logger.error("[HybridBridge] Metrics loop error, e)
+                    print(f"[HybridBridge] Metrics loop error: {e}")
                 time.sleep(0.1)
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("Hybrid Analog-DSP Node Bridge Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("Hybrid Analog-DSP Node Bridge Self-Test")
+    print("=" * 60)
 
     # List available devices
     bridge = HybridBridge()
     devices = bridge.list_devices()
 
-    logger.info("\nAvailable devices, len(devices))
+    print(f"\nAvailable devices: {len(devices)}")
     for i, device in enumerate(devices):
-        logger.info("  [%s] %s, i, device['port'], device['description'])
-
-    if not devices)
-        logger.info("\nSkipping connection test (no hardware detected)")
-    else, devices[0]['port'])
+        print(f"  [{i}] {device['port']}: {device['description']}")
+
+    if not devices:
+        print("  No devices found")
+        print("\nSkipping connection test (no hardware detected)")
+    else:
+        # Try to connect
+        print(f"\nAttempting connection to {devices[0]['port']}...")
         if bridge.connect():
-            logger.info("  OK)
+            print("  OK: Connected")
 
             # Get version
             version = bridge.get_version()
-            logger.info("  Firmware version, version)
+            print(f"  Firmware version: {version}")
 
             # Get statistics
             stats = bridge.get_statistics()
-            logger.info("  Statistics, stats)
+            print(f"  Statistics: {stats}")
 
             # Disconnect
             bridge.disconnect()
-            logger.info("  OK)
+            print("  OK: Disconnected")
         else:
-            logger.error("  FAILED)
+            print("  FAILED: Could not connect")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test Complete")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test Complete")
+    print("=" * 60)
diff --git a/server/hybrid_node.py b/server/hybrid_node.py
index 24c99f733d16b632fda20c83520b58f32b844dbe..3840e91ade7f0b35eec898d2734f2ea815eeb0f7 100644
--- a/server/hybrid_node.py
+++ b/server/hybrid_node.py
@@ -1,804 +1,1139 @@
 """
-HybridNode - Feature 025)
+HybridNode - Feature 025: Hybrid Node Integration (Analog–Digital Bridge)
 
 Integrates D-ASE engine with real/simulated analog I/O and external Φ modulation sources.
 
-Features, sensor, manual slider)
+Features:
+- Real-time audio I/O using sounddevice callbacks
+- Integration with ChromaticFieldProcessor for analog signal processing
+- External Φ modulation source support (microphone, sensor, manual slider)
 - Multi-channel (8-channel) @ 48 kHz processing
-
+- Real-time metrics broadcasting (ICI, coherence, centroid)
+- Low-latency operation (<2 ms)
 
 Requirements:
 - FR-001: HybridNode class manages analog I/O
 - FR-002: Real-time audio I/O using sounddevice callback
 - FR-006: Process 8 channels @ 48 kHz with < 2 ms latency
-- FR-007, coherence, centroid) ≤ 100 ms interval
+- FR-007: Broadcast metrics (ICI, coherence, centroid) ≤ 100 ms interval
 
 Success Criteria:
 - SC-001: Hybrid Mode operational without audio dropouts
 - SC-002: Φ input propagates to metrics < 2 ms latency
 - SC-003: ICI and coherence update visible @ ≥ 30 Hz
+- SC-004: System remains below 50% CPU load under 8-channel processing
+"""
 
 import numpy as np
 import sounddevice as sd
 import threading
 import time
 import queue
 from typing import Optional, Callable, Dict, List
 from dataclasses import dataclass, asdict
 from enum import Enum
 
 from .chromatic_field_processor import ChromaticFieldProcessor
 
 # Optional PhiRouter support (Feature 011)
-try, PhiRouterConfig, PhiSourcePriority
+try:
+    from phi_router import PhiRouter, PhiRouterConfig, PhiSourcePriority
     from phi_sensor_bridge import (MIDIInput, SerialSensorInput, AudioBeatDetector,
                                    SensorType, SensorConfig, SensorData)
     SENSOR_BINDING_AVAILABLE = True
-except ImportError)
-try, AdaptiveConfig, AdaptiveMode
+except ImportError:
+    SENSOR_BINDING_AVAILABLE = False
+
+# Optional PhiAdaptiveController support (Feature 012)
+try:
+    from phi_adaptive_controller import PhiAdaptiveController, AdaptiveConfig, AdaptiveMode
     ADAPTIVE_CONTROL_AVAILABLE = True
-except ImportError))
+except ImportError:
+    ADAPTIVE_CONTROL_AVAILABLE = False
+
+
+class PhiSource(Enum):
+    """External Φ modulation source types"""
+    MANUAL = "manual"              # Manual slider control
+    MICROPHONE = "microphone"      # Live microphone input
+    SENSOR = "sensor"              # External sensor (future)
     INTERNAL = "internal"          # Auto-generated Φ
 
 
 @dataclass
 class HybridNodeConfig:
     """Configuration for HybridNode"""
-    sample_rate)
+    sample_rate: int = 48000           # Audio sample rate (Hz)
     block_size: int = 512              # Processing block size in samples
     num_channels: int = 8              # Number of channels
-    input_device)
-    output_device)
+    input_device: Optional[int] = None # Input device index (None = default)
+    output_device: Optional[int] = None # Output device index (None = default)
     phi_source: PhiSource = PhiSource.INTERNAL  # Φ modulation source
-    metrics_broadcast_interval)
+    metrics_broadcast_interval: float = 0.033   # Metrics update interval (30 Hz)
     enable_logging: bool = True
 
 
 @dataclass
 class HybridMetrics:
     """Real-time metrics from hybrid processing"""
     timestamp: float
     ici: float                     # Inter-Criticality Interval
-    phase_coherence, 1]
-    spectral_centroid)
-    consciousness_level, 1]
+    phase_coherence: float         # Phase coherence [0, 1]
+    spectral_centroid: float       # Spectral centroid (Hz)
+    consciousness_level: float     # Consciousness level [0, 1]
     phi_phase: float               # Current Φ phase
     phi_depth: float               # Current Φ depth
-    cpu_load, 1]
-    latency_ms)
+    cpu_load: float                # Processing CPU load [0, 1]
+    latency_ms: float              # Processing latency (ms)
     dropouts: int                  # Audio dropout count
 
 
 class PhiModulator:
     """
     Feeds real or synthetic Φ waveform into HybridNode
 
     Supports multiple modulation sources:
     - Internal: Auto-generated golden ratio modulation
     - Manual: User-controlled slider values
     - Microphone: Live audio envelope extraction
-
+    - Sensor: External sensor input (future)
     """
 
-    def __init__(self, source: PhiSource) :
+    def __init__(self, source: PhiSource = PhiSource.INTERNAL):
         """
         Initialize PhiModulator
 
         Args:
-            source, dtype=np.float32)
+            source: Modulation source type
+        """
+        self.source = source
+        self.manual_phase = 0.0
+        self.manual_depth = 0.5
+
+        # Microphone input buffer for envelope extraction
+        self.mic_buffer = np.zeros(512, dtype=np.float32)
         self.mic_envelope = 0.0
 
         # Internal auto-modulation state
         self.internal_phase = 0.0
         self.internal_phase_increment = 0.0
 
         # Time tracking
         self.last_update_time = time.time()
 
-    @lru_cache(maxsize=128)
-    def update(self, dt) :
+    def update(self, dt: float = 0.0) -> tuple[float, float]:
         """
         Update and get current Φ phase and depth
 
         Args:
-            dt)
+            dt: Time delta since last update (seconds)
 
-        Returns, phi_depth) tuple
+        Returns:
+            (phi_phase, phi_depth) tuple
         """
-        if self.source == PhiSource.MANUAL, self.manual_depth)
+        if self.source == PhiSource.MANUAL:
+            # Return user-controlled values
+            return (self.manual_phase, self.manual_depth)
 
-        elif self.source == PhiSource.MICROPHONE))
+        elif self.source == PhiSource.MICROPHONE:
+            # Extract envelope from microphone input
+            envelope = np.mean(np.abs(self.mic_buffer))
             self.mic_envelope = 0.9 * self.mic_envelope + 0.1 * envelope
 
             # Map envelope to Φ depth
             phi_depth = np.clip(self.mic_envelope * 2.0, 0.0, 1.0)
 
             # Phase advances based on envelope
             phase_rate = 2 * np.pi * (1.0 + self.mic_envelope)
             self.internal_phase += phase_rate * dt
             self.internal_phase = self.internal_phase % (2 * np.pi)
 
             return (self.internal_phase, phi_depth)
 
-        elif self.source == PhiSource.INTERNAL)
+        elif self.source == PhiSource.INTERNAL:
+            # Auto-generated golden ratio modulation
+            # Phase advances at Φ-related rate
+            PHI_INV = 0.618033988749895
+            phase_rate = 2 * np.pi * PHI_INV  # ~3.88 rad/s
+            self.internal_phase += phase_rate * dt
+            self.internal_phase = self.internal_phase % (2 * np.pi)
 
             # Depth varies sinusoidally
             phi_depth = 0.5 + 0.3 * np.sin(self.internal_phase * 0.5)
 
             return (self.internal_phase, phi_depth)
 
         elif self.source == PhiSource.SENSOR:
-            # Future, 0.5)
+            # Future: external sensor integration
+            return (0.0, 0.5)
 
-        else, 0.5)
+        else:
+            return (0.0, 0.5)
 
-    @lru_cache(maxsize=128)
-    def set_manual(self, phase: float, depth: float) :
+    def set_manual(self, phase: float, depth: float):
         """
         Set manual Φ values
 
         Args:
-            phase, 2π]
-            depth, 1]
+            phase: Phase in radians [0, 2π]
+            depth: Depth [0, 1]
         """
         self.manual_phase = phase % (2 * np.pi)
         self.manual_depth = np.clip(depth, 0.0, 1.0)
 
-    @lru_cache(maxsize=128)
-    def set_source(self, source: PhiSource) :
+    def set_source(self, source: PhiSource):
         """
         Change modulation source
 
         Args:
-            source)
-    def feed_microphone_input(self, audio_data: np.ndarray) :
+            source: New modulation source
+        """
+        self.source = source
+
+    def feed_microphone_input(self, audio_data: np.ndarray):
         """
         Feed microphone data for envelope extraction
 
         Args:
-            audio_data, float32)
+            audio_data: Audio samples (mono, float32)
         """
         if audio_data.size > 0:
-            self.mic_buffer = audio_data[)
+            self.mic_buffer = audio_data[:512].astype(np.float32)
 
 
 class HybridNode:
+    """
+    HybridNode - Analog-Digital Bridge for D-ASE Engine
+
+    Manages real-time audio I/O and integrates with ChromaticFieldProcessor
+    for consciousness-aware analog signal processing.
+    """
+
+    def __init__(self, config: Optional[HybridNodeConfig] = None):
         """
         Initialize HybridNode
 
         Args:
-            config)
+            config: Configuration (uses defaults if None)
         """
         self.config = config or HybridNodeConfig()
 
         # Initialize ChromaticFieldProcessor (D-ASE engine)
-        logger.info("[HybridNode] Initializing ChromaticFieldProcessor...")
+        print(f"[HybridNode] Initializing ChromaticFieldProcessor...")
         self.processor = ChromaticFieldProcessor(
             num_channels=self.config.num_channels,
             sample_rate=self.config.sample_rate,
             block_size=self.config.block_size
+        )
 
         # Initialize Φ modulator
         self.phi_modulator = PhiModulator(source=self.config.phi_source)
 
         # Optional PhiRouter for sensor binding (Feature 011)
         self.phi_router: Optional[PhiRouter] = None
-        self.sensor_inputs, any] = {}  # Active sensor inputs
-        self.audio_beat_detector)
-        self.phi_adaptive)
-        self.metrics_callbacks, None]] = []
+        self.sensor_inputs: Dict[str, any] = {}  # Active sensor inputs
+        self.audio_beat_detector: Optional[AudioBeatDetector] = None
+
+        # Optional PhiAdaptiveController for feedback learning (Feature 012)
+        self.phi_adaptive: Optional[PhiAdaptiveController] = None
+
+        # Audio stream
+        self.stream = None
+        self.is_running = False
+
+        # Metrics storage and broadcasting
+        self.current_metrics = None
+        self.metrics_queue = queue.Queue(maxsize=100)
+        self.metrics_callbacks: List[Callable[[HybridMetrics], None]] = []
 
         # Performance tracking
         self.process_time_history = []
         self.dropout_count = 0
         self.frame_count = 0
 
         # Threading
         self.metrics_thread = None
         self.metrics_lock = threading.Lock()
 
         # Time tracking
         self.last_callback_time = time.time()
 
         if self.config.enable_logging:
-            logger.info("[HybridNode] Initialized)
-            logger.info("  Sample rate, self.config.sample_rate)
-            logger.info("  Block size, self.config.block_size)
-            logger.info("  Channels, self.config.num_channels)
-            logger.info("  Φ source, self.config.phi_source.value)
-
-    @lru_cache(maxsize=128)
-    def _audio_callback(self, indata, outdata, frames,
-                       time_info, status))
+            print(f"[HybridNode] Initialized:")
+            print(f"  Sample rate: {self.config.sample_rate} Hz")
+            print(f"  Block size: {self.config.block_size} samples")
+            print(f"  Channels: {self.config.num_channels}")
+            print(f"  Φ source: {self.config.phi_source.value}")
+
+    def _audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames: int,
+                       time_info, status):
+        """
+        Real-time audio callback (FR-002)
 
         Called by sounddevice for each audio block. Must be low-latency and lock-free.
 
         Args:
-            indata, channels]
-            outdata, channels]
+            indata: Input audio data [frames, channels]
+            outdata: Output audio data [frames, channels]
             frames: Number of frames
             time_info: Timing information
-            status)
+            status: Stream status flags
+        """
+        start_time = time.perf_counter()
 
         # Check for dropouts
         if status:
             self.dropout_count += 1
             if self.config.enable_logging:
-                logger.info("[HybridNode] Audio dropout, status)
+                print(f"[HybridNode] Audio dropout: {status}")
 
-        try)
-            if indata.ndim == 2, axis=1).astype(np.float32)
+        try:
+            # Extract mono input (mix down all input channels)
+            if indata.ndim == 2:
+                mono_input = np.mean(indata, axis=1).astype(np.float32)
             else:
                 mono_input = indata[:, 0].astype(np.float32)
 
             # Update Φ modulator
             current_time = time.perf_counter()
             dt = current_time - self.last_callback_time
             self.last_callback_time = current_time
 
             # Use PhiRouter if available (Feature 011), otherwise use PhiModulator
-            if self.phi_router)
+            if self.phi_router:
+                # Get Φ from router (which manages multiple sources)
                 phi_value, phi_phase = self.phi_router.get_current_phi()
                 phi_depth = phi_value  # Use Φ value as depth
 
                 # Feed audio to beat detector if active
-                if self.audio_beat_detector, self.config.sample_rate)
-            else, phi_depth = self.phi_modulator.update(dt)
+                if self.audio_beat_detector:
+                    self.audio_beat_detector.process_audio(mono_input, self.config.sample_rate)
+            else:
+                # Use simple PhiModulator
+                phi_phase, phi_depth = self.phi_modulator.update(dt)
 
                 # Feed microphone input to modulator if needed
-                if self.phi_modulator.source == PhiSource.MICROPHONE)
+                if self.phi_modulator.source == PhiSource.MICROPHONE:
+                    self.phi_modulator.feed_microphone_input(mono_input)
 
             # Process through ChromaticFieldProcessor (FR-006)
             # Returns [num_channels, block_size] multi-channel output
             processed_output = self.processor.processBlock(
                 input_block=mono_input,
                 phi_phase=phi_phase,
                 phi_depth=phi_depth
+            )
 
             # Mix multi-channel output to stereo/multi-channel output
             # For now, mix channels down to output channels
             num_output_channels = outdata.shape[1] if outdata.ndim == 2 else 1
 
             if num_output_channels == 1:
                 # Mono output: average all channels
                 outdata[:, 0] = np.mean(processed_output, axis=0)
             elif num_output_channels == 2:
                 # Stereo output: split channels
-                outdata[:, 0] = np.mean(processed_output[, axis=0)  # Left: channels 0-3
-                outdata[:, 1] = np.mean(processed_output[4, axis=0)  # Right: channels 4-7
+                outdata[:, 0] = np.mean(processed_output[:4], axis=0)  # Left: channels 0-3
+                outdata[:, 1] = np.mean(processed_output[4:], axis=0)  # Right: channels 4-7
             else:
-                # Multi-channel output, self.config.num_channels):
+                # Multi-channel output: map directly
+                for ch in range(min(num_output_channels, self.config.num_channels)):
                     outdata[:, ch] = processed_output[ch]
 
             # Calculate processing time
             elapsed = time.perf_counter() - start_time
             self.process_time_history.append(elapsed)
-            if len(self.process_time_history) > 100)
+            if len(self.process_time_history) > 100:
+                self.process_time_history.pop(0)
 
             # Get metrics from processor (lightweight, already calculated)
             metrics_dict = self.processor.getMetrics()
 
             # Create metrics snapshot
             latency_ms = elapsed * 1000
             cpu_load = elapsed / (frames / self.config.sample_rate) if frames > 0 else 0.0
 
             metrics = HybridMetrics(
                 timestamp=time.time(),
                 ici=metrics_dict['ici'],
                 phase_coherence=metrics_dict['phase_coherence'],
                 spectral_centroid=metrics_dict['spectral_centroid'],
                 consciousness_level=metrics_dict['consciousness_level'],
                 phi_phase=phi_phase,
                 phi_depth=phi_depth,
                 cpu_load=cpu_load,
                 latency_ms=latency_ms,
                 dropouts=self.dropout_count
+            )
 
             # Store current metrics (non-blocking)
-            with self.metrics_lock)
-            if self.phi_adaptive,
+            with self.metrics_lock:
+                self.current_metrics = metrics
+
+            # Feed metrics to adaptive controller (Feature 012)
+            if self.phi_adaptive:
+                self.phi_adaptive.update_metrics(
+                    ici=metrics.ici,
                     coherence=metrics.phase_coherence,
                     criticality=metrics.consciousness_level,  # Use consciousness as criticality proxy
                     phi_value=phi_depth,  # Current Phi value (mapped to depth)
                     phi_phase=phi_phase,
                     phi_depth=phi_depth,
                     active_source="adaptive"
+                )
 
             # Enqueue for broadcasting (non-blocking)
-            try)
+            try:
+                self.metrics_queue.put_nowait(metrics)
             except queue.Full:
                 # Drop oldest if queue full
-                try)
+                try:
+                    self.metrics_queue.get_nowait()
                     self.metrics_queue.put_nowait(metrics)
                 except:
                     pass
 
             self.frame_count += 1
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[HybridNode] Audio callback error, e)
+                print(f"[HybridNode] Audio callback error: {e}")
             # Output silence on error
             outdata.fill(0.0)
 
-    def _metrics_broadcast_loop(self) :
-            try)
+    def _metrics_broadcast_loop(self):
+        """
+        Metrics broadcasting thread (FR-007, SC-003)
+
+        Runs in background, broadcasting metrics at configured interval (≥30 Hz)
+        """
+        while self.is_running:
+            try:
+                # Get metrics from queue (blocking with timeout)
                 metrics = self.metrics_queue.get(timeout=self.config.metrics_broadcast_interval)
 
                 # Call all registered callbacks
                 for callback in self.metrics_callbacks:
-                    try)
+                    try:
+                        callback(metrics)
                     except Exception as e:
                         if self.config.enable_logging:
-                            logger.error("[HybridNode] Metrics callback error, e)
+                            print(f"[HybridNode] Metrics callback error: {e}")
 
-            except queue.Empty, continue
+            except queue.Empty:
+                # No new metrics, continue
                 pass
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[HybridNode] Metrics broadcast error, e)
+                    print(f"[HybridNode] Metrics broadcast error: {e}")
 
-    @lru_cache(maxsize=128)
-    def start(self) :
-            if self.config.enable_logging)
+    def start(self) -> bool:
+        """
+        Start hybrid mode processing (SC-001)
+
+        Returns:
+            True if started successfully, False otherwise
+        """
+        if self.is_running:
+            if self.config.enable_logging:
+                print("[HybridNode] Already running")
             return False
 
         try:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Starting hybrid mode...")
 
             # Start audio stream
             self.stream = sd.Stream(
                 samplerate=self.config.sample_rate,
                 blocksize=self.config.block_size,
                 device=(self.config.input_device, self.config.output_device),
                 channels=(2, 2),  # Stereo I/O (can be configured)
                 dtype=np.float32,
                 callback=self._audio_callback,
                 finished_callback=None
+            )
 
             self.stream.start()
             self.is_running = True
 
             # Start metrics broadcasting thread
             self.metrics_thread = threading.Thread(
                 target=self._metrics_broadcast_loop,
                 daemon=True
-
+            )
             self.metrics_thread.start()
 
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] ✓ Hybrid mode started")
 
             return True
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[HybridNode] Failed to start, e)
+                print(f"[HybridNode] Failed to start: {e}")
             self.is_running = False
             return False
 
-    @lru_cache(maxsize=128)
-    def stop(self) :
+    def stop(self) -> bool:
         """
         Stop hybrid mode processing
 
-        Returns, False otherwise
+        Returns:
+            True if stopped successfully, False otherwise
         """
         if not self.is_running:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Not running")
             return False
 
         try:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Stopping hybrid mode...")
 
             self.is_running = False
 
             # Stop audio stream
-            if self.stream)
+            if self.stream:
+                self.stream.stop()
                 self.stream.close()
                 self.stream = None
 
             # Wait for metrics thread
-            if self.metrics_thread and self.metrics_thread.is_alive())
+            if self.metrics_thread and self.metrics_thread.is_alive():
+                self.metrics_thread.join(timeout=1.0)
 
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] ✓ Hybrid mode stopped")
 
             return True
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[HybridNode] Failed to stop, e)
+                print(f"[HybridNode] Failed to stop: {e}")
             return False
 
-    def set_phi_source(self, source: PhiSource) :
-            source)
+    def set_phi_source(self, source: PhiSource):
+        """
+        Change Φ modulation source (FR-004)
+
+        Args:
+            source: New modulation source
+        """
+        self.phi_modulator.set_source(source)
         if self.config.enable_logging:
-            logger.info("[HybridNode] Φ source changed to, source.value)
+            print(f"[HybridNode] Φ source changed to: {source.value}")
 
-    def set_phi_manual(self, phase: float, depth: float) :
-            phase, 2π]
-            depth, 1]
+    def set_phi_manual(self, phase: float, depth: float):
+        """
+        Set manual Φ values (FR-004)
+
+        Args:
+            phase: Phase in radians [0, 2π]
+            depth: Depth [0, 1]
         """
         self.phi_modulator.set_manual(phase, depth)
 
-    def get_current_metrics(self) :
+    def get_current_metrics(self) -> Optional[HybridMetrics]:
         """
         Get current metrics snapshot
 
         Returns:
             Latest HybridMetrics or None if not running
         """
-        with self.metrics_lock, callback: Callable[[HybridMetrics], None]) :
-            callback)
+        with self.metrics_lock:
+            return self.current_metrics
 
-    @lru_cache(maxsize=128)
-    def unregister_metrics_callback(self, callback: Callable[[HybridMetrics], None]) :
+    def register_metrics_callback(self, callback: Callable[[HybridMetrics], None]):
+        """
+        Register callback for metrics updates (FR-007)
+
+        Callback will be called at configured interval (≥30 Hz)
+
+        Args:
+            callback: Function to call with HybridMetrics
+        """
+        self.metrics_callbacks.append(callback)
+
+    def unregister_metrics_callback(self, callback: Callable[[HybridMetrics], None]):
         """
         Unregister metrics callback
 
         Args:
             callback: Function to remove
         """
-        if callback in self.metrics_callbacks)
+        if callback in self.metrics_callbacks:
+            self.metrics_callbacks.remove(callback)
 
-    @lru_cache(maxsize=128)
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
         """
         Get performance statistics
 
         Returns:
             Dictionary with performance stats
         """
         if not self.process_time_history:
             return {
-                "avg_latency_ms",
-                "max_latency_ms",
-                "min_latency_ms",
-                "avg_cpu_load",
-                "dropout_count",
-                "frame_count",
-                "is_running") * 1000
+                "avg_latency_ms": 0.0,
+                "max_latency_ms": 0.0,
+                "min_latency_ms": 0.0,
+                "avg_cpu_load": 0.0,
+                "dropout_count": self.dropout_count,
+                "frame_count": self.frame_count,
+                "is_running": self.is_running
+            }
+
+        times_ms = [t * 1000 for t in self.process_time_history]
+        block_time_ms = (self.config.block_size / self.config.sample_rate) * 1000
 
         return {
-            "avg_latency_ms"),
-            "max_latency_ms"),
-            "min_latency_ms"),
-            "avg_cpu_load") / block_time_ms if block_time_ms > 0 else 0.0,
-            "dropout_count",
-            "frame_count",
-            "is_running")
-    def reset_statistics(self) :
+            "avg_latency_ms": np.mean(times_ms),
+            "max_latency_ms": np.max(times_ms),
+            "min_latency_ms": np.min(times_ms),
+            "avg_cpu_load": np.mean(times_ms) / block_time_ms if block_time_ms > 0 else 0.0,
+            "dropout_count": self.dropout_count,
+            "frame_count": self.frame_count,
+            "is_running": self.is_running
+        }
+
+    def reset_statistics(self):
+        """Reset performance statistics"""
+        self.process_time_history.clear()
+        self.dropout_count = 0
+        self.frame_count = 0
+
+    # Sensor Binding Methods (Feature 011)
+
+    def enable_sensor_binding(self) -> bool:
+        """
+        Enable sensor binding with PhiRouter (Feature 011)
+
+        Returns:
             True if enabled successfully
         """
         if not SENSOR_BINDING_AVAILABLE:
-            if self.config.enable_logging)")
+            if self.config.enable_logging:
+                print("[HybridNode] Sensor binding not available (missing phi_router/phi_sensor_bridge)")
             return False
 
         if self.phi_router:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Sensor binding already enabled")
             return True
 
         # Create PhiRouter
         router_config = PhiRouterConfig(enable_logging=self.config.enable_logging)
         self.phi_router = PhiRouter(router_config)
 
         # Register internal and manual sources
         self.phi_router.register_source("internal", PhiSourcePriority.INTERNAL)
         self.phi_router.register_source("manual", PhiSourcePriority.MANUAL)
 
         # Start router
         self.phi_router.start()
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[HybridNode] ✓ Sensor binding enabled")
 
         return True
 
-    def disable_sensor_binding(self) :
-            device_id)
-            cc_number)
-            channel)
+    def disable_sensor_binding(self):
+        """Disable sensor binding and stop all sensor inputs"""
+        # Stop all sensor inputs
+        for sensor_id, sensor_input in self.sensor_inputs.items():
+            if hasattr(sensor_input, 'stop'):
+                sensor_input.stop()
+
+        self.sensor_inputs.clear()
+
+        # Stop router
+        if self.phi_router:
+            self.phi_router.stop()
+            self.phi_router = None
+
+        if self.config.enable_logging:
+            print("[HybridNode] Sensor binding disabled")
+
+    def bind_midi_sensor(self, device_id: Optional[str] = None, cc_number: int = 1, channel: int = 0) -> bool:
+        """
+        Bind MIDI controller as Φ source (Feature 011, FR-001)
+
+        Args:
+            device_id: MIDI device name (None = first available)
+            cc_number: MIDI CC number to monitor (0-127)
+            channel: MIDI channel (0-15)
 
         Returns:
             True if bound successfully
         """
-        if not self.phi_router),
+        if not self.phi_router:
+            if not self.enable_sensor_binding():
+                return False
+
+        sensor_id = f"midi_cc{cc_number}"
+
+        # Create MIDI input
+        config = SensorConfig(
+            sensor_type=SensorType.MIDI_CC,
             device_id=device_id,
             midi_cc_number=cc_number,
             midi_channel=channel,
             enable_logging=self.config.enable_logging
+        )
+
+        def midi_callback(data: SensorData):
+            self.phi_router.update_source(sensor_id, data)
+
+        midi_input = MIDIInput(config, midi_callback)
 
-        def midi_callback(data: SensorData) :
+        if not midi_input.connect():
+            return False
+
+        # Register with router
+        self.phi_router.register_source(sensor_id, PhiSourcePriority.MIDI)
+
+        # Start MIDI input
+        if not midi_input.start():
             return False
 
         self.sensor_inputs[sensor_id] = midi_input
 
-        if self.config.enable_logging, cc_number)
+        if self.config.enable_logging:
+            print(f"[HybridNode] ✓ MIDI CC{cc_number} bound")
 
         return True
 
-    def bind_serial_sensor(self, device_id, baudrate,
-                          input_range, float] = (0.0, 1.0):
-            device_id)
+    def bind_serial_sensor(self, device_id: Optional[str] = None, baudrate: int = 9600,
+                          input_range: tuple[float, float] = (0.0, 1.0)) -> bool:
+        """
+        Bind serial sensor as Φ source (Feature 011, FR-001)
+
+        Args:
+            device_id: Serial port name (None = first available)
             baudrate: Serial baudrate
             input_range: Expected input value range
 
         Returns:
             True if bound successfully
         """
-        if not self.phi_router),
+        if not self.phi_router:
+            if not self.enable_sensor_binding():
+                return False
+
+        sensor_id = f"serial_{device_id or 'auto'}"
+
+        # Create serial sensor input
+        config = SensorConfig(
+            sensor_type=SensorType.SERIAL_ANALOG,
             device_id=device_id,
             serial_baudrate=baudrate,
             input_range=input_range,
             enable_logging=self.config.enable_logging
+        )
 
-        def serial_callback(data: SensorData) :
+        def serial_callback(data: SensorData):
+            self.phi_router.update_source(sensor_id, data)
+
+        serial_input = SerialSensorInput(config, serial_callback)
+
+        if not serial_input.connect():
+            return False
+
+        # Register with router
+        self.phi_router.register_source(sensor_id, PhiSourcePriority.SERIAL)
+
+        # Start serial input
+        if not serial_input.start():
             return False
 
         self.sensor_inputs[sensor_id] = serial_input
 
         if self.config.enable_logging:
-            logger.info("[HybridNode] ✓ Serial sensor bound, device_id)
+            print(f"[HybridNode] ✓ Serial sensor bound: {device_id}")
 
         return True
 
-    def bind_audio_beat_detector(self) :
+    def bind_audio_beat_detector(self) -> bool:
+        """
+        Bind audio beat detector as Φ source (Feature 011, FR-001)
+
+        Returns:
             True if bound successfully
         """
-        if not self.phi_router),
+        if not self.phi_router:
+            if not self.enable_sensor_binding():
+                return False
+
+        sensor_id = "audio_beat"
+
+        # Create beat detector
+        config = SensorConfig(
+            sensor_type=SensorType.AUDIO_BEAT,
             enable_logging=self.config.enable_logging
+        )
+
+        def beat_callback(data: SensorData):
+            self.phi_router.update_source(sensor_id, data)
+
+        self.audio_beat_detector = AudioBeatDetector(config, beat_callback)
 
-        def beat_callback(data: SensorData) :
+        # Register with router
+        self.phi_router.register_source(sensor_id, PhiSourcePriority.AUDIO_BEAT)
+
+        if self.config.enable_logging:
+            print(f"[HybridNode] ✓ Audio beat detector bound")
+
+        return True
+
+    def unbind_sensor(self, sensor_id: str) -> bool:
         """
         Unbind a sensor
 
         Args:
             sensor_id: Sensor identifier to unbind
 
         Returns:
             True if unbound successfully
         """
-        if sensor_id in self.sensor_inputs, 'stop'))
+        if sensor_id in self.sensor_inputs:
+            sensor_input = self.sensor_inputs[sensor_id]
+
+            if hasattr(sensor_input, 'stop'):
+                sensor_input.stop()
 
             del self.sensor_inputs[sensor_id]
 
-            if self.phi_router)
+            if self.phi_router:
+                self.phi_router.unregister_source(sensor_id)
 
             if self.config.enable_logging:
-                logger.info("[HybridNode] Sensor unbound, sensor_id)
+                print(f"[HybridNode] Sensor unbound: {sensor_id}")
 
             return True
 
         return False
 
-    def get_sensor_status(self) :
+    def get_sensor_status(self) -> Dict:
+        """
+        Get sensor binding status (Feature 011, FR-004)
+
+        Returns:
             Dictionary with sensor status
         """
         if not self.phi_router:
             return {
-                "sensor_binding_enabled",
-                "active_sensors",
-                "router_status")
+                "sensor_binding_enabled": False,
+                "active_sensors": [],
+                "router_status": None
+            }
+
+        router_status = self.phi_router.get_status()
 
         return {
-            "sensor_binding_enabled",
-            "active_sensors")),
-            "router_status")
+            "sensor_binding_enabled": True,
+            "active_sensors": list(self.sensor_inputs.keys()),
+            "router_status": asdict(router_status)
         }
 
     # Adaptive Control Methods (Feature 012)
 
-    def enable_adaptive_control(self, mode) :
-            mode, "predictive", or "learning")
+    def enable_adaptive_control(self, mode: str = "reactive") -> bool:
+        """
+        Enable adaptive Phi control (Feature 012)
+
+        Args:
+            mode: Adaptive mode ("reactive", "predictive", or "learning")
 
         Returns:
             True if enabled successfully
         """
         if not ADAPTIVE_CONTROL_AVAILABLE:
-            if self.config.enable_logging)")
+            if self.config.enable_logging:
+                print("[HybridNode] Adaptive control not available (missing phi_adaptive_controller)")
             return False
 
         if self.phi_adaptive:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Adaptive control already enabled")
             return True
 
         # Create adaptive controller
         adaptive_config = AdaptiveConfig(
             target_ici=0.5,
             ici_tolerance=0.05,
             update_rate_hz=10.0,
             enable_logging=self.config.enable_logging
-
+        )
         self.phi_adaptive = PhiAdaptiveController(adaptive_config)
 
         # Set Phi update callback to update router
-        def phi_update_callback(phi_value: float, phi_phase: float) :
-            if self.phi_router, SensorType
+        def phi_update_callback(phi_value: float, phi_phase: float):
+            if self.phi_router:
+                # Update router's manual source with adaptive values
+                from phi_sensor_bridge import SensorData, SensorType
                 import time
 
                 data = SensorData(
                     sensor_type=SensorType.MIDI_CC,
                     timestamp=time.time(),
                     raw_value=phi_value,
                     normalized_value=phi_value,
                     source_id="adaptive"
-
+                )
                 self.phi_router.update_source("adaptive", data)
 
         self.phi_adaptive.set_phi_update_callback(phi_update_callback)
 
         # Map mode string to enum
         mode_map = {
-            "reactive",
-            "predictive",
-            "learning"), AdaptiveMode.REACTIVE)
+            "reactive": AdaptiveMode.REACTIVE,
+            "predictive": AdaptiveMode.PREDICTIVE,
+            "learning": AdaptiveMode.LEARNING
+        }
+        adaptive_mode = mode_map.get(mode.lower(), AdaptiveMode.REACTIVE)
 
         # Enable controller
         self.phi_adaptive.enable(adaptive_mode)
 
         # Register adaptive source with router if available
-        if self.phi_router, PhiSourcePriority.MANUAL)
+        if self.phi_router:
+            self.phi_router.register_source("adaptive", PhiSourcePriority.MANUAL)
 
-        if self.config.enable_logging)", mode)
+        if self.config.enable_logging:
+            print(f"[HybridNode] Adaptive control enabled ({mode} mode)")
 
         return True
 
-    def disable_adaptive_control(self) :
+    def disable_adaptive_control(self):
         """Disable adaptive Phi control"""
-        if self.phi_adaptive)
+        if self.phi_adaptive:
+            self.phi_adaptive.disable()
 
             # Unregister adaptive source from router
-            if self.phi_router)
+            if self.phi_router:
+                self.phi_router.unregister_source("adaptive")
 
             self.phi_adaptive = None
 
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[HybridNode] Adaptive control disabled")
+
+    def set_adaptive_manual_override(self, enabled: bool):
+        """
+        Set manual override for adaptive control (SC-004)
 
-    def set_adaptive_manual_override(self, enabled: bool) :
+        Args:
             enabled: True to enable manual override
         """
-        if self.phi_adaptive)
+        if self.phi_adaptive:
+            self.phi_adaptive.set_manual_override(enabled)
+
+    def get_adaptive_status(self) -> Optional[Dict]:
+        """
+        Get adaptive control status (Feature 012)
 
-    def get_adaptive_status(self) :
+        Returns:
             Dictionary with adaptive status or None if not enabled
         """
-        if not self.phi_adaptive)
+        if not self.phi_adaptive:
+            return None
+
+        status = self.phi_adaptive.get_status()
         from dataclasses import asdict
         return asdict(status)
 
-    def save_adaptive_session(self, filepath) :
+    def save_adaptive_session(self, filepath: str) -> bool:
         """
         Save current adaptive session to file
 
         Args:
             filepath: Path to save file
 
         Returns:
             True if saved successfully
         """
-        if not self.phi_adaptive)
+        if not self.phi_adaptive:
+            return False
 
-    def load_adaptive_session(self, filepath) :
+        return self.phi_adaptive.save_session(filepath)
+
+    def load_adaptive_session(self, filepath: str) -> bool:
         """
         Load adaptive session from file
 
         Args:
             filepath: Path to session file
 
         Returns:
             True if loaded successfully
         """
-        if not self.phi_adaptive)
+        if not self.phi_adaptive:
+            return False
 
-    def trigger_adaptive_learning(self) :
+        return self.phi_adaptive.load_session(filepath)
+
+    def trigger_adaptive_learning(self) -> bool:
         """
         Trigger learning from current session
 
         Returns:
             True if learning successful
         """
-        if not self.phi_adaptive)
+        if not self.phi_adaptive:
+            return False
+
+        return self.phi_adaptive.learn_from_current_session()
 
     @staticmethod
-    def list_midi_devices() :
+    def list_midi_devices() -> List[str]:
         """
         List available MIDI devices
 
         Returns:
             List of MIDI device names
         """
-        if not SENSOR_BINDING_AVAILABLE)
+        if not SENSOR_BINDING_AVAILABLE:
+            return []
+
+        return MIDIInput.list_devices()
 
     @staticmethod
-    def list_serial_devices() :
+    def list_serial_devices() -> List[str]:
         """
         List available serial devices
 
         Returns:
             List of serial port names
         """
-        if not SENSOR_BINDING_AVAILABLE)
+        if not SENSOR_BINDING_AVAILABLE:
+            return []
+
+        return SerialSensorInput.list_devices()
 
     @staticmethod
-    def list_audio_devices() :
+    def list_audio_devices() -> List[Dict]:
         """
         List available audio devices
 
-        Returns, channels, etc.
+        Returns:
+            List of device dictionaries with name, channels, etc.
         """
         devices = []
-        for i, dev in enumerate(sd.query_devices():
+        for i, dev in enumerate(sd.query_devices()):
             devices.append({
-                "index",
-                "name",
-                "max_input_channels",
-                "max_output_channels",
-                "default_samplerate")
+                "index": i,
+                "name": dev['name'],
+                "max_input_channels": dev['max_input_channels'],
+                "max_output_channels": dev['max_output_channels'],
+                "default_samplerate": dev['default_samplerate']
+            })
         return devices
 
 
 # Self-test function
-def _self_test() :5], dev['index'], dev['name'])
+def _self_test():
+    """Run basic self-test of HybridNode"""
+    print("=" * 60)
+    print("HybridNode Self-Test")
+    print("=" * 60)
+
+    try:
+        # List audio devices
+        print("\n1. Listing audio devices...")
+        devices = HybridNode.list_audio_devices()
+        print(f"   Found {len(devices)} audio devices:")
+        for dev in devices[:5]:  # Show first 5
+            print(f"   [{dev['index']}] {dev['name']}")
 
         # Create HybridNode
-        logger.info("\n2. Creating HybridNode...")
+        print("\n2. Creating HybridNode...")
         config = HybridNodeConfig(enable_logging=True)
         node = HybridNode(config)
-        logger.info("   ✓ HybridNode created")
+        print("   ✓ HybridNode created")
 
         # Test Φ modulator
-        logger.info("\n3. Testing Φ modulator...")
+        print("\n3. Testing Φ modulator...")
         phase, depth = node.phi_modulator.update(0.1)
-        logger.info("   Internal modulation, depth=%s", phase, depth)
+        print(f"   Internal modulation: phase={phase:.2f}, depth={depth:.2f}")
 
         node.set_phi_source(PhiSource.MANUAL)
         node.set_phi_manual(1.57, 0.7)
         phase, depth = node.phi_modulator.update(0.0)
-        logger.info("   Manual modulation, depth=%s", phase, depth)
-        logger.info("   ✓ Φ modulator working")
+        print(f"   Manual modulation: phase={phase:.2f}, depth={depth:.2f}")
+        print("   ✓ Φ modulator working")
 
         # Register metrics callback
-        logger.info("\n4. Registering metrics callback...")
+        print("\n4. Registering metrics callback...")
         metrics_received = []
 
-        @lru_cache(maxsize=128)
-        def metrics_callback(metrics: HybridMetrics) :
+        def metrics_callback(metrics: HybridMetrics):
+            metrics_received.append(metrics)
+
+        node.register_metrics_callback(metrics_callback)
+        print("   ✓ Callback registered")
+
+        # Start hybrid mode (brief test)
+        print("\n5. Starting hybrid mode (3 second test)...")
+        if node.start():
+            print("   ✓ Hybrid mode started")
+
+            # Run for 3 seconds
+            time.sleep(3.0)
+
+            # Get statistics
+            stats = node.get_statistics()
+            print(f"\n   Performance Statistics:")
+            print(f"   - Frames processed: {stats['frame_count']}")
+            print(f"   - Average latency: {stats['avg_latency_ms']:.2f} ms")
+            print(f"   - Max latency: {stats['max_latency_ms']:.2f} ms")
+            print(f"   - CPU load: {stats['avg_cpu_load']*100:.1f}%")
+            print(f"   - Dropouts: {stats['dropout_count']}")
+
+            # Check metrics broadcasting
+            print(f"\n   Metrics received: {len(metrics_received)}")
+            if metrics_received:
                 latest = metrics_received[-1]
-                logger.info("   Latest metrics)
-                logger.info("   - ICI, latest.ici)
-                logger.info("   - Coherence, latest.phase_coherence)
-                logger.info("   - Centroid, latest.spectral_centroid)
-                logger.info("   - Consciousness, latest.consciousness_level)
+                print(f"   Latest metrics:")
+                print(f"   - ICI: {latest.ici:.4f}")
+                print(f"   - Coherence: {latest.phase_coherence:.4f}")
+                print(f"   - Centroid: {latest.spectral_centroid:.1f} Hz")
+                print(f"   - Consciousness: {latest.consciousness_level:.4f}")
 
             # Check success criteria
-            logger.info("\n   Success Criteria)
+            print(f"\n   Success Criteria:")
             sc001 = stats['dropout_count'] == 0
             sc002 = stats['max_latency_ms'] < 2.0
             sc003 = len(metrics_received) >= 90  # Should get ~90 metrics in 3s at 30Hz
             sc004 = stats['avg_cpu_load'] < 0.5
 
-            logger.error("   SC-001 (No dropouts), '✓ PASS' if sc001 else '✗ FAIL')
-            logger.error("   SC-002 (Latency < 2ms))", '✓ PASS' if sc002 else '✗ FAIL', stats['max_latency_ms'])
-            logger.error("   SC-003 (Metrics ≥30Hz))", '✓ PASS' if sc003 else '✗ FAIL', len(metrics_received)/3)
-            logger.error("   SC-004 (CPU < 50%))", '✓ PASS' if sc004 else '✗ FAIL', stats['avg_cpu_load']*100)
+            print(f"   SC-001 (No dropouts): {'✓ PASS' if sc001 else '✗ FAIL'}")
+            print(f"   SC-002 (Latency < 2ms): {'✓ PASS' if sc002 else '✗ FAIL'} ({stats['max_latency_ms']:.2f}ms)")
+            print(f"   SC-003 (Metrics ≥30Hz): {'✓ PASS' if sc003 else '✗ FAIL'} ({len(metrics_received)/3:.1f}Hz)")
+            print(f"   SC-004 (CPU < 50%): {'✓ PASS' if sc004 else '✗ FAIL'} ({stats['avg_cpu_load']*100:.1f}%)")
 
             # Stop
             node.stop()
-            logger.info("   ✓ Hybrid mode stopped")
+            print("   ✓ Hybrid mode stopped")
 
             passed = sc001 and sc002 and sc003 and sc004
-        else)
+        else:
+            print("   ✗ Failed to start hybrid mode")
             passed = False
 
-        logger.info("\n" + "=" * 60)
-        if passed)
-        else)
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        if passed:
+            print("Self-Test PASSED ✓")
+        else:
+            print("Self-Test FAILED ✗")
+        print("=" * 60)
         return passed
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/ici_engine.py b/server/ici_engine.py
index cf15de88fd73ad24a26387e365a0a3960430e793..15ac5b0e4c81888d3cdf0ddba46a3c83d1762369 100644
--- a/server/ici_engine.py
+++ b/server/ici_engine.py
@@ -1,287 +1,457 @@
 """
 Integrated Chromatic Information Engine
 Feature 014: Full spectral-phase integration model for real-time ICI computation
 
 Implements:
 - FR-001: IntegratedChromaticInformation class
 - FR-002: 8-channel buffer processing
 - FR-003: Cross-spectral power and phase computation
 - FR-004: ICI integration formula
-
+- FR-005: Exponential smoothing (α = 0.2)
 - FR-006: Scalar and matrix output
 - FR-007: Performance target < 0.5ms per block
 - FR-008: Configuration options
 
-Formula) / N(N-1)
+Formula:
+  ICI = Σᵢ≠ⱼ |AᵢAⱼ| / Ā² · cos(φᵢ − φⱼ) / N(N-1)
 
 Success Criteria:
 - SC-001: Processing time < 0.5ms per block
 - SC-002: Correlation r > 0.8 with coherence
 - SC-003: Matrix accuracy ± 0.01
+- SC-004: No missed frames at 48kHz
+"""
 
 import numpy as np
 import time
 from typing import Optional, Tuple, Dict
 from dataclasses import dataclass
 from scipy import fft
 
 
 @dataclass
 class ICIConfig:
     """Configuration for ICI Engine"""
 
     # Number of channels
-    num_channels)
+    num_channels: int = 8
+
+    # Smoothing factor (FR-005)
     smoothing_alpha: float = 0.2
 
     # FFT parameters
-    fft_size)
+    fft_size: int = 512  # Must match audio buffer size
+
+    # Output options (FR-006)
     output_matrix: bool = False  # Output full 8x8 matrix
     output_vector: bool = False  # Output per-channel summary
 
     # Performance options
     use_rfft: bool = True  # Use real FFT for efficiency
 
     # Logging
     enable_logging: bool = False
     log_interval: float = 5.0  # Log stats every 5s
 
 
 class IntegratedChromaticInformation:
     """
     Real-time ICI engine with spectral-phase integration
 
     Computes Integrated Chromatic Information by analyzing
     cross-spectral power and phase coherence between all
     channel pairs in real-time.
 
+    Features:
+    - FFT-based spectral analysis
+    - Cross-channel phase coherence
+    - Normalized information integration
+    - Exponential smoothing
+    - High-performance (< 0.5ms per block)
     """
 
-    def __init__(self, config: Optional[ICIConfig]) :
+    def __init__(self, config: Optional[ICIConfig] = None):
         """
         Initialize ICI Engine
 
         Args:
-            config)
+            config: ICIConfig (uses defaults if None)
         """
         self.config = config or ICIConfig()
 
         # Current state
         self.current_ici: float = 0.0
-        self.smoothed_ici)
-        self.ici_matrix, self.config.num_channels))
+        self.smoothed_ici: float = 0.0
+
+        # Matrix storage (FR-006)
+        self.ici_matrix: np.ndarray = np.zeros((self.config.num_channels, self.config.num_channels))
 
         # Performance tracking
         self.processing_times: list = []
         self.total_frames: int = 0
-        self.last_log_time)
+        self.last_log_time: float = 0.0
+
+        # Pre-allocate arrays for efficiency
+        self._init_buffers()
 
-        logger.info("[ICIEngine] Initialized")
-        logger.info("[ICIEngine]   num_channels=%s", self.config.num_channels)
-        logger.info("[ICIEngine]   fft_size=%s", self.config.fft_size)
-        logger.info("[ICIEngine]   smoothing_alpha=%s", self.config.smoothing_alpha)
-        logger.info("[ICIEngine]   use_rfft=%s", self.config.use_rfft)
+        print("[ICIEngine] Initialized")
+        print(f"[ICIEngine]   num_channels={self.config.num_channels}")
+        print(f"[ICIEngine]   fft_size={self.config.fft_size}")
+        print(f"[ICIEngine]   smoothing_alpha={self.config.smoothing_alpha}")
+        print(f"[ICIEngine]   use_rfft={self.config.use_rfft}")
 
-    @lru_cache(maxsize=128)
-    def _init_buffers(self) :
+    def _init_buffers(self):
         """Pre-allocate buffers for efficiency"""
         N = self.config.num_channels
         fft_size = self.config.fft_size
 
         # FFT output buffer
         if self.config.use_rfft:
             # Real FFT outputs N//2+1 frequency bins
             freq_bins = fft_size // 2 + 1
-        else, freq_bins), dtype=np.complex128)
+        else:
+            freq_bins = fft_size
+
+        self.fft_buffer = np.zeros((N, freq_bins), dtype=np.complex128)
 
         # Magnitude and phase buffers
         self.magnitudes = np.zeros((N, freq_bins))
         self.phases = np.zeros((N, freq_bins))
 
         # Cross-spectral matrix
         self.cross_spectral = np.zeros((N, N))
 
-    @lru_cache(maxsize=128)
-    def process_block(self, audio_buffer) :
-            audio_buffer, buffer_size)
+    def process_block(self, audio_buffer: np.ndarray) -> Tuple[float, Optional[np.ndarray]]:
+        """
+        Process audio block and compute ICI (FR-002, FR-003, FR-004)
+
+        Args:
+            audio_buffer: Audio buffer of shape (num_channels, buffer_size)
 
-        Returns, ici_matrix_optional)
+        Returns:
+            Tuple of (ici_scalar, ici_matrix_optional)
         """
         start_time = time.perf_counter()
 
         # Validate input
         if audio_buffer.shape[0] != self.config.num_channels:
-            logger.warning("[ICIEngine] WARNING, got %s", self.config.num_channels, audio_buffer.shape[0])
+            print(f"[ICIEngine] WARNING: Expected {self.config.num_channels} channels, got {audio_buffer.shape[0]}")
             return self.smoothed_ici, None
 
         if audio_buffer.shape[1] != self.config.fft_size:
-            logger.warning("[ICIEngine] WARNING, got %s", self.config.fft_size, audio_buffer.shape[1])
+            print(f"[ICIEngine] WARNING: Expected buffer size {self.config.fft_size}, got {audio_buffer.shape[1]}")
             return self.smoothed_ici, None
 
         # Check for NaN or invalid input (edge case)
-        if np.any(np.isnan(audio_buffer)) or np.any(np.isinf(audio_buffer):
-            logger.warning("[ICIEngine] WARNING, skipping frame")
+        if np.any(np.isnan(audio_buffer)) or np.any(np.isinf(audio_buffer)):
+            print("[ICIEngine] WARNING: NaN or Inf detected, skipping frame")
             elapsed = (time.perf_counter() - start_time) * 1000
             self.processing_times.append(elapsed)
             return self.smoothed_ici, None
 
-        # Step 1)
+        # Step 1: Compute FFT for all channels (FR-003)
         self._compute_spectra(audio_buffer)
 
-        # Step 2)
+        # Step 2: Compute cross-spectral power and phase differences (FR-003)
         self._compute_cross_spectral()
 
-        # Step 3)
+        # Step 3: Apply ICI integration formula (FR-004)
         ici_value = self._compute_ici()
 
-        # Step 4)
+        # Step 4: Apply exponential smoothing (FR-005)
         self.current_ici = ici_value
         self.smoothed_ici = (self.config.smoothing_alpha * ici_value +
                              (1 - self.config.smoothing_alpha) * self.smoothed_ici)
 
         # Track performance (FR-007)
         elapsed = (time.perf_counter() - start_time) * 1000  # Convert to ms
         self.processing_times.append(elapsed)
         self.total_frames += 1
 
         # Log if needed
         current_time = time.time()
-        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval)
+        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval:
+            self._log_stats()
             self.last_log_time = current_time
 
         # Return matrix if requested (FR-006)
         matrix_output = self.ici_matrix.copy() if self.config.output_matrix else None
 
         return self.smoothed_ici, matrix_output
 
-    @lru_cache(maxsize=128)
-    def _compute_spectra(self, audio_buffer: np.ndarray) :
+    def _compute_spectra(self, audio_buffer: np.ndarray):
         """
         Compute FFT spectra for all channels
 
         Args:
-            audio_buffer, buffer_size)
+            audio_buffer: Audio buffer of shape (num_channels, buffer_size)
         """
-        for i in range(self.config.num_channels))
+        for i in range(self.config.num_channels):
+            # Apply window to reduce spectral leakage
+            window = np.hanning(self.config.fft_size)
             windowed_signal = audio_buffer[i] * window
 
             # Compute FFT
-            if self.config.use_rfft)
+            if self.config.use_rfft:
+                # Real FFT (more efficient for real signals)
                 spectrum = fft.rfft(windowed_signal)
-            else)
+            else:
+                spectrum = fft.fft(windowed_signal)
 
             self.fft_buffer[i] = spectrum
 
             # Compute magnitude and phase
             self.magnitudes[i] = np.abs(spectrum)
             self.phases[i] = np.angle(spectrum)
 
-    @lru_cache(maxsize=128)
-    def _compute_cross_spectral(self) :
-        - Cross-spectral power: |Aᵢ||Aⱼ|
+    def _compute_cross_spectral(self):
+        """
+        Compute cross-spectral power and phase coherence matrix (FR-003)
 
+        For each channel pair (i, j):
+        - Cross-spectral power: |Aᵢ||Aⱼ|
+        - Phase coherence: cos(φᵢ - φⱼ)
         """
         N = self.config.num_channels
 
         # Average magnitude for normalization
         avg_magnitude = np.mean(self.magnitudes)
         avg_magnitude_sq = avg_magnitude ** 2
 
         # Avoid division by zero
-        if avg_magnitude_sq < 1e-10)
+        if avg_magnitude_sq < 1e-10:
+            self.ici_matrix.fill(0.0)
             return
 
         # Compute for all pairs
         for i in range(N):
-                if i == j, j] = 0.0  # Diagonal is zero
+            for j in range(N):
+                if i == j:
+                    self.ici_matrix[i, j] = 0.0  # Diagonal is zero
                     continue
 
                 # Average cross-spectral power across frequency bins
                 # |Aᵢ||Aⱼ| averaged over frequencies
                 cross_power = np.mean(self.magnitudes[i] * self.magnitudes[j])
 
                 # Average phase coherence across frequency bins
                 # cos(φᵢ - φⱼ) averaged over frequencies
                 phase_diff = self.phases[i] - self.phases[j]
                 phase_coherence = np.mean(np.cos(phase_diff))
 
-                # Combined metric, j] = (cross_power / avg_magnitude_sq) * phase_coherence
+                # Combined metric: normalized cross-power weighted by phase coherence
+                self.ici_matrix[i, j] = (cross_power / avg_magnitude_sq) * phase_coherence
 
-    @lru_cache(maxsize=128)
-    def _compute_ici(self) :
+    def _compute_ici(self) -> float:
+        """
+        Apply ICI integration formula (FR-004)
+
+        ICI = Σᵢ≠ⱼ |AᵢAⱼ| / Ā² · cos(φᵢ − φⱼ) / N(N-1)
+
+        Returns:
+            Scalar ICI value in range [0, 1]
+        """
+        N = self.config.num_channels
+
+        # Sum all off-diagonal elements
+        # (diagonal is already set to 0)
+        ici_sum = np.sum(self.ici_matrix)
+
+        # Normalize by number of pairs: N(N-1)
+        num_pairs = N * (N - 1)
+        ici_value = ici_sum / num_pairs if num_pairs > 0 else 0.0
+
+        # Clamp to [0, 1] range
+        # Note: cos can be negative, so sum can be negative
+        # We'll normalize to [0, 1] by scaling from [-1, 1]
+        ici_normalized = (ici_value + 1.0) / 2.0
+        ici_normalized = np.clip(ici_normalized, 0.0, 1.0)
+
+        return float(ici_normalized)
+
+    def get_statistics(self) -> Dict:
+        """
+        Get performance statistics (FR-007)
+
+        Returns:
+            Dictionary with performance metrics
+        """
+        if len(self.processing_times) == 0:
             return {
-                'current_ici',
-                'smoothed_ici',
-                'total_frames',
-                'avg_processing_time_ms',
-                'max_processing_time_ms',
-                'p95_processing_time_ms')
+                'current_ici': self.current_ici,
+                'smoothed_ici': self.smoothed_ici,
+                'total_frames': self.total_frames,
+                'avg_processing_time_ms': 0.0,
+                'max_processing_time_ms': 0.0,
+                'p95_processing_time_ms': 0.0
+            }
+
+        # Get recent processing times (last 1000)
         recent_times = self.processing_times[-1000:]
 
         return {
-            'current_ici'),
-            'smoothed_ici'),
-            'total_frames',
-            'avg_processing_time_ms')),
-            'max_processing_time_ms')),
-            'p95_processing_time_ms', 95)),
-            'p99_processing_time_ms', 99))
+            'current_ici': float(self.current_ici),
+            'smoothed_ici': float(self.smoothed_ici),
+            'total_frames': self.total_frames,
+            'avg_processing_time_ms': float(np.mean(recent_times)),
+            'max_processing_time_ms': float(np.max(recent_times)),
+            'p95_processing_time_ms': float(np.percentile(recent_times, 95)),
+            'p99_processing_time_ms': float(np.percentile(recent_times, 99))
         }
 
-    @lru_cache(maxsize=128)
-    def get_matrix(self) : "
-              f"ICI={stats['smoothed_ici'], "
+    def get_matrix(self) -> np.ndarray:
+        """
+        Get current ICI matrix (FR-006)
+
+        Returns:
+            8x8 matrix of cross-channel ICI values
+        """
+        return self.ici_matrix.copy()
+
+    def get_vector_summary(self) -> np.ndarray:
+        """
+        Get per-channel ICI summary vector (FR-006)
+
+        Returns:
+            8-element vector with per-channel average ICI
+        """
+        # Average ICI for each channel (excluding diagonal)
+        N = self.config.num_channels
+        vector = np.zeros(N)
+
+        for i in range(N):
+            # Average of row i (excluding diagonal)
+            row_sum = np.sum(self.ici_matrix[i, :]) - self.ici_matrix[i, i]
+            vector[i] = row_sum / (N - 1) if N > 1 else 0.0
+
+        return vector
+
+    def _log_stats(self):
+        """Log performance statistics"""
+        stats = self.get_statistics()
+
+        print(f"[ICIEngine] Stats: "
+              f"ICI={stats['smoothed_ici']:.4f}, "
               f"frames={stats['total_frames']}, "
-              f"avg_time={stats['avg_processing_time_ms'], "
-              f"p95={stats['p95_processing_time_ms'])
+              f"avg_time={stats['avg_processing_time_ms']:.3f}ms, "
+              f"p95={stats['p95_processing_time_ms']:.3f}ms")
+
+    def reset(self):
+        """Reset engine state"""
+        self.current_ici = 0.0
+        self.smoothed_ici = 0.0
+        self.ici_matrix.fill(0.0)
+        self.processing_times.clear()
+        self.total_frames = 0
+
+        print("[ICIEngine] State reset")
+
+
+# Self-test function
+def _self_test():
+    """Test ICIEngine"""
+    print("=" * 60)
+    print("ICI Engine Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
+    config = ICIConfig(num_channels=8, fft_size=512, smoothing_alpha=0.2)
+    engine = IntegratedChromaticInformation(config)
+
+    assert engine.config.num_channels == 8
+    assert engine.config.fft_size == 512
+    assert engine.smoothed_ici == 0.0
+    print("   OK: Initialization")
+
+    # Test 2: Process synthetic audio
+    print("\n2. Testing audio processing...")
+
+    # Create synthetic 8-channel audio buffer
+    buffer_size = 512
+    audio_buffer = np.zeros((8, buffer_size))
+
+    # Generate test signals with different frequencies
+    t = np.linspace(0, 1, buffer_size)
+    for ch in range(8):
+        freq = 100 + ch * 50  # Different frequency per channel
+        audio_buffer[ch] = 0.1 * np.sin(2 * np.pi * freq * t)
+
+    # Process block
+    ici, matrix = engine.process_block(audio_buffer)
+
+    print(f"   ICI value: {ici:.4f}")
+    print(f"   Matrix shape: {engine.ici_matrix.shape}")
 
-    @lru_cache(maxsize=128)
-    def reset(self) :.3f}ms exceeds 1.0ms target"
+    assert 0.0 <= ici <= 1.0, f"ICI should be in [0, 1], got {ici}"
+    assert engine.ici_matrix.shape == (8, 8), "Matrix should be 8x8"
+    print("   OK: Audio processing")
+
+    # Test 3: Performance measurement
+    print("\n3. Testing performance...")
+
+    # Process multiple blocks
+    for i in range(100):
+        audio_buffer = np.random.randn(8, 512) * 0.1
+        ici, _ = engine.process_block(audio_buffer)
+
+    stats = engine.get_statistics()
+    avg_time = stats['avg_processing_time_ms']
+    p95_time = stats['p95_processing_time_ms']
+
+    print(f"   Average time: {avg_time:.3f}ms")
+    print(f"   95th percentile: {p95_time:.3f}ms")
+    print(f"   Frames processed: {stats['total_frames']}")
+
+    # Verify SC-001: < 1.0ms per block (conservative target for development)
+    # Production target is < 0.5ms which may require hardware-specific optimization
+    assert avg_time < 1.0, f"Average time {avg_time:.3f}ms exceeds 1.0ms target"
 
     if avg_time < 0.5:
-        logger.info("   OK)")
+        print("   OK: Performance target met (< 0.5ms)")
     else:
-        logger.info("   OK)", avg_time)
+        print(f"   OK: Performance acceptable ({avg_time:.3f}ms < 1.0ms target)")
 
-    # Test 4)
+    # Test 4: Matrix output
+    print("\n4. Testing matrix output...")
 
     matrix = engine.get_matrix()
     vector = engine.get_vector_summary()
 
     assert matrix.shape == (8, 8), "Matrix should be 8x8"
     assert vector.shape == (8,), "Vector should be length 8"
     assert np.allclose(np.diag(matrix), 0.0), "Diagonal should be zero"
 
-    logger.info("   Matrix range, %s]", matrix.min(), matrix.max())
-    logger.info("   Vector range, %s]", vector.min(), vector.max())
-    logger.info("   OK)
+    print(f"   Matrix range: [{matrix.min():.3f}, {matrix.max():.3f}]")
+    print(f"   Vector range: [{vector.min():.3f}, {vector.max():.3f}]")
+    print("   OK: Matrix output")
 
-    # Test 5)
-    logger.info("\n5. Testing NaN handling...")
+    # Test 5: NaN handling (edge case)
+    print("\n5. Testing NaN handling...")
 
     nan_buffer = np.full((8, 512), np.nan)
     ici_before = engine.smoothed_ici
     ici, _ = engine.process_block(nan_buffer)
 
     # Should preserve last valid state
     assert ici == ici_before, "Should preserve last valid ICI on NaN input"
-    logger.info("   OK)
+    print("   OK: NaN handling")
 
-    # Test 6)
+    # Test 6: Reset
+    print("\n6. Testing reset...")
 
     engine.reset()
     assert engine.smoothed_ici == 0.0
     assert engine.total_frames == 0
     assert len(engine.processing_times) == 0
-    logger.info("   OK)
+    print("   OK: Reset")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/integration/__pycache__/__init__.cpython-311.pyc b/server/integration/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index ecb0c463073245e663bfe84e9b428da47c24b3cc..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 339
zcmYjN%}N6?5Kh*gEs+(^9wgTu>H`$<;>n8_J(WU8cZclIG+B~p*`9n0UqBFF!^iOI
z3oI=3(31&Px)YfBhWWlP$$S(ABQciSzW$2!CpG^;_n<$I;EhydMiiORiawHYws~jw
zCV+A>pfy~}793+wHG1C2aw!$?TAE6O=cQdW)_@5d4~i+990q>M?*=3cC^c^WoTCYM
zpwI``CXLj35idxRJ>+Y2XcS*SSOb`WEqfSe-?<Q#Erm!RDunnXefrt%%7c3WHy^xe
wE(973LR>fPZgjJ<)w70Mmq&WqhQExdJED|!*(p8lNJ<yvta#1SaVK&73)nGeYXATM

diff --git a/server/integration/components.py b/server/integration/components.py
index dd7920a2ecc0f376e5fb121a5494e5a4e07198df..7a9107b28b680cd65b2b3d7331b179f6f82e26be 100644
--- a/server/integration/components.py
+++ b/server/integration/components.py
@@ -18,75 +18,368 @@ from server.ab_snapshot import ABSnapshot
 from server.metrics_streamer import MetricsStreamer
 from server.latency_api import LatencyStreamer
 from server.auto_phi import AutoPhiLearner, AutoPhiConfig
 from server.criticality_balancer import CriticalityBalancer, CriticalityBalancerConfig
 from server.state_memory import StateMemory, StateMemoryConfig
 from server.state_classifier import StateClassifierGraph, StateClassifierConfig
 from server.predictive_model import PredictiveModel, PredictiveModelConfig
 from server.session_recorder import SessionRecorder, SessionRecorderConfig
 from server.timeline_player import TimelinePlayer, TimelinePlayerConfig
 from server.data_exporter import DataExporter, ExportConfig
 from server.node_sync import NodeSynchronizer, NodeSyncConfig, NodeRole
 from server.phasenet_protocol import PhaseNetNode, PhaseNetConfig as PhaseNetProtoConfig
 from server.cluster_monitor import ClusterMonitor, ClusterMonitorConfig
 from server.hw_interface import HardwareInterface
 from server.hybrid_bridge import HybridBridge
 from server.hybrid_node import HybridNode, HybridNodeConfig, PhiSource
 from server.session_comparator import SessionComparator
 from server.correlation_analyzer import CorrelationAnalyzer
 from server.chromatic_visualizer import ChromaticVisualizer, VisualizerConfig
 from server.state_sync_manager import StateSyncManager, SyncConfig
 
 logger = logging.getLogger(__name__)
 
 
 class ComponentRegistry:
+    """
+    Central registry for all Soundlab components
+
+    Manages lifecycle of all system components based on configuration.
+    Provides centralized access to initialized components.
+    """
+
+    def __init__(self, config: SoundlabConfig):
+        """Initialize component registry"""
+        self.config = config
+
+        # Core components (always initialized)
         self.audio_server: Optional[AudioServer] = None
         self.preset_store: Optional[PresetStore] = None
         self.ab_snapshot: Optional[ABSnapshot] = None
         self.metrics_streamer: Optional[MetricsStreamer] = None
-        self.latency_streamer)
+        self.latency_streamer: Optional[LatencyStreamer] = None
+
+        # Adaptive features (optional)
         self.auto_phi_learner: Optional[AutoPhiLearner] = None
         self.criticality_balancer: Optional[CriticalityBalancer] = None
         self.state_memory: Optional[StateMemory] = None
         self.state_classifier: Optional[StateClassifierGraph] = None
-        self.predictive_model)
+        self.predictive_model: Optional[PredictiveModel] = None
+
+        # Recording/playback (optional)
         self.session_recorder: Optional[SessionRecorder] = None
         self.timeline_player: Optional[TimelinePlayer] = None
         self.data_exporter: Optional[DataExporter] = None
-        self.session_comparator)
+        self.session_comparator: Optional[SessionComparator] = None
+
+        # Networking (optional)
         self.node_synchronizer: Optional[NodeSynchronizer] = None
         self.phasenet_node: Optional[PhaseNetNode] = None
-        self.cluster_monitor)
+        self.cluster_monitor: Optional[ClusterMonitor] = None
+
+        # Visualization/analysis (optional)
         self.correlation_analyzer: Optional[CorrelationAnalyzer] = None
         self.chromatic_visualizer: Optional[ChromaticVisualizer] = None
-        self.state_sync_manager)
+        self.state_sync_manager: Optional[StateSyncManager] = None
+
+        # Hardware (optional)
         self.hardware_interface: Optional[HardwareInterface] = None
         self.hybrid_bridge: Optional[HybridBridge] = None
-        self.hybrid_node)
+        self.hybrid_node: Optional[HybridNode] = None
+
+        logger.info("ComponentRegistry initialized")
+
+    def initialize_all(self) -> None:
+        """Initialize all enabled components"""
+        logger.info("Initializing components...")
+
+        # Core components (always enabled)
+        self._init_core_components()
+
+        # Optional features
+        if self.config.features.auto_phi:
+            self._init_auto_phi()
+
+        if self.config.features.criticality_balancer:
+            self._init_criticality_balancer()
+
+        if self.config.features.state_memory:
+            self._init_state_memory()
+
+        if self.config.features.state_classifier:
+            self._init_state_classifier()
+
+        if self.config.features.predictive_model:
+            self._init_predictive_model()
+
+        if self.config.features.session_recorder:
+            self._init_session_recorder()
+
+        if self.config.features.timeline_player:
+            self._init_timeline_player()
+
+        if self.config.features.data_exporter:
+            self._init_data_exporter()
+
+        if self.config.features.node_sync:
+            self._init_node_sync()
+
+        if self.config.features.phasenet:
+            self._init_phasenet()
+
+        if self.config.features.cluster_monitor:
+            self._init_cluster_monitor()
+
+        if self.config.hardware.bridge_enabled:
+            self._init_hardware_bridge()
+
+        if self.config.hardware.hybrid_bridge_enabled:
+            self._init_hybrid_bridge()
+
+        if self.config.hardware.hybrid_node_enabled:
+            self._init_hybrid_node()
+
+        # Analysis/visualization
+        self._init_correlation_analyzer()
+        self._init_chromatic_visualizer()
+        self._init_state_sync_manager()
+        self._init_session_comparator()
+
+        logger.info("Component initialization complete")
+
+    def _init_core_components(self) -> None:
+        """Initialize core components"""
+        logger.info("Initializing core components...")
+
+        # Audio server
+        self.audio_server = AudioServer(
+            input_device=self.config.audio.input_device,
+            output_device=self.config.audio.output_device,
+            enable_logging=self.config.server.enable_logging
+        )
+
+        # Preset management
+        self.preset_store = PresetStore()
+        self.ab_snapshot = ABSnapshot()
+
+        # Streaming
+        self.metrics_streamer = MetricsStreamer()
+        self.latency_streamer = LatencyStreamer()
+
+        logger.info("Core components initialized")
+
+    def _init_auto_phi(self) -> None:
+        """Initialize Auto-Φ Learner"""
+        logger.info("Initializing Auto-Φ Learner...")
+        config = AutoPhiConfig(
+            enabled=True,
+            k_depth=0.25,
+            gamma_phase=0.1,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.auto_phi_learner = AutoPhiLearner(config)
+
+    def _init_criticality_balancer(self) -> None:
+        """Initialize Criticality Balancer"""
+        logger.info("Initializing Criticality Balancer...")
+        config = CriticalityBalancerConfig(
+            enabled=True,
+            beta_coupling=0.1,
+            delta_amplitude=0.05,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.criticality_balancer = CriticalityBalancer(config)
+
+    def _init_state_memory(self) -> None:
+        """Initialize State Memory"""
+        logger.info("Initializing State Memory...")
+        config = StateMemoryConfig(
+            enabled=True,
+            buffer_size=256,
+            trend_window=30,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.state_memory = StateMemory(config)
+
+    def _init_state_classifier(self) -> None:
+        """Initialize State Classifier"""
+        logger.info("Initializing State Classifier...")
+        config = StateClassifierConfig(
+            hysteresis_threshold=0.1,
+            min_state_duration=0.5,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.state_classifier = StateClassifierGraph(config)
+
+    def _init_predictive_model(self) -> None:
+        """Initialize Predictive Model"""
+        logger.info("Initializing Predictive Model...")
+        config = PredictiveModelConfig(
+            buffer_size=128,
+            prediction_horizon=1.5,
+            min_buffer_size=50,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.predictive_model = PredictiveModel(config)
+
+    def _init_session_recorder(self) -> None:
+        """Initialize Session Recorder"""
+        logger.info("Initializing Session Recorder...")
+        config = SessionRecorderConfig(
+            storage_dir=Path("data/sessions"),
+            enable_logging=self.config.server.enable_logging
+        )
+        self.session_recorder = SessionRecorder(config)
+
+    def _init_timeline_player(self) -> None:
+        """Initialize Timeline Player"""
+        logger.info("Initializing Timeline Player...")
+        config = TimelinePlayerConfig(
+            storage_dir=Path("data/sessions"),
+            enable_logging=self.config.server.enable_logging
+        )
+        self.timeline_player = TimelinePlayer(config)
+
+    def _init_data_exporter(self) -> None:
+        """Initialize Data Exporter"""
+        logger.info("Initializing Data Exporter...")
+        config = ExportConfig(
+            output_dir=Path("data/exports"),
+            enable_logging=self.config.server.enable_logging
+        )
+        self.data_exporter = DataExporter(config)
+
+    def _init_node_sync(self) -> None:
+        """Initialize Node Synchronizer"""
+        logger.info("Initializing Node Synchronizer...")
+        role = NodeRole.MASTER if self.config.node_sync.role == "master" else NodeRole.CLIENT
+        config = NodeSyncConfig(
+            role=role,
+            master_url=self.config.node_sync.master_url,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.node_synchronizer = NodeSynchronizer(config)
+
+    def _init_phasenet(self) -> None:
+        """Initialize PhaseNet Protocol"""
+        logger.info("Initializing PhaseNet...")
+        config = PhaseNetProtoConfig(
+            port=self.config.phasenet.port,
+            encryption_key=self.config.phasenet.encryption_key,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.phasenet_node = PhaseNetNode(config)
+
+    def _init_cluster_monitor(self) -> None:
+        """Initialize Cluster Monitor"""
+        logger.info("Initializing Cluster Monitor...")
+        config = ClusterMonitorConfig(
+            enable_logging=self.config.server.enable_logging
+        )
+        self.cluster_monitor = ClusterMonitor(config)
+
+    def _init_hardware_bridge(self) -> None:
+        """Initialize Hardware I²S Bridge"""
+        logger.info("Initializing Hardware Bridge...")
+        self.hardware_interface = HardwareInterface(
+            port=self.config.hardware.bridge_port,
+            baudrate=self.config.hardware.bridge_baudrate
+        )
+
+    def _init_hybrid_bridge(self) -> None:
+        """Initialize Hybrid Analog-DSP Bridge"""
+        logger.info("Initializing Hybrid Bridge...")
+        self.hybrid_bridge = HybridBridge(
+            port=self.config.hardware.hybrid_bridge_port,
+            baudrate=self.config.hardware.hybrid_bridge_baudrate
+        )
+
+    def _init_hybrid_node(self) -> None:
+        """Initialize Hybrid Node"""
+        logger.info("Initializing Hybrid Node...")
+        config = HybridNodeConfig(
+            analog_input_device=self.config.hardware.hybrid_node_input_device,
+            analog_output_device=self.config.hardware.hybrid_node_output_device,
+            phi_source=PhiSource.EXTERNAL,
+            enable_logging=self.config.server.enable_logging
+        )
+        self.hybrid_node = HybridNode(config)
+
+    def _init_correlation_analyzer(self) -> None:
+        """Initialize Correlation Analyzer"""
+        logger.info("Initializing Correlation Analyzer...")
+        self.correlation_analyzer = CorrelationAnalyzer()
+
+    def _init_chromatic_visualizer(self) -> None:
+        """Initialize Chromatic Visualizer"""
+        logger.info("Initializing Chromatic Visualizer...")
+        config = VisualizerConfig(
+            enable_logging=self.config.server.enable_logging
+        )
+        self.chromatic_visualizer = ChromaticVisualizer(config)
+
+    def _init_state_sync_manager(self) -> None:
+        """Initialize State Sync Manager"""
+        logger.info("Initializing State Sync Manager...")
+        config = SyncConfig(
+            enable_logging=self.config.server.enable_logging
+        )
+        self.state_sync_manager = StateSyncManager(config)
+
+    def _init_session_comparator(self) -> None:
+        """Initialize Session Comparator"""
+        logger.info("Initializing Session Comparator...")
+        self.session_comparator = SessionComparator()
+
+    def shutdown_all(self) -> None:
+        """Shutdown all components gracefully"""
+        logger.info("Shutting down components...")
+
+        if self.audio_server:
+            self.audio_server.stop()
+
+        if self.session_recorder:
+            self.session_recorder.stop_recording()
+
+        if self.node_synchronizer:
+            self.node_synchronizer.disconnect()
+
+        if self.phasenet_node:
+            self.phasenet_node.disconnect()
+
+        if self.hardware_interface:
+            self.hardware_interface.disconnect()
+
+        if self.hybrid_bridge:
+            self.hybrid_bridge.disconnect()
+
+        logger.info("All components shut down")
 
-    def initialize_all(self) :
+    def get_status(self) -> Dict[str, Any]:
         """Get status of all components"""
         return {
             "audio_server": {
-                "running",
-                "sample_rate",
-                "buffer_size",
+                "running": self.audio_server.is_running if self.audio_server else False,
+                "sample_rate": self.audio_server.SAMPLE_RATE if self.audio_server else None,
+                "buffer_size": self.audio_server.BUFFER_SIZE if self.audio_server else None
+            },
             "features": {
-                "auto_phi",
-                "criticality_balancer",
-                "state_memory",
-                "state_classifier",
-                "predictive_model",
-                "session_recorder",
-                "timeline_player",
-                "data_exporter",
+                "auto_phi": self.auto_phi_learner is not None,
+                "criticality_balancer": self.criticality_balancer is not None,
+                "state_memory": self.state_memory is not None,
+                "state_classifier": self.state_classifier is not None,
+                "predictive_model": self.predictive_model is not None,
+                "session_recorder": self.session_recorder is not None,
+                "timeline_player": self.timeline_player is not None,
+                "data_exporter": self.data_exporter is not None
+            },
             "networking": {
-                "node_sync",
-                "phasenet",
-                "cluster_monitor",
+                "node_sync": self.node_synchronizer is not None,
+                "phasenet": self.phasenet_node is not None,
+                "cluster_monitor": self.cluster_monitor is not None
+            },
             "hardware": {
-                "hardware_bridge",
-                "hybrid_bridge",
+                "hardware_bridge": self.hardware_interface is not None,
+                "hybrid_bridge": self.hybrid_bridge is not None,
                 "hybrid_node": self.hybrid_node is not None
             }
         }
diff --git a/server/latency_api.py b/server/latency_api.py
index d4ca12e150d608680c7cae92051850461c7707c8..a2e90eef7da7fd3ad7aa1ba94fdc53d38cc14546 100644
--- a/server/latency_api.py
+++ b/server/latency_api.py
@@ -1,388 +1,489 @@
 """
 Latency REST API & WebSocket - Diagnostics and Monitoring
 
-Implements FR-004, FR-008)
-
+Implements FR-004, FR-008: Complete API for latency telemetry and calibration
+"""
 
 from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Query
 from fastapi.responses import JSONResponse
 from typing import Optional, List
 import asyncio
 import json
 import time
 
 from .latency_manager import LatencyManager
 from .latency_frame import LatencyFrame
 
 
 # Initialize manager (will be set by main app)
 latency_manager: Optional[LatencyManager] = None
 
 
-class LatencyStreamer)
+class LatencyStreamer:
+    """
+    WebSocket broadcaster for real-time latency telemetry
+
+    Similar to MetricsStreamer but for latency diagnostics
+    """
+
+    TARGET_FPS = 10  # 10 Hz for latency updates (less frequent than metrics)
     MAX_CLIENTS = 5
 
-    def __init__(self, manager: LatencyManager) :
+    def __init__(self, manager: LatencyManager):
         """
         Initialize latency streamer
 
         Args:
             manager: LatencyManager instance
         """
         self.manager = manager
-        self.clients, websocket):
+        self.clients: List[WebSocket] = []
+        self.is_running = False
+        self.broadcast_task = None
+
+    async def connect_client(self, websocket: WebSocket):
         """
         Connect new WebSocket client
 
         Args:
-            websocket) >= self.MAX_CLIENTS, reason=f"Max clients ({self.MAX_CLIENTS}) reached")
+            websocket: WebSocket connection
+        """
+        if len(self.clients) >= self.MAX_CLIENTS:
+            await websocket.close(code=1008, reason=f"Max clients ({self.MAX_CLIENTS}) reached")
             return
 
         await websocket.accept()
         self.clients.append(websocket)
 
-        logger.info("[LatencyStreamer] Client connected (total)", len(self.clients))
+        print(f"[LatencyStreamer] Client connected (total: {len(self.clients)})")
 
         # Send initial frame immediately
         frame = self.manager.get_current_frame()
-        try))
-        except, websocket: WebSocket) :
+        try:
+            await websocket.send_text(frame.to_json())
+        except:
+            pass
+
+    def disconnect_client(self, websocket: WebSocket):
         """
         Disconnect WebSocket client
 
         Args:
             websocket: WebSocket connection
         """
-        if websocket in self.clients)
-            logger.info("[LatencyStreamer] Client disconnected (total)", len(self.clients))
+        if websocket in self.clients:
+            self.clients.remove(websocket)
+            print(f"[LatencyStreamer] Client disconnected (total: {len(self.clients)})")
 
-    async def broadcast_frame(self, frame):
+    async def broadcast_frame(self, frame: LatencyFrame):
         """
         Broadcast latency frame to all connected clients
 
         Args:
             frame: LatencyFrame to broadcast
         """
-        if not self.clients)
+        if not self.clients:
+            return
+
+        json_data = frame.to_json()
 
         # Send to all clients, remove disconnected ones
         disconnected = []
 
         for client in self.clients:
-            try)
-            except)
+            try:
+                await client.send_text(json_data)
+            except:
+                disconnected.append(client)
 
         # Remove disconnected clients
-        for client in disconnected)
+        for client in disconnected:
+            self.disconnect_client(client)
 
-    async def broadcast_loop(self))", self.TARGET_FPS)
+    async def broadcast_loop(self):
+        """Background task to broadcast latency frames at TARGET_FPS"""
+        print(f"[LatencyStreamer] Broadcast loop started ({self.TARGET_FPS} Hz)")
 
         frame_interval = 1.0 / self.TARGET_FPS
         last_frame_time = 0.0
 
-        while self.is_running)
+        while self.is_running:
+            current_time = time.time()
 
             # Check if it's time for next frame
-            if current_time - last_frame_time >= frame_interval)
+            if current_time - last_frame_time >= frame_interval:
+                # Get current latency frame
+                frame = self.manager.get_current_frame()
 
                 # Broadcast to all clients
                 await self.broadcast_frame(frame)
 
                 last_frame_time = current_time
 
             # Sleep until next frame (with some margin)
             await asyncio.sleep(frame_interval * 0.5)
 
-        logger.info("[LatencyStreamer] Broadcast loop stopped")
+        print("[LatencyStreamer] Broadcast loop stopped")
 
     async def start(self):
         """Start broadcast loop"""
-        if self.is_running))
+        if self.is_running:
+            return
+
+        self.is_running = True
+        self.broadcast_task = asyncio.create_task(self.broadcast_loop())
 
     async def stop(self):
         """Stop broadcast loop"""
         self.is_running = False
 
-        if self.broadcast_task)
+        if self.broadcast_task:
+            self.broadcast_task.cancel()
             try:
                 await self.broadcast_task
             except asyncio.CancelledError:
                 pass
 
         # Close all client connections
         for client in self.clients[:]:
-            try)
-            except)
+            try:
+                await client.close()
+            except:
+                pass
+
+        self.clients.clear()
 
 
 # Global streamer instance
 latency_streamer: Optional[LatencyStreamer] = None
 
 
-def create_latency_api(manager) :
+def create_latency_api(manager: LatencyManager) -> FastAPI:
     """
     Create FastAPI application with latency endpoints
 
     Args:
         manager: LatencyManager instance
 
-    Returns, version="1.0")
+    Returns:
+        FastAPI application
+    """
+    app = FastAPI(title="Soundlab Latency API", version="1.0")
 
     # Store references
     global latency_manager, latency_streamer
     latency_manager = manager
     latency_streamer = LatencyStreamer(manager)
 
     # --- REST Endpoints ---
 
     @app.get("/api/latency/current")
     async def get_current_latency():
         """
         Get current latency frame
 
+        Returns:
+            Current LatencyFrame as JSON
+        """
+        frame = latency_manager.get_current_frame()
         return JSONResponse(content=frame.to_dict())
 
     @app.get("/api/latency/stats")
     async def get_latency_statistics():
         """
         Get comprehensive latency statistics
 
-        Returns, drift stats, alignment status
+        Returns:
+            Dictionary with latency metrics, drift stats, alignment status
         """
         stats = latency_manager.get_statistics()
         return JSONResponse(content=stats)
 
     @app.post("/api/latency/calibrate")
     async def run_calibration():
         """
         Trigger impulse response calibration
 
-        Note)
+        Note: Requires audio loopback (output → input)
 
         Returns:
             Calibration results with success status
         """
-        try)
+        try:
+            print("[LatencyAPI] Starting calibration...")
 
             # Run calibration (blocking operation)
             success = latency_manager.calibrate()
 
-            if not success,
+            if not success:
+                raise HTTPException(
+                    status_code=500,
                     detail="Calibration failed - check audio loopback connection"
+                )
 
             # Get updated frame
             frame = latency_manager.get_current_frame()
 
             return JSONResponse(content={
-                'success',
-                'calibrated',
-                'total_latency_ms',
-                'effective_latency_ms'),
-                'quality',
-                'aligned'),
-                'latency_frame')
+                'success': True,
+                'calibrated': latency_manager.is_calibrated,
+                'total_latency_ms': frame.total_measured_ms,
+                'effective_latency_ms': frame.get_effective_latency(),
+                'quality': frame.calibration_quality,
+                'aligned': latency_manager.is_aligned(),
+                'latency_frame': frame.to_dict()
             })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.post("/api/latency/compensation/set")
-    async def set_compensation_offset(offset_ms, description="Compensation offset in ms"):
+    async def set_compensation_offset(offset_ms: float = Query(..., description="Compensation offset in ms")):
         """
         Manually set compensation offset
 
         Args:
             offset_ms: Compensation offset in milliseconds
 
         Returns:
             Updated latency frame
         """
         try:
-            if offset_ms < 0 or offset_ms > 200,
+            if offset_ms < 0 or offset_ms > 200:
+                raise HTTPException(
+                    status_code=400,
                     detail="Offset must be between 0 and 200 ms"
+                )
 
             # Update compensation
             latency_manager.latency_frame.compensation_offset_ms = offset_ms
             latency_manager.delay_line.set_delay_ms(offset_ms, latency_manager.sample_rate)
 
             frame = latency_manager.get_current_frame()
 
             return JSONResponse(content={
-                'ok',
-                'compensation_offset_ms',
-                'effective_latency_ms'),
-                'aligned')
+                'ok': True,
+                'compensation_offset_ms': offset_ms,
+                'effective_latency_ms': frame.get_effective_latency(),
+                'aligned': latency_manager.is_aligned()
             })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.post("/api/latency/compensation/adjust")
-    async def adjust_compensation(delta_ms, description="Compensation adjustment in ms"):
+    async def adjust_compensation(delta_ms: float = Query(..., description="Compensation adjustment in ms")):
         """
         Adjust compensation offset by delta
 
         Args:
-            delta_ms, negative = reduce delay)
+            delta_ms: Amount to adjust (positive = add delay, negative = reduce delay)
 
         Returns:
             Updated latency frame
         """
         try:
             current_offset = latency_manager.latency_frame.compensation_offset_ms
             new_offset = current_offset + delta_ms
 
-            if new_offset < 0 or new_offset > 200,
-                    detail=f"Adjusted offset ({new_offset) out of range [0, 200]"
+            if new_offset < 0 or new_offset > 200:
+                raise HTTPException(
+                    status_code=400,
+                    detail=f"Adjusted offset ({new_offset:.2f} ms) out of range [0, 200]"
+                )
 
             # Update compensation
             latency_manager.latency_frame.compensation_offset_ms = new_offset
             latency_manager.delay_line.set_delay_ms(new_offset, latency_manager.sample_rate)
 
             frame = latency_manager.get_current_frame()
 
             return JSONResponse(content={
-                'ok',
-                'compensation_offset_ms',
-                'delta_applied_ms',
-                'effective_latency_ms'),
-                'aligned')
+                'ok': True,
+                'compensation_offset_ms': new_offset,
+                'delta_applied_ms': delta_ms,
+                'effective_latency_ms': frame.get_effective_latency(),
+                'aligned': latency_manager.is_aligned()
             })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.post("/api/latency/compensation/manual")
-    async def set_manual_offset(offset_ms, description="Manual offset in ms")))
+    async def set_manual_offset(offset_ms: float = Query(..., description="Manual offset in ms")):
+        """
+        Set manual user offset (separate from auto compensation)
 
         Args:
             offset_ms: Manual offset in milliseconds
 
         Returns:
             Updated latency frame
         """
         try:
-            if offset_ms < -50 or offset_ms > 50,
+            if offset_ms < -50 or offset_ms > 50:
+                raise HTTPException(
+                    status_code=400,
                     detail="Manual offset must be between -50 and +50 ms"
+                )
 
             latency_manager.latency_frame.manual_offset_ms = offset_ms
 
             frame = latency_manager.get_current_frame()
 
             return JSONResponse(content={
-                'ok',
-                'manual_offset_ms',
-                'effective_latency_ms'),
-                'aligned')
+                'ok': True,
+                'manual_offset_ms': offset_ms,
+                'effective_latency_ms': frame.get_effective_latency(),
+                'aligned': latency_manager.is_aligned()
             })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.get("/api/latency/drift")
     async def get_drift_statistics():
         """
         Get drift monitoring statistics
 
+        Returns:
+            Drift metrics and history
+        """
+        drift_stats = latency_manager.drift_monitor.get_statistics()
         return JSONResponse(content=drift_stats)
 
     @app.post("/api/latency/drift/reset")
-    async def reset_drift_monitor())
+    async def reset_drift_monitor():
+        """
+        Reset drift monitor (clear history)
 
         Returns:
             Success status
         """
-        try)()
+        try:
+            # Reset drift monitor
+            latency_manager.drift_monitor = type(latency_manager.drift_monitor)()
 
             return JSONResponse(content={
-                'ok',
-                'message')
+                'ok': True,
+                'message': 'Drift monitor reset'
+            })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.get("/api/latency/aligned")
-    async def check_alignment(tolerance_ms, description="Alignment tolerance in ms"):
+    async def check_alignment(tolerance_ms: float = Query(5.0, description="Alignment tolerance in ms")):
         """
         Check if system is aligned within tolerance
 
         Args:
-            tolerance_ms: Tolerance in milliseconds (default)
+            tolerance_ms: Tolerance in milliseconds (default: 5.0)
 
+        Returns:
+            Alignment status and effective latency
+        """
+        frame = latency_manager.get_current_frame()
         effective = frame.get_effective_latency()
         aligned = frame.is_aligned(tolerance_ms)
 
         return JSONResponse(content={
-            'aligned',
-            'effective_latency_ms',
-            'tolerance_ms',
-            'within_tolerance') <= tolerance_ms
+            'aligned': aligned,
+            'effective_latency_ms': effective,
+            'tolerance_ms': tolerance_ms,
+            'within_tolerance': abs(effective) <= tolerance_ms
         })
 
     # --- WebSocket Endpoint ---
 
     @app.websocket("/ws/latency")
-    async def websocket_latency_stream(websocket))
+    async def websocket_latency_stream(websocket: WebSocket):
+        """
+        WebSocket endpoint for real-time latency telemetry
+
+        Streams LatencyFrame JSON at 10 Hz
+        """
+        await latency_streamer.connect_client(websocket)
 
         try:
             # Keep connection alive and handle any incoming messages
-            while True, commands, etc.)
-                try), timeout=1.0)
+            while True:
+                # Wait for messages (ping/pong, commands, etc.)
+                try:
+                    data = await asyncio.wait_for(websocket.receive_text(), timeout=1.0)
 
                     # Handle commands
-                    try)
+                    try:
+                        msg = json.loads(data)
 
                         if msg.get('type') == 'ping':
-                            await websocket.send_text(json.dumps({'type'))
+                            await websocket.send_text(json.dumps({'type': 'pong'}))
 
-                        elif msg.get('type') == 'get_stats')
+                        elif msg.get('type') == 'get_stats':
+                            stats = latency_manager.get_statistics()
                             await websocket.send_text(json.dumps({
-                                'type',
-                                'data'))
+                                'type': 'stats',
+                                'data': stats
+                            }))
 
                     except json.JSONDecodeError:
                         pass  # Ignore invalid JSON
 
-                except asyncio.TimeoutError, continue loop
+                except asyncio.TimeoutError:
+                    # No message received, continue loop
                     pass
 
-        except WebSocketDisconnect)
+        except WebSocketDisconnect:
+            latency_streamer.disconnect_client(websocket)
         except Exception as e:
-            logger.error("[LatencyAPI] WebSocket error, e)
+            print(f"[LatencyAPI] WebSocket error: {e}")
             latency_streamer.disconnect_client(websocket)
 
     # --- Lifecycle Events ---
 
     @app.on_event("startup")
-    async def startup_event())
+    async def startup_event():
+        """Start latency streamer on app startup"""
+        print("[LatencyAPI] Starting latency streamer...")
         await latency_streamer.start()
 
     @app.on_event("shutdown")
-    async def shutdown_event())
+    async def shutdown_event():
+        """Stop latency streamer on app shutdown"""
+        print("[LatencyAPI] Stopping latency streamer...")
         await latency_streamer.stop()
 
     return app
 
 
 # Standalone server for testing
-if __name__ == "__main__")
+if __name__ == "__main__":
+    import uvicorn
+
+    # Initialize manager
+    manager = LatencyManager()
 
     # Create app
     app = create_latency_api(manager)
 
-    logger.info("=" * 60)
-    logger.info("Latency API Test Server")
-    logger.info("=" * 60)
-    logger.info("\nStarting server at http://localhost)
-    logger.info("API docs at http://localhost)
-    logger.info("\nEndpoints)
-    logger.info("  GET  /api/latency/current          - Current latency frame")
-    logger.info("  GET  /api/latency/stats            - Full statistics")
-    logger.info("  POST /api/latency/calibrate        - Run calibration")
-    logger.info("  POST /api/latency/compensation/set - Set compensation offset")
-    logger.info("  POST /api/latency/compensation/adjust - Adjust offset by delta")
-    logger.info("  POST /api/latency/compensation/manual - Set manual user offset")
-    logger.info("  GET  /api/latency/drift            - Drift statistics")
-    logger.info("  POST /api/latency/drift/reset      - Reset drift monitor")
-    logger.info("  GET  /api/latency/aligned          - Check alignment")
-    logger.info("  WS   /ws/latency                   - WebSocket stream (10 Hz)")
-    logger.info("\nPress Ctrl+C to stop")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Latency API Test Server")
+    print("=" * 60)
+    print("\nStarting server at http://localhost:8001")
+    print("API docs at http://localhost:8001/docs")
+    print("\nEndpoints:")
+    print("  GET  /api/latency/current          - Current latency frame")
+    print("  GET  /api/latency/stats            - Full statistics")
+    print("  POST /api/latency/calibrate        - Run calibration")
+    print("  POST /api/latency/compensation/set - Set compensation offset")
+    print("  POST /api/latency/compensation/adjust - Adjust offset by delta")
+    print("  POST /api/latency/compensation/manual - Set manual user offset")
+    print("  GET  /api/latency/drift            - Drift statistics")
+    print("  POST /api/latency/drift/reset      - Reset drift monitor")
+    print("  GET  /api/latency/aligned          - Check alignment")
+    print("  WS   /ws/latency                   - WebSocket stream (10 Hz)")
+    print("\nPress Ctrl+C to stop")
+    print("=" * 60)
 
     # Run server
     uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info")
diff --git a/server/latency_frame.py b/server/latency_frame.py
index c718882c9673f554f726d12e4bca87a7e27c9faf..a212ca15ffacdbcc4da168c24ae9c18461f11a4b 100644
--- a/server/latency_frame.py
+++ b/server/latency_frame.py
@@ -1,151 +1,238 @@
 """
 LatencyFrame - Latency Telemetry Data Structure
 
 Implements data container for latency measurements, drift tracking, and compensation state
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import time
 from typing import Optional, Dict
 from dataclasses import dataclass, asdict
 import json
 
 
 @dataclass
 class LatencyFrame:
     """
     Complete latency state snapshot
 
-    Holds, engine, OS)
+    Holds:
+    - Measured latency components (hardware, engine, OS)
     - Computed total and compensation offset
     - Drift metrics
     - Timestamps for synchronization
     """
 
     # Timestamps (monotonic time in seconds)
     timestamp: float  # When this frame was created
-    audio_callback_time)
+    audio_callback_time: Optional[float] = None  # Audio callback start time
+
+    # Latency components (milliseconds)
     hw_input_latency_ms: float = 0.0  # Hardware input buffer latency
     hw_output_latency_ms: float = 0.0  # Hardware output buffer latency
     engine_latency_ms: float = 0.0  # D-ASE processing latency
     os_latency_ms: float = 0.0  # OS/driver latency
     total_measured_ms: float = 0.0  # Round-trip measured latency
 
     # Compensation
     compensation_offset_ms: float = 0.0  # Applied compensation offset
     manual_offset_ms: float = 0.0  # User manual adjustment
 
     # Drift tracking
     drift_ms: float = 0.0  # Cumulative drift from expected timing
     drift_rate_ms_per_sec: float = 0.0  # Rate of drift accumulation
 
     # Calibration state
     calibrated: bool = False  # True if calibration has been performed
-    calibration_quality, based on variance
+    calibration_quality: float = 0.0  # 0-1, based on variance
     last_calibration_time: Optional[float] = None
 
     # Buffer state
     buffer_size_samples: int = 512  # Current audio buffer size
     sample_rate: int = 48000  # Sample rate
-    buffer_fullness, current buffer utilization
+    buffer_fullness: float = 0.0  # 0-1, current buffer utilization
 
     # Performance
-    cpu_load, current CPU utilization
+    cpu_load: float = 0.0  # 0-1, current CPU utilization
+
+    def compute_total(self):
+        """
+        Compute total latency from components (FR-002)
 
-    @lru_cache(maxsize=128)
-    def compute_total(self) :
+        L_total = L_hw_in + L_hw_out + L_engine + L_os
+        """
+        self.total_measured_ms = (
+            self.hw_input_latency_ms +
+            self.hw_output_latency_ms +
+            self.engine_latency_ms +
+            self.os_latency_ms
+        )
+
+    def get_effective_latency(self) -> float:
         """
         Get effective latency after compensation
 
-        Returns, tolerance_ms) :
-            tolerance_ms: Acceptable alignment error (default)
+        Returns:
+            Effective latency in milliseconds
+        """
+        return self.total_measured_ms - self.compensation_offset_ms - self.manual_offset_ms
+
+    def is_aligned(self, tolerance_ms: float = 5.0) -> bool:
+        """
+        Check if latency is within alignment tolerance (SC-002)
+
+        Args:
+            tolerance_ms: Acceptable alignment error (default: 5ms)
 
+        Returns:
+            True if effective latency is within tolerance
+        """
+        effective = self.get_effective_latency()
         return abs(effective) <= tolerance_ms
 
-    def get_buffer_latency_ms(self) :
+    def get_buffer_latency_ms(self) -> float:
         """
         Calculate theoretical buffer latency from size and sample rate
 
         Returns:
             Buffer latency in milliseconds
         """
-        if self.sample_rate > 0) * 1000.0
+        if self.sample_rate > 0:
+            return (self.buffer_size_samples / self.sample_rate) * 1000.0
         return 0.0
 
-    def to_dict(self) :
+    def to_dict(self) -> Dict:
+        """Convert to dictionary for JSON serialization"""
+        return asdict(self)
+
+    def to_json(self, pretty: bool = False) -> str:
         """
         Convert to JSON string
 
         Args:
-            pretty, format with indentation
+            pretty: If True, format with indentation
 
-        if pretty, indent=2)
-        else)
+        Returns:
+            JSON string
+        """
+        data = self.to_dict()
+        if pretty:
+            return json.dumps(data, indent=2)
+        else:
+            return json.dumps(data)
 
     @classmethod
-    def from_dict(cls, data) :
+    def from_dict(cls, data: Dict) -> 'LatencyFrame':
         """Create LatencyFrame from dictionary"""
-        valid_fields = {k, v in data.items() if k in cls.__dataclass_fields__}
+        valid_fields = {k: v for k, v in data.items() if k in cls.__dataclass_fields__}
         return cls(**valid_fields)
 
     @classmethod
-    def from_json(cls, json_str) :
+    def from_json(cls, json_str: str) -> 'LatencyFrame':
+        """Create LatencyFrame from JSON string"""
+        data = json.loads(json_str)
+        return cls.from_dict(data)
+
+    def __repr__(self) -> str:
         return (
-            f"LatencyFrame(total={self.total_measured_ms, "
-            f"offset={self.compensation_offset_ms, "
-            f"drift={self.drift_ms, "
+            f"LatencyFrame(total={self.total_measured_ms:.1f}ms, "
+            f"offset={self.compensation_offset_ms:.1f}ms, "
+            f"drift={self.drift_ms:.2f}ms, "
             f"calibrated={self.calibrated})"
+        )
+
 
-@lru_cache(maxsize=128)
-def create_default_latency_frame() :
+def create_default_latency_frame() -> LatencyFrame:
     """
     Create a default latency frame with typical values
 
-    Returns),
+    Returns:
+        LatencyFrame with reasonable defaults
+    """
+    return LatencyFrame(
+        timestamp=time.time(),
         hw_input_latency_ms=5.0,  # Typical input buffer
         hw_output_latency_ms=5.0,  # Typical output buffer
         engine_latency_ms=2.0,  # D-ASE processing
         os_latency_ms=1.0,  # OS overhead
         buffer_size_samples=512,
         sample_rate=48000
+    )
+
 
 # Self-test function
-@lru_cache(maxsize=128)
-def _self_test() : %sms (expected)", buffer_latency, expected_buffer)
+def _self_test():
+    """Test LatencyFrame functionality"""
+    print("=" * 60)
+    print("LatencyFrame Self-Test")
+    print("=" * 60)
+
+    try:
+        # Test basic creation
+        print("\n1. Creating latency frame...")
+        frame = create_default_latency_frame()
+        print(f"   {frame}")
+        print("   ✓ Creation OK")
+
+        # Test total computation
+        print("\n2. Testing total computation...")
+        frame.compute_total()
+        print(f"   Computed total: {frame.total_measured_ms:.1f}ms")
+        expected = frame.hw_input_latency_ms + frame.hw_output_latency_ms + frame.engine_latency_ms + frame.os_latency_ms
+        assert abs(frame.total_measured_ms - expected) < 0.01
+        print("   ✓ Total computation OK")
+
+        # Test effective latency
+        print("\n3. Testing effective latency...")
+        frame.compensation_offset_ms = 5.0
+        frame.manual_offset_ms = 2.0
+        effective = frame.get_effective_latency()
+        print(f"   Effective latency: {effective:.1f}ms")
+        print(f"   (total {frame.total_measured_ms:.1f}ms - compensation {frame.compensation_offset_ms:.1f}ms - manual {frame.manual_offset_ms:.1f}ms)")
+        print("   ✓ Effective latency OK")
+
+        # Test alignment check
+        print("\n4. Testing alignment check...")
+        frame.compensation_offset_ms = frame.total_measured_ms - 2.0  # 2ms effective
+        is_aligned = frame.is_aligned(tolerance_ms=5.0)
+        print(f"   Aligned (tolerance=5ms): {is_aligned}")
+        assert is_aligned
+        print("   ✓ Alignment check OK")
+
+        # Test buffer latency calculation
+        print("\n5. Testing buffer latency calculation...")
+        buffer_latency = frame.get_buffer_latency_ms()
+        expected_buffer = (512 / 48000) * 1000  # ~10.67ms
+        print(f"   Buffer latency: {buffer_latency:.2f}ms (expected: {expected_buffer:.2f}ms)")
         assert abs(buffer_latency - expected_buffer) < 0.1
-        logger.info("   ✓ Buffer latency OK")
+        print("   ✓ Buffer latency OK")
 
         # Test JSON serialization
-        logger.info("\n6. Testing JSON serialization...")
+        print("\n6. Testing JSON serialization...")
         json_str = frame.to_json()
         frame_restored = LatencyFrame.from_json(json_str)
         assert frame_restored.total_measured_ms == frame.total_measured_ms
         assert frame_restored.buffer_size_samples == frame.buffer_size_samples
-        logger.info("   JSON length, len(json_str))
-        logger.info("   ✓ JSON round-trip OK")
+        print(f"   JSON length: {len(json_str)} bytes")
+        print("   ✓ JSON round-trip OK")
 
         # Test drift scenarios
-        logger.info("\n7. Testing drift scenarios...")
+        print("\n7. Testing drift scenarios...")
         frame.drift_ms = 3.5
         frame.drift_rate_ms_per_sec = 0.2
-        logger.info("   Drift, frame.drift_ms, frame.drift_rate_ms_per_sec)
-        logger.info("   ✓ Drift tracking OK")
+        print(f"   Drift: {frame.drift_ms:.2f}ms @ {frame.drift_rate_ms_per_sec:.2f}ms/s")
+        print("   ✓ Drift tracking OK")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/latency_logger.py b/server/latency_logger.py
index e4e7e0907f90068c46e1deeaaf44530d4b579489..e8a49869d3d30c130a98078b0d4a49a941cd1d1f 100644
--- a/server/latency_logger.py
+++ b/server/latency_logger.py
@@ -1,295 +1,421 @@
 """
 LatencyLogger - Session-based Latency Telemetry Logging
 
-Implements FR-009)
-
+Implements FR-009: Structured logging to /logs/latency/ with CSV + JSONL formats
+"""
 
 import os
 import csv
 import json
 import gzip
 from pathlib import Path
 from datetime import datetime
 from typing import Optional, TextIO
 import threading
 
 from .latency_frame import LatencyFrame
 
 
 class LatencyLogger:
     """
     Session-based logger for latency telemetry
 
+    Features:
+    - Dual-format output (CSV + JSONL)
     - Session-based file organization
     - Automatic compression on close
     - Gap detection and warning
     - Thread-safe logging
     """
 
     # Gap detection threshold
     GAP_THRESHOLD_MS = 100  # Warn if >100ms between frames
 
-    def __init__(self, log_dir: Optional[str]) :
+    def __init__(self, log_dir: Optional[str] = None):
         """
         Initialize latency logger
 
         Args:
-            log_dir)
+            log_dir: Directory for log files (None = ../logs/latency/)
         """
-        if log_dir is None), "..", "logs", "latency")
+        if log_dir is None:
+            log_dir = os.path.join(os.path.dirname(__file__), "..", "logs", "latency")
 
         self.log_dir = Path(log_dir)
         self.log_dir.mkdir(parents=True, exist_ok=True)
 
         # Session tracking
         self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
         self.session_start = datetime.now()
 
         # File handles
         self.csv_file: Optional[TextIO] = None
         self.csv_writer: Optional[csv.DictWriter] = None
         self.jsonl_file: Optional[TextIO] = None
 
         # State
         self.frame_count = 0
-        self.last_timestamp)
+        self.last_timestamp: Optional[float] = None
+        self.gap_count = 0
+
+        # Thread safety
+        self.lock = threading.Lock()
 
         # Initialize files
         self._init_files()
 
-        logger.info("[LatencyLogger] Initialized")
-        logger.info("[LatencyLogger] Session, self.session_id)
-        logger.info("[LatencyLogger] Log dir, self.log_dir)
+        print(f"[LatencyLogger] Initialized")
+        print(f"[LatencyLogger] Session: {self.session_id}")
+        print(f"[LatencyLogger] Log dir: {self.log_dir}")
 
-    def _init_files(self) :
+    def _init_files(self):
         """Initialize CSV and JSONL log files"""
-        try, 'w', newline='')
+        try:
+            # CSV file
+            csv_path = self.log_dir / f"latency_{self.session_id}.csv"
+            self.csv_file = open(csv_path, 'w', newline='')
 
             # CSV header fields
             fieldnames = [
                 'timestamp',
                 'hw_input_latency_ms',
                 'hw_output_latency_ms',
                 'engine_latency_ms',
                 'os_latency_ms',
                 'total_measured_ms',
                 'compensation_offset_ms',
                 'manual_offset_ms',
                 'effective_latency_ms',
                 'drift_ms',
                 'drift_rate_ms_per_sec',
                 'calibrated',
                 'calibration_quality',
                 'buffer_size_samples',
                 'sample_rate',
                 'buffer_fullness',
                 'cpu_load',
                 'aligned_5ms'
             ]
 
             self.csv_writer = csv.DictWriter(self.csv_file, fieldnames=fieldnames)
             self.csv_writer.writeheader()
             self.csv_file.flush()
 
             # JSONL file
             jsonl_path = self.log_dir / f"latency_{self.session_id}.jsonl"
             self.jsonl_file = open(jsonl_path, 'w')
 
             # Write session header to JSONL
             session_header = {
-                'type',
-                'session_id',
-                'timestamp'),
-                'log_dir')
+                'type': 'session_start',
+                'session_id': self.session_id,
+                'timestamp': self.session_start.isoformat(),
+                'log_dir': str(self.log_dir)
             }
             self.jsonl_file.write(json.dumps(session_header) + '\n')
             self.jsonl_file.flush()
 
-            logger.info("[LatencyLogger] Created CSV, csv_path.name)
-            logger.info("[LatencyLogger] Created JSONL, jsonl_path.name)
+            print(f"[LatencyLogger] Created CSV: {csv_path.name}")
+            print(f"[LatencyLogger] Created JSONL: {jsonl_path.name}")
 
         except Exception as e:
-            logger.error("[LatencyLogger] ERROR: Failed to initialize log files, e)
-            if self.csv_file)
-            if self.jsonl_file)
+            print(f"[LatencyLogger] ERROR: Failed to initialize log files: {e}")
+            if self.csv_file:
+                self.csv_file.close()
+            if self.jsonl_file:
+                self.jsonl_file.close()
             raise
 
-    def log_frame(self, frame: LatencyFrame) :
+    def log_frame(self, frame: LatencyFrame):
         """
         Log a latency frame to both CSV and JSONL
 
         Args:
             frame: LatencyFrame to log
         """
         with self.lock:
             try:
                 # Check for gaps
-                if self.last_timestamp is not None) * 1000.0
+                if self.last_timestamp is not None:
+                    gap_ms = (frame.timestamp - self.last_timestamp) * 1000.0
 
                     if gap_ms > self.GAP_THRESHOLD_MS:
                         self.gap_count += 1
-                        logger.warning("[LatencyLogger] WARNING: Gap detected: %s ms (count)", gap_ms, self.gap_count)
+                        print(f"[LatencyLogger] WARNING: Gap detected: {gap_ms:.1f} ms (count: {self.gap_count})")
 
                         # Log gap event to JSONL
                         gap_event = {
-                            'type',
-                            'timestamp',
-                            'gap_ms',
-                            'gap_count') + '\n')
+                            'type': 'gap',
+                            'timestamp': frame.timestamp,
+                            'gap_ms': gap_ms,
+                            'gap_count': self.gap_count
+                        }
+                        self.jsonl_file.write(json.dumps(gap_event) + '\n')
 
                 # Write to CSV
                 csv_row = {
-                    'timestamp',
-                    'hw_input_latency_ms',
-                    'hw_output_latency_ms',
-                    'engine_latency_ms',
-                    'os_latency_ms',
-                    'total_measured_ms',
-                    'compensation_offset_ms',
-                    'manual_offset_ms',
-                    'effective_latency_ms'),
-                    'drift_ms',
-                    'drift_rate_ms_per_sec',
-                    'calibrated',
-                    'calibration_quality',
-                    'buffer_size_samples',
-                    'sample_rate',
-                    'buffer_fullness',
-                    'cpu_load',
-                    'aligned_5ms')
+                    'timestamp': frame.timestamp,
+                    'hw_input_latency_ms': frame.hw_input_latency_ms,
+                    'hw_output_latency_ms': frame.hw_output_latency_ms,
+                    'engine_latency_ms': frame.engine_latency_ms,
+                    'os_latency_ms': frame.os_latency_ms,
+                    'total_measured_ms': frame.total_measured_ms,
+                    'compensation_offset_ms': frame.compensation_offset_ms,
+                    'manual_offset_ms': frame.manual_offset_ms,
+                    'effective_latency_ms': frame.get_effective_latency(),
+                    'drift_ms': frame.drift_ms,
+                    'drift_rate_ms_per_sec': frame.drift_rate_ms_per_sec,
+                    'calibrated': frame.calibrated,
+                    'calibration_quality': frame.calibration_quality,
+                    'buffer_size_samples': frame.buffer_size_samples,
+                    'sample_rate': frame.sample_rate,
+                    'buffer_fullness': frame.buffer_fullness,
+                    'cpu_load': frame.cpu_load,
+                    'aligned_5ms': frame.is_aligned(5.0)
                 }
 
                 self.csv_writer.writerow(csv_row)
 
                 # Write to JSONL (full frame as JSON)
                 jsonl_entry = {
-                    'type',
+                    'type': 'frame',
                     **frame.to_dict()
                 }
                 self.jsonl_file.write(json.dumps(jsonl_entry) + '\n')
 
                 # Flush periodically (every 10 frames)
                 self.frame_count += 1
-                if self.frame_count % 10 == 0)
+                if self.frame_count % 10 == 0:
+                    self.csv_file.flush()
                     self.jsonl_file.flush()
 
                 self.last_timestamp = frame.timestamp
 
             except Exception as e:
-                logger.error("[LatencyLogger] ERROR: Failed to log frame, e)
+                print(f"[LatencyLogger] ERROR: Failed to log frame: {e}")
 
-    def log_calibration_event(self, success: bool, details: dict) :
+    def log_calibration_event(self, success: bool, details: dict):
         """
         Log a calibration event
 
         Args:
             success: True if calibration succeeded
             details: Calibration details dictionary
         """
         with self.lock:
             try:
                 event = {
-                    'type',
-                    'timestamp').isoformat(),
-                    'success',
+                    'type': 'calibration',
+                    'timestamp': datetime.now().isoformat(),
+                    'success': success,
                     **details
                 }
 
                 self.jsonl_file.write(json.dumps(event) + '\n')
                 self.jsonl_file.flush()
 
-                logger.info("[LatencyLogger] Logged calibration event, success)
+                print(f"[LatencyLogger] Logged calibration event: success={success}")
 
             except Exception as e:
-                logger.error("[LatencyLogger] ERROR: Failed to log calibration event, e)
+                print(f"[LatencyLogger] ERROR: Failed to log calibration event: {e}")
 
-    def log_drift_correction(self, correction_ms: float, reason: str) :
+    def log_drift_correction(self, correction_ms: float, reason: str):
         """
         Log a drift correction event
 
         Args:
             correction_ms: Correction applied in milliseconds
             reason: Reason for correction
         """
         with self.lock:
             try:
                 event = {
-                    'type',
-                    'timestamp').isoformat(),
-                    'correction_ms',
-                    'reason') + '\n')
+                    'type': 'drift_correction',
+                    'timestamp': datetime.now().isoformat(),
+                    'correction_ms': correction_ms,
+                    'reason': reason
+                }
+
+                self.jsonl_file.write(json.dumps(event) + '\n')
                 self.jsonl_file.flush()
 
-                logger.info("[LatencyLogger] Logged drift correction)", correction_ms, reason)
+                print(f"[LatencyLogger] Logged drift correction: {correction_ms:+.2f} ms ({reason})")
 
             except Exception as e:
-                logger.error("[LatencyLogger] ERROR: Failed to log drift correction, e)
+                print(f"[LatencyLogger] ERROR: Failed to log drift correction: {e}")
 
-    def get_session_statistics(self) :
+    def get_session_statistics(self) -> dict:
         """
         Get session statistics
 
-        Returns) - self.session_start).total_seconds()
+        Returns:
+            Dictionary with session stats
+        """
+        elapsed = (datetime.now() - self.session_start).total_seconds()
 
         return {
-            'session_id',
-            'session_start'),
-            'elapsed_seconds',
-            'frame_count',
-            'gap_count',
-            'average_fps',
-            'log_dir')
+            'session_id': self.session_id,
+            'session_start': self.session_start.isoformat(),
+            'elapsed_seconds': elapsed,
+            'frame_count': self.frame_count,
+            'gap_count': self.gap_count,
+            'average_fps': self.frame_count / elapsed if elapsed > 0 else 0,
+            'log_dir': str(self.log_dir)
         }
 
-    def close(self) :
+    def close(self):
         """Close log files and compress"""
-        with self.lock)
+        with self.lock:
+            print("[LatencyLogger] Closing session...")
 
             # Write session end to JSONL
             if self.jsonl_file and not self.jsonl_file.closed:
                 session_end = {
-                    'type',
-                    'timestamp').isoformat(),
-                    'statistics')
+                    'type': 'session_end',
+                    'timestamp': datetime.now().isoformat(),
+                    'statistics': self.get_session_statistics()
                 }
                 self.jsonl_file.write(json.dumps(session_end) + '\n')
                 self.jsonl_file.flush()
 
             # Close files
-            if self.csv_file and not self.csv_file.closed)
+            if self.csv_file and not self.csv_file.closed:
+                self.csv_file.close()
 
-            if self.jsonl_file and not self.jsonl_file.closed)
+            if self.jsonl_file and not self.jsonl_file.closed:
+                self.jsonl_file.close()
 
             # Compress files
             self._compress_files()
 
-            logger.info("[LatencyLogger] Session closed, self.frame_count)
+            print(f"[LatencyLogger] Session closed: {self.frame_count} frames logged")
 
-    def _compress_files(self) :
+    def _compress_files(self):
         """Compress CSV and JSONL files with gzip"""
-        try), csv_path.name)
-                with open(csv_path, 'rb') as f_in, 'wb') as f_out)
+        try:
+            csv_path = self.log_dir / f"latency_{self.session_id}.csv"
+            jsonl_path = self.log_dir / f"latency_{self.session_id}.jsonl"
+
+            # Compress CSV
+            if csv_path.exists():
+                print(f"[LatencyLogger] Compressing {csv_path.name}...")
+                with open(csv_path, 'rb') as f_in:
+                    with gzip.open(f"{csv_path}.gz", 'wb') as f_out:
+                        f_out.writelines(f_in)
                 csv_path.unlink()  # Remove original
-                logger.info("[LatencyLogger] Created %s.gz", csv_path.name)
+                print(f"[LatencyLogger] Created {csv_path.name}.gz")
 
             # Compress JSONL
-            if jsonl_path.exists(), jsonl_path.name)
-                with open(jsonl_path, 'rb') as f_in, 'wb') as f_out)
+            if jsonl_path.exists():
+                print(f"[LatencyLogger] Compressing {jsonl_path.name}...")
+                with open(jsonl_path, 'rb') as f_in:
+                    with gzip.open(f"{jsonl_path}.gz", 'wb') as f_out:
+                        f_out.writelines(f_in)
                 jsonl_path.unlink()  # Remove original
-                logger.info("[LatencyLogger] Created %s.gz", jsonl_path.name)
+                print(f"[LatencyLogger] Created {jsonl_path.name}.gz")
 
         except Exception as e:
-            logger.error("[LatencyLogger] WARNING: Compression failed, e)
+            print(f"[LatencyLogger] WARNING: Compression failed: {e}")
 
-    def __del__(self), 'csv_file') and self.csv_file and not self.csv_file.closed)
+    def __del__(self):
+        """Ensure cleanup on destruction"""
+        if hasattr(self, 'csv_file') and self.csv_file and not self.csv_file.closed:
+            self.close()
 
 
 # Self-test function
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test LatencyLogger functionality"""
+    print("=" * 60)
+    print("LatencyLogger Self-Test")
+    print("=" * 60)
+
+    try:
+        import tempfile
+        import shutil
+        from latency_frame import create_default_latency_frame
+        import time
+
+        # Create temporary directory for testing
+        temp_dir = tempfile.mkdtemp()
+        print(f"\n1. Using temp directory: {temp_dir}")
+
+        # Initialize logger
+        print("\n2. Initializing logger...")
+        logger = LatencyLogger(log_dir=temp_dir)
+        print(f"   Session: {logger.session_id}")
+        print("   ✓ Logger initialized")
+
+        # Log some frames
+        print("\n3. Logging frames...")
+        for i in range(20):
+            frame = create_default_latency_frame()
+            frame.timestamp = time.time()
+            frame.hw_input_latency_ms = 5.0 + i * 0.1
+            frame.drift_ms = i * 0.05
+
+            logger.log_frame(frame)
+
+            time.sleep(0.01)  # Small delay
+
+        print(f"   Logged {logger.frame_count} frames")
+        print("   ✓ Frame logging OK")
+
+        # Log calibration event
+        print("\n4. Logging calibration event...")
+        logger.log_calibration_event(True, {
+            'total_latency_ms': 13.5,
+            'quality': 0.95
+        })
+        print("   ✓ Calibration event logged")
+
+        # Log drift correction
+        print("\n5. Logging drift correction...")
+        logger.log_drift_correction(-2.5, "Exceeded 2ms threshold")
+        print("   ✓ Drift correction logged")
+
+        # Get session statistics
+        print("\n6. Getting session statistics...")
+        stats = logger.get_session_statistics()
+        print(f"   Session ID: {stats['session_id']}")
+        print(f"   Frames: {stats['frame_count']}")
+        print(f"   Average FPS: {stats['average_fps']:.1f}")
+        print(f"   Gaps: {stats['gap_count']}")
+        print("   ✓ Statistics OK")
+
+        # Close logger (triggers compression)
+        print("\n7. Closing logger...")
+        logger.close()
+        print("   ✓ Logger closed")
+
+        # Verify compressed files exist
+        print("\n8. Verifying compressed files...")
+        csv_gz = Path(temp_dir) / f"latency_{logger.session_id}.csv.gz"
+        jsonl_gz = Path(temp_dir) / f"latency_{logger.session_id}.jsonl.gz"
+
+        assert csv_gz.exists(), "CSV.gz not found"
+        assert jsonl_gz.exists(), "JSONL.gz not found"
+
+        print(f"   CSV.gz size: {csv_gz.stat().st_size} bytes")
+        print(f"   JSONL.gz size: {jsonl_gz.stat().st_size} bytes")
+        print("   ✓ Compression OK")
+
+        # Cleanup
+        print("\n9. Cleanup...")
+        shutil.rmtree(temp_dir)
+        print("   ✓ Cleanup complete")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/latency_manager.py b/server/latency_manager.py
index c3d400855c3af124209a598481b005f511d3e298..5dc2ac5ddd06878a94ed1d71cf492a1a99389feb 100644
--- a/server/latency_manager.py
+++ b/server/latency_manager.py
@@ -1,503 +1,655 @@
 """
 LatencyManager - Calibration, Measurement, and Compensation
 
-Implements FR-001, FR-003, FR-004, FR-005, FR-006, FR-007)
-
+Implements FR-001, FR-003, FR-004, FR-005, FR-006, FR-007:
+- Impulse response calibration with sounddevice.playrec
+- Continuous drift monitoring and correction
+- Delay line buffer for real-time compensation
+- Synchronized timestamping
+"""
 
 import numpy as np
 import sounddevice as sd
 import time
 import threading
 from typing import Optional, Tuple, Dict, List
 from collections import deque
 from datetime import datetime
 import os
 from pathlib import Path
 
 from .latency_frame import LatencyFrame, create_default_latency_frame
 
 
 class DriftMonitor:
+    """
+    Monitors long-term timing drift (FR-006, FR-007)
 
     Tracks cumulative drift and rate of change to maintain <2ms per 10min
     """
 
     DRIFT_HISTORY_DURATION = 600.0  # 10 minutes
     DRIFT_ALERT_THRESHOLD = 2.0  # ms per 10 min
 
-    def __init__(self) :
+    def __init__(self):
         """Initialize drift monitor"""
-        self.drift_samples)  # (timestamp, drift_ms)
+        self.drift_samples: deque = deque(maxlen=10000)  # (timestamp, drift_ms)
         self.start_time = time.time()
         self.last_correction_time = self.start_time
         self.cumulative_drift_ms = 0.0
 
-    def add_measurement(self, expected_time: float, actual_time: float) :
+    def add_measurement(self, expected_time: float, actual_time: float):
         """
         Add a timing measurement
 
         Args:
-            expected_time)
-            actual_time)
+            expected_time: Expected callback time (monotonic)
+            actual_time: Actual callback time (monotonic)
         """
         drift_ms = (actual_time - expected_time) * 1000.0
         self.drift_samples.append((actual_time, drift_ms))
         self.cumulative_drift_ms += drift_ms
 
-    @lru_cache(maxsize=128)
-    def get_drift_rate(self) :
+    def get_drift_rate(self) -> float:
         """
         Calculate drift rate in ms/second
 
+        Returns:
+            Drift rate (ms/s)
         """
-        if len(self.drift_samples) < 2)
+        if len(self.drift_samples) < 2:
+            return 0.0
+
+        # Linear regression over recent samples
+        times = np.array([s[0] for s in self.drift_samples])
         drifts = np.array([s[1] for s in self.drift_samples])
 
-        if len(times) < 10, len(times))
+        if len(times) < 10:
+            return 0.0
+
+        # Use last 100 samples or all if fewer
+        window_size = min(100, len(times))
         times = times[-window_size:]
         drifts = drifts[-window_size:]
 
         # Fit line: drift = rate * time + offset
         time_span = times[-1] - times[0]
 
-        if time_span < 1.0) :
+        if time_span < 1.0:  # Need at least 1 second of data
+            return 0.0
+
+        drift_change = drifts[-1] - drifts[0]
+        rate = drift_change / time_span  # ms/s
+
+        return rate
+
+    def get_current_drift(self) -> float:
         """
         Get current instantaneous drift
 
         Returns:
             Current drift in milliseconds
         """
-        if not self.drift_samples) :
+        if not self.drift_samples:
+            return 0.0
+
+        return self.drift_samples[-1][1]
+
+    def needs_correction(self) -> bool:
         """
         Check if drift correction is needed
 
-        Returns) - self.last_correction_time) / 60.0
+        Returns:
+            True if drift exceeds threshold
+        """
+        elapsed_minutes = (time.time() - self.last_correction_time) / 60.0
+
+        if elapsed_minutes < 1.0:  # Check at most once per minute
+            return False
+
+        # Check if drift exceeds proportional threshold
+        threshold = (elapsed_minutes / 10.0) * self.DRIFT_ALERT_THRESHOLD
+
+        if abs(self.cumulative_drift_ms) > threshold:
+            return True
 
-        if elapsed_minutes < 1.0) * self.DRIFT_ALERT_THRESHOLD
+        return False
 
-        if abs(self.cumulative_drift_ms) > threshold, correction_ms: float) :
+    def apply_correction(self, correction_ms: float):
         """
         Record that a drift correction was applied
 
         Args:
-            correction_ms)
+            correction_ms: Correction applied (positive = add delay)
         """
         self.cumulative_drift_ms = 0.0
         self.last_correction_time = time.time()
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
         """Get drift statistics"""
         return {
-            'current_drift_ms'),
-            'drift_rate_ms_per_sec'),
-            'cumulative_drift_ms',
-            'sample_count'),
-            'elapsed_minutes') - self.start_time) / 60.0
+            'current_drift_ms': self.get_current_drift(),
+            'drift_rate_ms_per_sec': self.get_drift_rate(),
+            'cumulative_drift_ms': self.cumulative_drift_ms,
+            'sample_count': len(self.drift_samples),
+            'elapsed_minutes': (time.time() - self.start_time) / 60.0
         }
 
 
-class DelayLineBuffer)
+class DelayLineBuffer:
+    """
+    Circular buffer for latency compensation (FR-003)
 
     Provides sample-accurate delay with fractional sample interpolation
     """
 
-    def __init__(self, max_delay_samples: int, num_channels: int) :
+    def __init__(self, max_delay_samples: int, num_channels: int = 1):
         """
         Initialize delay buffer
 
         Args:
             max_delay_samples: Maximum delay in samples
-            num_channels, num_channels), dtype=np.float32)
+            num_channels: Number of audio channels
+        """
+        self.max_delay_samples = max_delay_samples
+        self.num_channels = num_channels
+
+        # Allocate buffer with extra space for interpolation
+        self.buffer = np.zeros((max_delay_samples + 4, num_channels), dtype=np.float32)
         self.write_pos = 0
         self.current_delay_samples = 0.0  # Can be fractional
 
-    @lru_cache(maxsize=128)
-    def set_delay_ms(self, delay_ms: float, sample_rate: int) :
+    def set_delay_ms(self, delay_ms: float, sample_rate: int):
         """
         Set delay in milliseconds
 
         Args:
             delay_ms: Delay in milliseconds
-            sample_rate) * sample_rate
+            sample_rate: Audio sample rate
+        """
+        delay_samples = (delay_ms / 1000.0) * sample_rate
         self.current_delay_samples = max(0.0, min(delay_samples, self.max_delay_samples))
 
-    @lru_cache(maxsize=128)
-    def process(self, input_block) :
+    def process(self, input_block: np.ndarray) -> np.ndarray:
         """
         Process audio block with delay
 
         Args:
-            input_block, num_channels)
+            input_block: Input audio (num_samples, num_channels)
 
+        Returns:
+            Delayed audio (same shape as input)
         """
         num_samples = input_block.shape[0]
         output = np.zeros_like(input_block)
 
-        for i in range(num_samples))
+        for i in range(num_samples):
+            # Write input to buffer
+            self.buffer[self.write_pos] = input_block[i]
+
+            # Calculate read position (can be fractional)
             read_pos_float = self.write_pos - self.current_delay_samples
 
-            if read_pos_float < 0)
+            if read_pos_float < 0:
+                read_pos_float += self.max_delay_samples
+
+            # Linear interpolation for fractional delay
+            read_pos_int = int(read_pos_float)
             frac = read_pos_float - read_pos_int
 
             pos0 = read_pos_int % self.max_delay_samples
             pos1 = (read_pos_int + 1) % self.max_delay_samples
 
             # Interpolate
             output[i] = self.buffer[pos0] * (1.0 - frac) + self.buffer[pos1] * frac
 
             # Advance write position
             self.write_pos = (self.write_pos + 1) % self.max_delay_samples
 
         return output
 
 
 class LatencyManager:
     """
     Complete latency measurement and compensation system
 
+    Features:
+    - Impulse response calibration
+    - Hardware + OS latency measurement
+    - Continuous drift monitoring
+    - Real-time delay compensation
+    - Synchronized timestamping
+    """
+
+    # Calibration settings
+    IMPULSE_DURATION = 0.1  # 100ms impulse burst
+    IMPULSE_FREQUENCY = 1000.0  # 1kHz sine wave
+    IMPULSE_AMPLITUDE = 0.5
+    CALIBRATION_SAMPLE_RATE = 48000
+
+    # Tolerance and drift thresholds (from spec)
     TARGET_ALIGNMENT_MS = 5.0  # SC-002
     MAX_DRIFT_PER_10MIN = 2.0  # SC-003
 
     def __init__(self,
-                 sample_rate,
-                 buffer_size,
-                 input_device,
-                 output_device):
+                 sample_rate: int = 48000,
+                 buffer_size: int = 512,
+                 input_device: Optional[int] = None,
+                 output_device: Optional[int] = None):
         """
         Initialize latency manager
 
         Args:
             sample_rate: Audio sample rate
             buffer_size: Audio buffer size in samples
-            input_device)
-            output_device)
+            input_device: Input device index (None = default)
+            output_device: Output device index (None = default)
         """
         self.sample_rate = sample_rate
         self.buffer_size = buffer_size
         self.input_device = input_device
         self.output_device = output_device
 
         # State
         self.latency_frame = create_default_latency_frame()
         self.latency_frame.sample_rate = sample_rate
         self.latency_frame.buffer_size_samples = buffer_size
 
         self.drift_monitor = DriftMonitor()
 
         # Delay line for compensation
         max_delay_ms = 200.0  # Support up to 200ms compensation
         max_delay_samples = int((max_delay_ms / 1000.0) * sample_rate)
         self.delay_line = DelayLineBuffer(max_delay_samples, num_channels=2)
 
         # Calibration state
         self.is_calibrated = False
         self.calibration_lock = threading.Lock()
 
         # Performance tracking
         self.expected_callback_time = None
 
-        logger.info("[LatencyManager] Initialized")
-        logger.info("[LatencyManager] Sample rate, sample_rate)
-        logger.info("[LatencyManager] Buffer size, buffer_size)
-        logger.info("[LatencyManager] Max compensation, max_delay_ms)
+        print(f"[LatencyManager] Initialized")
+        print(f"[LatencyManager] Sample rate: {sample_rate} Hz")
+        print(f"[LatencyManager] Buffer size: {buffer_size} samples")
+        print(f"[LatencyManager] Max compensation: {max_delay_ms:.1f} ms")
+
+    def calibrate(self) -> bool:
+        """
+        Perform impulse response calibration (FR-001)
+
+        Plays impulse, records loopback, measures round-trip latency
 
-    @lru_cache(maxsize=128)
-    def calibrate(self) :
-            try)
+        Returns:
+            True if calibration successful
+        """
+        print("\n[LatencyManager] Starting calibration...")
+        print("[LatencyManager] WARNING: Ensure audio loopback is connected!")
+        print("[LatencyManager] (Output → Input with cable or virtual loopback)")
+
+        with self.calibration_lock:
+            try:
+                # Generate impulse signal
+                impulse_samples = int(self.IMPULSE_DURATION * self.CALIBRATION_SAMPLE_RATE)
                 t = np.linspace(0, self.IMPULSE_DURATION, impulse_samples)
                 impulse = self.IMPULSE_AMPLITUDE * np.sin(2 * np.pi * self.IMPULSE_FREQUENCY * t)
 
                 # Add short silence before and after
                 silence_samples = int(0.05 * self.CALIBRATION_SAMPLE_RATE)
                 impulse = np.concatenate([
                     np.zeros(silence_samples),
                     impulse,
                     np.zeros(impulse_samples)  # Extra silence for latency measurement
                 ])
 
                 # Reshape for stereo output
                 impulse_stereo = np.column_stack([impulse, impulse])
 
-                logger.info("[LatencyManager] Playing %ss impulse...", len(impulse)/self.CALIBRATION_SAMPLE_RATE)
+                print(f"[LatencyManager] Playing {len(impulse)/self.CALIBRATION_SAMPLE_RATE:.2f}s impulse...")
 
                 # Play and record simultaneously
                 recording = sd.playrec(
                     impulse_stereo,
                     samplerate=self.CALIBRATION_SAMPLE_RATE,
                     channels=2,
                     device=(self.input_device, self.output_device),
                     blocking=True
+                )
 
-                logger.info("[LatencyManager] Recording complete, analyzing...")
+                print("[LatencyManager] Recording complete, analyzing...")
 
                 # Analyze recording to find impulse response
                 # Use cross-correlation to find delay
-                input_signal = impulse[)]
+                input_signal = impulse[:len(recording)]
                 recorded_signal = recording[:, 0]  # Use left channel
 
                 # Normalize signals
                 input_signal = input_signal / (np.max(np.abs(input_signal)) + 1e-10)
                 recorded_signal = recorded_signal / (np.max(np.abs(recorded_signal)) + 1e-10)
 
                 # Cross-correlation
                 correlation = np.correlate(recorded_signal, input_signal, mode='full')
 
                 # Find peak
                 peak_index = np.argmax(np.abs(correlation))
                 delay_samples = peak_index - len(input_signal) + 1
 
                 # Convert to milliseconds
                 measured_latency_ms = (delay_samples / self.CALIBRATION_SAMPLE_RATE) * 1000.0
 
-                logger.info("[LatencyManager] Measured round-trip latency, measured_latency_ms)
+                print(f"[LatencyManager] Measured round-trip latency: {measured_latency_ms:.2f} ms")
 
                 # Validate measurement
                 if measured_latency_ms < 0 or measured_latency_ms > 500:
-                    logger.error("[LatencyManager] ERROR: Invalid latency measurement, measured_latency_ms)
-                    logger.info("[LatencyManager] Check that audio loopback is properly connected")
+                    print(f"[LatencyManager] ERROR: Invalid latency measurement: {measured_latency_ms:.2f} ms")
+                    print("[LatencyManager] Check that audio loopback is properly connected")
                     return False
 
                 # Calculate quality metric (based on correlation peak sharpness)
                 peak_value = np.abs(correlation[peak_index])
                 mean_value = np.mean(np.abs(correlation))
                 quality = min(1.0, peak_value / (mean_value * 10 + 1e-10))
 
-                logger.info("[LatencyManager] Calibration quality, quality)
+                print(f"[LatencyManager] Calibration quality: {quality:.2f}")
 
                 if quality < 0.3:
-                    logger.warning("[LatencyManager] WARNING)
+                    print("[LatencyManager] WARNING: Low calibration quality - results may be inaccurate")
 
                 # Get device-reported latencies
                 device_info = sd.query_devices(self.input_device, 'input')
                 hw_input_latency = device_info['default_low_input_latency'] * 1000.0  # Convert to ms
 
                 device_info = sd.query_devices(self.output_device, 'output')
                 hw_output_latency = device_info['default_low_output_latency'] * 1000.0
 
                 # Estimate component latencies
                 buffer_latency = (self.buffer_size / self.sample_rate) * 1000.0
 
                 # Update latency frame
                 self.latency_frame.hw_input_latency_ms = hw_input_latency
                 self.latency_frame.hw_output_latency_ms = hw_output_latency
                 self.latency_frame.engine_latency_ms = buffer_latency  # Estimate
                 self.latency_frame.os_latency_ms = max(0, measured_latency_ms - hw_input_latency - hw_output_latency - buffer_latency)
 
                 self.latency_frame.compute_total()
 
                 # Set compensation offset to measured latency
                 self.latency_frame.compensation_offset_ms = measured_latency_ms
                 self.latency_frame.calibrated = True
                 self.latency_frame.calibration_quality = quality
                 self.latency_frame.last_calibration_time = time.time()
 
                 # Configure delay line
                 self.delay_line.set_delay_ms(measured_latency_ms, self.sample_rate)
 
                 self.is_calibrated = True
 
-                logger.info("[LatencyManager] ✓ Calibration complete")
-                logger.info("[LatencyManager] Component breakdown)
-                logger.info("[LatencyManager]   HW Input, hw_input_latency)
-                logger.info("[LatencyManager]   HW Output, hw_output_latency)
-                logger.info("[LatencyManager]   Engine, buffer_latency)
-                logger.info("[LatencyManager]   OS, self.latency_frame.os_latency_ms)
-                logger.info("[LatencyManager]   Total, self.latency_frame.total_measured_ms)
+                print("[LatencyManager] ✓ Calibration complete")
+                print(f"[LatencyManager] Component breakdown:")
+                print(f"[LatencyManager]   HW Input:  {hw_input_latency:.2f} ms")
+                print(f"[LatencyManager]   HW Output: {hw_output_latency:.2f} ms")
+                print(f"[LatencyManager]   Engine:    {buffer_latency:.2f} ms")
+                print(f"[LatencyManager]   OS:        {self.latency_frame.os_latency_ms:.2f} ms")
+                print(f"[LatencyManager]   Total:     {self.latency_frame.total_measured_ms:.2f} ms")
 
                 return True
 
             except Exception as e:
-                logger.error("[LatencyManager] ✗ Calibration failed, e)
+                print(f"[LatencyManager] ✗ Calibration failed: {e}")
                 import traceback
                 traceback.print_exc()
                 return False
 
-    @lru_cache(maxsize=128)
-    def compensate_block(self, audio_block) :
+    def compensate_block(self, audio_block: np.ndarray) -> np.ndarray:
         """
         Apply latency compensation to audio block
 
         Args:
-            audio_block, num_channels)
+            audio_block: Input audio (num_samples, num_channels)
 
+        Returns:
+            Compensated audio (same shape)
         """
-        if not self.is_calibrated)
+        if not self.is_calibrated:
+            return audio_block  # Pass through if not calibrated
 
-    @lru_cache(maxsize=128)
-    def update_timing(self, callback_time: float, expected_time: Optional[float]) :
+        return self.delay_line.process(audio_block)
+
+    def update_timing(self, callback_time: float, expected_time: Optional[float] = None):
         """
         Update timing measurements for drift monitoring
 
         Args:
-            callback_time)
-            expected_time), or None to auto-calculate
+            callback_time: Actual callback time (monotonic)
+            expected_time: Expected callback time (monotonic), or None to auto-calculate
         """
         if expected_time is None:
-            if self.expected_callback_time is None, callback_time)
+            if self.expected_callback_time is None:
+                self.expected_callback_time = callback_time
+                return
+
+            # Calculate expected time based on buffer size
+            buffer_duration = self.buffer_size / self.sample_rate
+            expected_time = self.expected_callback_time + buffer_duration
+
+        # Record drift measurement
+        self.drift_monitor.add_measurement(expected_time, callback_time)
 
         # Update latency frame
         self.latency_frame.drift_ms = self.drift_monitor.get_current_drift()
         self.latency_frame.drift_rate_ms_per_sec = self.drift_monitor.get_drift_rate()
 
         # Check if correction needed
-        if self.drift_monitor.needs_correction())
+        if self.drift_monitor.needs_correction():
+            correction = -self.latency_frame.drift_ms  # Negative because we want to counteract drift
+            self.apply_drift_correction(correction)
 
         # Update expected time for next callback
         self.expected_callback_time = callback_time
 
-    def apply_drift_correction(self, correction_ms: float) :
+    def apply_drift_correction(self, correction_ms: float):
         """
         Apply drift correction
 
         Args:
             correction_ms: Correction to apply in milliseconds
         """
-        logger.info("[LatencyManager] Applying drift correction, correction_ms)
+        print(f"[LatencyManager] Applying drift correction: {correction_ms:+.2f} ms")
 
         # Update compensation offset
         self.latency_frame.compensation_offset_ms += correction_ms
 
         # Update delay line
         self.delay_line.set_delay_ms(
             self.latency_frame.compensation_offset_ms,
             self.sample_rate
+        )
 
         # Notify drift monitor
         self.drift_monitor.apply_correction(correction_ms)
 
-    def get_current_frame(self) :
+    def get_current_frame(self) -> LatencyFrame:
         """
         Get current latency frame with timestamp
 
+        Returns:
+            Current LatencyFrame snapshot
+        """
+        self.latency_frame.timestamp = time.time()
         return self.latency_frame
 
-    def is_aligned(self, tolerance_ms) :
+    def is_aligned(self, tolerance_ms: Optional[float] = None) -> bool:
         """
         Check if system is within alignment tolerance
 
         Args:
-            tolerance_ms)
+            tolerance_ms: Tolerance in ms (None = use TARGET_ALIGNMENT_MS)
 
         Returns:
             True if aligned within tolerance
         """
-        if tolerance_ms is None)
+        if tolerance_ms is None:
+            tolerance_ms = self.TARGET_ALIGNMENT_MS
 
-    def get_statistics(self) :
+        return self.latency_frame.is_aligned(tolerance_ms)
+
+    def get_statistics(self) -> Dict:
         """
         Get comprehensive latency statistics
 
         Returns:
             Dictionary with all latency metrics
         """
         return {
-            'calibrated',
-            'latency'),
-            'drift'),
-            'aligned'),
-            'effective_latency_ms')
+            'calibrated': self.is_calibrated,
+            'latency': self.latency_frame.to_dict(),
+            'drift': self.drift_monitor.get_statistics(),
+            'aligned': self.is_aligned(),
+            'effective_latency_ms': self.latency_frame.get_effective_latency()
         }
 
 
 # Self-test function
-def _self_test() :, 0]))
+def _self_test():
+    """Test LatencyManager functionality"""
+    print("=" * 60)
+    print("LatencyManager Self-Test")
+    print("=" * 60)
+
+    try:
+        # Test drift monitor
+        print("\n1. Testing DriftMonitor...")
+        drift_mon = DriftMonitor()
+
+        # Simulate some drift measurements
+        base_time = time.time()
+        for i in range(100):
+            expected = base_time + i * 0.01
+            actual = expected + 0.0001 * i  # Gradual drift
+            drift_mon.add_measurement(expected, actual)
+
+        drift_rate = drift_mon.get_drift_rate()
+        print(f"   Drift rate: {drift_rate:.4f} ms/s")
+        print("   ✓ DriftMonitor OK")
+
+        # Test delay line
+        print("\n2. Testing DelayLineBuffer...")
+        delay_line = DelayLineBuffer(max_delay_samples=4800, num_channels=2)  # 100ms at 48kHz
+
+        # Set 10ms delay
+        delay_line.set_delay_ms(10.0, 48000)
+
+        # Create test signal (impulse)
+        test_signal = np.zeros((480, 2))  # 10ms at 48kHz
+        test_signal[0, :] = 1.0
+
+        # Process
+        output = delay_line.process(test_signal)
+
+        # Check that impulse is delayed
+        input_peak_pos = np.argmax(np.abs(test_signal[:, 0]))
         output_peak_pos = np.argmax(np.abs(output[:, 0]))
 
-        logger.info("   Input peak at sample, input_peak_pos)
-        logger.info("   Output peak at sample, output_peak_pos)
+        print(f"   Input peak at sample: {input_peak_pos}")
+        print(f"   Output peak at sample: {output_peak_pos}")
 
-        # Note)
+        # Note: First block won't show delay (buffer is empty)
         # Process a second block to see the delay
         test_signal2 = np.zeros((480, 2))
         output2 = delay_line.process(test_signal2)
 
         output_peak_pos2 = np.argmax(np.abs(output2[:, 0]))
-        logger.info("   Second block peak at sample, output_peak_pos2)
-        logger.info("   ✓ DelayLineBuffer OK")
+        print(f"   Second block peak at sample: {output_peak_pos2}")
+        print("   ✓ DelayLineBuffer OK")
 
         # Test LatencyManager initialization
-        logger.info("\n3. Testing LatencyManager initialization...")
+        print("\n3. Testing LatencyManager initialization...")
         manager = LatencyManager(sample_rate=48000, buffer_size=512)
 
         assert manager.sample_rate == 48000
         assert manager.buffer_size == 512
         assert not manager.is_calibrated
 
-        logger.info("   ✓ LatencyManager initialization OK")
+        print("   ✓ LatencyManager initialization OK")
 
         # Test timing updates
-        logger.info("\n4. Testing timing updates...")
+        print("\n4. Testing timing updates...")
         current_time = time.time()
         manager.update_timing(current_time)
 
         # Simulate a few callbacks
-        for i in range(10))  # Perfect timing
+        for i in range(10):
+            current_time += (512 / 48000)  # Perfect timing
             manager.update_timing(current_time)
 
         stats = manager.get_statistics()
-        logger.info("   Drift, stats['drift']['current_drift_ms'])
-        logger.info("   ✓ Timing updates OK")
+        print(f"   Drift: {stats['drift']['current_drift_ms']:.4f} ms")
+        print("   ✓ Timing updates OK")
 
         # Test latency frame retrieval
-        logger.info("\n5. Testing latency frame retrieval...")
+        print("\n5. Testing latency frame retrieval...")
         frame = manager.get_current_frame()
         assert frame.sample_rate == 48000
         assert frame.buffer_size_samples == 512
-        logger.info("   Frame, frame)
-        logger.info("   ✓ Latency frame OK")
-
-        # Note)
-        logger.info("   ⚠ Skipping (requires audio loopback hardware)")
-        logger.info("   To test calibration manually)
-        logger.info("     1. Connect audio output → input (cable or virtual)")
-        logger.info("     2. Run)
-        logger.info("     3. Follow calibration prompts")
-
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
-        logger.info("\nNote)
+        print(f"   Frame: {frame}")
+        print("   ✓ Latency frame OK")
+
+        # Note: Skipping actual calibration test as it requires audio hardware
+        print("\n6. Calibration test...")
+        print("   ⚠ Skipping (requires audio loopback hardware)")
+        print("   To test calibration manually:")
+        print("     1. Connect audio output → input (cable or virtual)")
+        print("     2. Run: python latency_manager.py")
+        print("     3. Follow calibration prompts")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        print("\nNote: Full calibration test requires audio hardware")
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    # Run self-test
+    success = _self_test()
 
-    if success)
-        logger.info("Interactive Calibration Test")
-        logger.info("=" * 60)
+    if success:
+        print("\n" + "=" * 60)
+        print("Interactive Calibration Test")
+        print("=" * 60)
 
-        response = input("\nRun interactive calibration test? (requires audio loopback) [y/N])
+        response = input("\nRun interactive calibration test? (requires audio loopback) [y/N]: ")
 
         if response.lower() == 'y':
-            logger.info("\n🔊 IMPORTANT)
-            logger.info("   Options)
-            logger.info("   1. Physical cable)
-            logger.info("   2. Virtual audio cable (VB-Audio Cable, BlackHole, etc.)")
+            print("\n🔊 IMPORTANT: Connect audio output to input before proceeding!")
+            print("   Options:")
+            print("   1. Physical cable: Output jack → Input jack")
+            print("   2. Virtual audio cable (VB-Audio Cable, BlackHole, etc.)")
             input("\nPress Enter when ready...")
 
             manager = LatencyManager()
             success = manager.calibrate()
 
-            if success)
-                logger.info("Calibration Results)
-                logger.info("=" * 60)
+            if success:
+                print("\n" + "=" * 60)
+                print("Calibration Results:")
+                print("=" * 60)
 
                 stats = manager.get_statistics()
-                logger.info("\nCalibrated, stats['calibrated'])
-                logger.info("Total Latency, stats['latency']['total_measured_ms'])
-                logger.info("Effective Latency, stats['effective_latency_ms'])
-                logger.info("Aligned, stats['aligned'])
-                logger.info("Quality, stats['latency']['calibration_quality'])
-
-                logger.info("\nComponent Breakdown)
-                logger.info("  HW Input, stats['latency']['hw_input_latency_ms'])
-                logger.info("  HW Output, stats['latency']['hw_output_latency_ms'])
-                logger.info("  Engine, stats['latency']['engine_latency_ms'])
-                logger.info("  OS, stats['latency']['os_latency_ms'])
-
+                print(f"\nCalibrated: {stats['calibrated']}")
+                print(f"Total Latency: {stats['latency']['total_measured_ms']:.2f} ms")
+                print(f"Effective Latency: {stats['effective_latency_ms']:.2f} ms")
+                print(f"Aligned: {stats['aligned']}")
+                print(f"Quality: {stats['latency']['calibration_quality']:.2f}")
+
+                print("\nComponent Breakdown:")
+                print(f"  HW Input:  {stats['latency']['hw_input_latency_ms']:.2f} ms")
+                print(f"  HW Output: {stats['latency']['hw_output_latency_ms']:.2f} ms")
+                print(f"  Engine:    {stats['latency']['engine_latency_ms']:.2f} ms")
+                print(f"  OS:        {stats['latency']['os_latency_ms']:.2f} ms")
diff --git a/server/main.py b/server/main.py
index dbb20ae413790a68434588c8b896dd15a69ff730..c212446cc51fcd5228005f454a5e0ef61b028236 100644
--- a/server/main.py
+++ b/server/main.py
@@ -1,475 +1,3494 @@
-# ============================================================
-# SOUNDLAB / D-ASE MAIN SERVER MODULE  (Corrected Version)
-# ============================================================
-# ✅ Fixes applied, Any, List, Optional)
-#  - Made WebSocket broadcasting async-safe
-#  - Added initialization checks for audio_server, metrics_streamer, auto_phi_learner
-#  - Replaced bare except blocks with explicit Exception handling
-# ============================================================
-
-
+"""
+Soundlab Main Server - Complete System Integration
 
+Integrates all components:
+- AudioServer (real-time audio processing)
+- PresetAPI (preset management)
+- MetricsAPI (real-time metrics streaming)
+- LatencyAPI (latency diagnostics and compensation)
+- WebSocket streams for metrics and latency
+- Unified FastAPI application
 
+Run with: python main.py
+"""
 
 import asyncio
-from typing import Any, List, Optional
-
-from fastapi import FastAPI, WebSocket, WebSocketDisconnect
-from fastapi.responses import HTMLResponse
-from pydantic import BaseModel
-
-# Import internal components (mock safely if missing)
-try:
-    from server.audio_server import AudioServer
-    from server.metrics_streamer import MetricsStreamer
-    from server.auto_phi_learner import AutoPhiLearner
-except ImportError:
-    print("⚠️ Warning, MetricsStreamer, AutoPhiLearner).")
-
-
-
-# ============================================================
-# APP INITIALIZATION
-# ============================================================
-
-app = FastAPI(title="Soundlab + D-ASE Engine", version="2.0")
-
-class MainApp:
-    and Auto-Φ Learner components together.
+import signal
+import sys
+from typing import Optional
+from fastapi import FastAPI
+from fastapi.middleware.cors import CORSMiddleware
+from fastapi.staticfiles import StaticFiles
+from fastapi.responses import FileResponse
+import uvicorn
+import os
+from pathlib import Path
+
+# Import all components
+from .audio_server import AudioServer
+from .preset_store import PresetStore
+from .ab_snapshot import ABSnapshot
+from .metrics_streamer import MetricsStreamer
+from .preset_api import create_preset_api
+from .latency_api import create_latency_api, LatencyStreamer
+from .auto_phi import AutoPhiLearner, AutoPhiConfig
+from .criticality_balancer import CriticalityBalancer, CriticalityBalancerConfig
+from .state_memory import StateMemory, StateMemoryConfig
+from .state_classifier import StateClassifierGraph, StateClassifierConfig
+from .predictive_model import PredictiveModel, PredictiveModelConfig
+from .session_recorder import SessionRecorder, SessionRecorderConfig
+from .timeline_player import TimelinePlayer, TimelinePlayerConfig
+from .data_exporter import DataExporter, ExportConfig, ExportRequest, ExportFormat
+from .node_sync import NodeSynchronizer, NodeSyncConfig, NodeRole
+from .phasenet_protocol import PhaseNetNode, PhaseNetConfig
+from .cluster_monitor import ClusterMonitor, ClusterMonitorConfig
+from .hw_interface import HardwareInterface
+from .hybrid_bridge import HybridBridge
+from .hybrid_node import HybridNode, HybridNodeConfig, PhiSource, HybridMetrics
+from .session_comparator import SessionComparator, SessionStats, ComparisonResult
+from .correlation_analyzer import CorrelationAnalyzer, CorrelationMatrix
+from .chromatic_visualizer import ChromaticVisualizer, VisualizerConfig
+from .state_sync_manager import StateSyncManager, SyncConfig
+
+
+class SoundlabServer:
+    """
+    Complete Soundlab server with all features integrated
+
+    Features:
+    - Real-time audio processing (48kHz @ 512 samples)
+    - REST APIs (presets, metrics, latency)
+    - WebSocket streaming (metrics @ 30Hz, latency @ 10Hz)
+    - A/B preset comparison
+    - Latency compensation with calibration
+    - Comprehensive logging
     """
 
-     def __init__(self):
-        """Safely initialize submodules with fallback warnings."""
-        self.audio_server = None
-        self.metrics_streamer = None
-        self.auto_phi_learner = None
-
-        # Audio Server initialization
-        try:
-            from server.audio_server import AudioServer
-            self.audio_server = AudioServer()
-        except Exception as e:
-            print(f"⚠️ AudioServer init failed: {e}")
-            self.audio_server = None
-
-        # Metrics Streamer initialization
-        try:
-            from server.metrics_streamer import MetricsStreamer
-            self.metrics_streamer = MetricsStreamer()
-        except Exception as e:
-            print(f"⚠️ MetricsStreamer init failed: {e}")
-            self.metrics_streamer = None
-
-        # Auto-Φ Learner initialization
-        try:
-            from server.auto_phi import AutoPhiLearner
-            self.auto_phi_learner = AutoPhiLearner()
-        except Exception as e:
-            print(f"⚠️ AutoPhiLearner init failed: {e}")
-            self.auto_phi_learner = None
-
-# ============================================================
-# CALLBACK FUNCTIONS
-# ============================================================
-
-
-
-
-
-    # --------------------------------------------------------
-    # 1. Auto-Φ Update Callback
-    # --------------------------------------------------------
-    def auto_phi_update_callback(self, param_name, value) :
-        """Callback for Auto-Φ Learner to update live parameters."""
-        if not self.audio_server:
-            print("⚠️ Warning)
-            return
-
-        try,
+    def __init__(self,
+                 host: str = "0.0.0.0",
+                 port: int = 8000,
+                 audio_input_device: Optional[int] = None,
+                 audio_output_device: Optional[int] = None,
+                 enable_logging: bool = True,
+                 enable_cors: bool = True,
+                 enable_auto_phi: bool = False,
+                 enable_criticality_balancer: bool = False,
+                 enable_state_memory: bool = False,
+                 enable_state_classifier: bool = False,
+                 enable_predictive_model: bool = False,
+                 enable_session_recorder: bool = True,
+                 enable_timeline_player: bool = True,
+                 enable_data_exporter: bool = True,
+                 enable_node_sync: bool = False,
+                 node_sync_role: str = "master",
+                 node_sync_master_url: Optional[str] = None,
+                 enable_phasenet: bool = False,
+                 phasenet_port: int = 9000,
+                 phasenet_key: Optional[str] = None,
+                 enable_cluster_monitor: bool = False,
+                 enable_hardware_bridge: bool = False,
+                 hardware_port: Optional[str] = None,
+                 hardware_baudrate: int = 115200,
+                 enable_hybrid_bridge: bool = False,
+                 hybrid_port: Optional[str] = None,
+                 hybrid_baudrate: int = 115200,
+                 enable_hybrid_node: bool = False,
+                 hybrid_node_input_device: Optional[int] = None,
+                 hybrid_node_output_device: Optional[int] = None):
+        """
+        Initialize Soundlab server
+
+        Args:
+            host: Server host address
+            port: Server port
+            audio_input_device: Audio input device index (None = default)
+            audio_output_device: Audio output device index (None = default)
+            enable_logging: Enable metrics/latency logging
+            enable_cors: Enable CORS for web clients
+        """
+        print("=" * 60)
+        print("SOUNDLAB SERVER")
+        print("Real-Time Audio Processing & Consciousness Telemetry")
+        print("=" * 60)
+
+        self.host = host
+        self.port = port
+        self.enable_logging = enable_logging
+
+        # Initialize audio server
+        print("\n[Main] Initializing audio server...")
+        self.audio_server = AudioServer(
+            input_device=audio_input_device,
+            output_device=audio_output_device,
+            enable_logging=enable_logging
+        )
+
+        # Initialize preset management
+        print("\n[Main] Initializing preset store...")
+        self.preset_store = PresetStore()
+        self.ab_snapshot = ABSnapshot()
+
+        # Initialize metrics streamer
+        print("\n[Main] Initializing metrics streamer...")
+        self.metrics_streamer = MetricsStreamer()
+
+        # Initialize Auto-Φ Learner (Feature 011)
+        print("\n[Main] Initializing Auto-Φ Learner...")
+        auto_phi_config = AutoPhiConfig(
+            enabled=enable_auto_phi,
+            k_depth=0.25,
+            gamma_phase=0.1,
+            enable_logging=enable_logging
+        )
+        self.auto_phi_learner = AutoPhiLearner(auto_phi_config)
+
+        # Wire audio server metrics to streamer and auto-phi learner
+        def metrics_callback(frame):
+            # Send to metrics streamer
+            asyncio.run(self.metrics_streamer.enqueue_frame(frame))
+            # Send to auto-phi learner
+            self.auto_phi_learner.process_metrics(frame)
+
+        self.audio_server.metrics_callback = metrics_callback
+
+        # Wire Auto-Φ Learner parameter updates to audio server (FR-005)
+        def auto_phi_update_callback(param_name: str, value: float):
+            """Callback for Auto-Φ Learner to update parameters"""
+            self.audio_server.update_parameter(
+                param_type='phi',
                 channel=None,
-                param_name=str(param_name).replace('phi_', ''),  # Remove 'phi_' prefix if present
+                param_name=param_name.replace('phi_', ''),  # Remove 'phi_' prefix
                 value=value
+            )
+
+        self.auto_phi_learner.update_callback = auto_phi_update_callback
+
+        # Initialize Criticality Balancer (Feature 012)
+        print("\n[Main] Initializing Criticality Balancer...")
+        criticality_balancer_config = CriticalityBalancerConfig(
+            enabled=enable_criticality_balancer,
+            beta_coupling=0.1,
+            delta_amplitude=0.05,
+            enable_logging=enable_logging
+        )
+        self.criticality_balancer = CriticalityBalancer(criticality_balancer_config)
+
+        # Wire Criticality Balancer to metrics stream
+        def metrics_callback_extended(frame):
+            # Send to metrics streamer
+            asyncio.run(self.metrics_streamer.enqueue_frame(frame))
+            # Send to auto-phi learner
+            self.auto_phi_learner.process_metrics(frame)
+            # Send to criticality balancer
+            self.criticality_balancer.process_metrics(frame)
+
+        self.audio_server.metrics_callback = metrics_callback_extended
+
+        # Wire Criticality Balancer batch updates to audio server (FR-006)
+        def criticality_balancer_update_callback(update_data):
+            """Callback for Criticality Balancer batch updates"""
+            if update_data.get('type') == 'update_coupling':
+                # Apply coupling matrix
+                coupling_matrix = update_data.get('coupling_matrix')
+                amplitudes = update_data.get('amplitudes')
+
+                # Update coupling in audio server (if supported)
+                # For now, just update amplitudes
+                if amplitudes:
+                    for channel_idx, amplitude in enumerate(amplitudes):
+                        self.audio_server.update_parameter(
+                            param_type='channel',
+                            channel=channel_idx,
+                            param_name='amplitude',
+                            value=amplitude
+                        )
+
+        self.criticality_balancer.update_callback = criticality_balancer_update_callback
+
+        # Initialize State Memory (Feature 013)
+        print("\n[Main] Initializing State Memory...")
+        state_memory_config = StateMemoryConfig(
+            enabled=enable_state_memory,
+            buffer_size=256,
+            trend_window=30,
+            enable_logging=enable_logging
+        )
+        self.state_memory = StateMemory(state_memory_config)
+
+        # Wire State Memory to metrics stream and Auto-Phi Learner
+        def metrics_callback_with_memory(frame):
+            # Send to metrics streamer
+            asyncio.run(self.metrics_streamer.enqueue_frame(frame))
+            # Send to auto-phi learner
+            self.auto_phi_learner.process_metrics(frame)
+            # Send to criticality balancer
+            self.criticality_balancer.process_metrics(frame)
+            # Send to state memory (FR-002)
+            if self.state_memory.config.enabled:
+                criticality = getattr(frame, 'criticality', 1.0)
+                coherence = getattr(frame, 'phase_coherence', 0.0)
+                ici = getattr(frame, 'ici', 0.0)
+                phi_depth = self.auto_phi_learner.state.phi_depth
+                phi_phase = self.auto_phi_learner.state.phi_phase
+                self.state_memory.add_frame(criticality, coherence, ici, phi_depth, phi_phase)
+
+        self.audio_server.metrics_callback = metrics_callback_with_memory
+
+        # Wire State Memory bias to Auto-Phi Learner (FR-004)
+        def state_memory_bias_callback(bias: float):
+            """Feed predictive bias from State Memory to Auto-Phi Learner"""
+            self.auto_phi_learner.external_bias = bias
 
-        except Exception as e:
-            print(f"❌ Error updating parameter {param_name})
+        self.state_memory.bias_callback = state_memory_bias_callback
+
+        # Initialize State Classifier (Feature 015)
+        print("\n[Main] Initializing State Classifier...")
+        state_classifier_config = StateClassifierConfig(
+            hysteresis_threshold=0.1,
+            min_state_duration=0.5,
+            enable_logging=enable_logging
+        )
+        self.state_classifier = StateClassifierGraph(state_classifier_config)
+
+        # Wire State Classifier to metrics stream
+        def metrics_callback_with_classifier(frame):
+            # Send to metrics streamer
+            asyncio.run(self.metrics_streamer.enqueue_frame(frame))
+            # Send to auto-phi learner
+            self.auto_phi_learner.process_metrics(frame)
+            # Send to criticality balancer
+            self.criticality_balancer.process_metrics(frame)
+            # Send to state memory
+            if self.state_memory.config.enabled:
+                criticality = getattr(frame, 'criticality', 1.0)
+                coherence = getattr(frame, 'phase_coherence', 0.0)
+                ici = getattr(frame, 'ici', 0.0)
+                phi_depth = self.auto_phi_learner.state.phi_depth
+                phi_phase = self.auto_phi_learner.state.phi_phase
+                self.state_memory.add_frame(criticality, coherence, ici, phi_depth, phi_phase)
+            # Send to state classifier
+            if self.state_classifier:
+                ici = getattr(frame, 'ici', 0.0)
+                coherence = getattr(frame, 'phase_coherence', 0.0)
+                centroid = getattr(frame, 'spectral_centroid', 0.0)
+                self.state_classifier.classify_state(ici, coherence, centroid)
+
+        self.audio_server.metrics_callback = metrics_callback_with_classifier
+
+        # Wire State Classifier state changes to WebSocket broadcast
+        def state_change_callback(event):
+            """Broadcast state changes via WebSocket (FR-005)"""
+            asyncio.run(self.metrics_streamer.broadcast_event(event))
+
+        self.state_classifier.state_change_callback = state_change_callback
+
+        # Initialize Predictive Model (Feature 016)
+        print("\n[Main] Initializing Predictive Model...")
+        predictive_model_config = PredictiveModelConfig(
+            buffer_size=128,
+            prediction_horizon=1.5,
+            min_buffer_size=50,
+            enable_logging=enable_logging
+        )
+        self.predictive_model = PredictiveModel(predictive_model_config) if enable_predictive_model else None
+
+        # Wire Predictive Model to State Memory and State Classifier
+        if self.predictive_model:
+            # Update metrics callback to feed Predictive Model
+            def metrics_callback_with_predictor(frame):
+                # Send to metrics streamer
+                asyncio.run(self.metrics_streamer.enqueue_frame(frame))
+                # Send to auto-phi learner
+                self.auto_phi_learner.process_metrics(frame)
+                # Send to criticality balancer
+                self.criticality_balancer.process_metrics(frame)
+                # Send to state memory
+                if self.state_memory.config.enabled:
+                    criticality = getattr(frame, 'criticality', 1.0)
+                    coherence = getattr(frame, 'phase_coherence', 0.0)
+                    ici = getattr(frame, 'ici', 0.0)
+                    phi_depth = self.auto_phi_learner.state.phi_depth
+                    phi_phase = self.auto_phi_learner.state.phi_phase
+                    self.state_memory.add_frame(criticality, coherence, ici, phi_depth, phi_phase)
+                # Send to state classifier
+                if self.state_classifier:
+                    ici = getattr(frame, 'ici', 0.0)
+                    coherence = getattr(frame, 'phase_coherence', 0.0)
+                    centroid = getattr(frame, 'spectral_centroid', 0.0)
+                    self.state_classifier.classify_state(ici, coherence, centroid)
+                # Send to predictive model (FR-002)
+                if self.predictive_model:
+                    ici = getattr(frame, 'ici', 0.0)
+                    coherence = getattr(frame, 'phase_coherence', 0.0)
+                    criticality = getattr(frame, 'criticality', 1.0)
+                    current_state = self.state_classifier.current_state.value if self.state_classifier else "AWAKE"
+                    import time
+                    self.predictive_model.add_frame(ici, coherence, criticality, current_state, time.time())
+                # Send to PhaseNet (FR-004)
+                if self.phasenet and self.phasenet.is_running:
+                    phi_phase = self.auto_phi_learner.state.phi_phase
+                    phi_depth = self.auto_phi_learner.state.phi_depth
+                    criticality = getattr(frame, 'criticality', 1.0)
+                    coherence = getattr(frame, 'phase_coherence', 0.0)
+                    ici = getattr(frame, 'ici', 0.0)
+                    self.phasenet.update_phase(phi_phase, phi_depth, criticality, coherence, ici)
+                # Send to Chromatic Visualizer (Feature 016)
+                if self.chromatic_visualizer:
+                    # Extract channel spectral data
+                    spectral = getattr(frame, 'spectral_analysis', {})
+                    channel_centroids = spectral.get('channel_centroids', [100, 200, 300, 400, 500, 600, 700, 800])
+                    channel_rms = spectral.get('channel_rms', [0.5] * 8)
+
+                    # Get Phi and metrics
+                    phi_phase = self.auto_phi_learner.state.phi_phase
+                    phi_depth = self.auto_phi_learner.state.phi_depth
+                    ici = getattr(frame, 'ici', 0.5)
+                    coherence = getattr(frame, 'phase_coherence', 0.5)
+                    criticality = getattr(frame, 'criticality', 1.0)
+
+                    # Update chromatic state
+                    self.chromatic_visualizer.update_state(
+                        channel_frequencies=channel_centroids[:8],
+                        channel_amplitudes=channel_rms[:8],
+                        phi_phase=phi_phase,
+                        phi_depth=phi_depth,
+                        ici=ici,
+                        coherence=coherence,
+                        criticality=criticality
+                    )
+
+                # Update StateSyncManager (Feature 017)
+                if self.state_sync_manager:
+                    phi_phase = self.auto_phi_learner.state.phi_phase
+                    phi_depth = self.auto_phi_learner.state.phi_depth
+                    ici = getattr(frame, 'ici', 0.5)
+                    coherence = getattr(frame, 'phase_coherence', 0.5)
+                    criticality = getattr(frame, 'criticality', 1.0)
+
+                    # Get phi breathing from chromatic visualizer if available
+                    phi_breathing = 0.5
+                    if self.chromatic_visualizer:
+                        chrom_state = self.chromatic_visualizer.get_current_state()
+                        if chrom_state:
+                            phi_breathing = chrom_state.get('phi_breathing', 0.5)
+
+                    # Update synchronized state
+                    self.state_sync_manager.update_state(
+                        ici=ici,
+                        coherence=coherence,
+                        criticality=criticality,
+                        phi_phase=phi_phase,
+                        phi_depth=phi_depth,
+                        phi_breathing=phi_breathing,
+                        chromatic_enabled=True,
+                        control_matrix_active=True,
+                        adaptive_enabled=self.auto_phi_learner.enabled,
+                        adaptive_mode=self.auto_phi_learner.mode if self.auto_phi_learner.enabled else None,
+                        is_recording=self.session_recorder.is_recording if self.session_recorder else False,
+                        is_playing=self.timeline_player.is_playing if self.timeline_player else False,
+                        cluster_nodes_count=len(self.phasenet.peers) if self.phasenet else 0
+                    )
+
+                    # Broadcast to dashboard clients
+                    asyncio.run(self.state_sync_manager.broadcast_state())
+
+            self.audio_server.metrics_callback = metrics_callback_with_predictor
+
+            # Wire forecast callback to Auto-Phi and Criticality Balancer (FR-006)
+            def forecast_callback(forecast_json):
+                """Handle forecast for preemptive adjustments"""
+                # Broadcast forecast via WebSocket
+                asyncio.run(self.metrics_streamer.broadcast_event(forecast_json))
+
+                # Preemptive adjustment based on predicted criticality (SC-003)
+                predicted_criticality = forecast_json['predicted_metrics']['criticality']
+                confidence = forecast_json['confidence']
+
+                # Only act on high-confidence predictions
+                if confidence > 0.7:
+                    # If predicted criticality > 1.05, reduce phi_depth proactively
+                    if predicted_criticality > 1.05:
+                        # Calculate preemptive bias (similar to State Memory)
+                        overshoot_expected = predicted_criticality - 1.0
+                        preemptive_bias = -0.3 * overshoot_expected * confidence
+
+                        # Apply via Auto-Phi Learner external bias
+                        self.auto_phi_learner.external_bias = preemptive_bias
+
+                        if self.enable_logging:
+                            print(f"[PredictiveModel] Preemptive adjustment: bias={preemptive_bias:.3f}, "
+                                  f"predicted_crit={predicted_criticality:.2f}, conf={confidence:.2f}")
+
+                    # If predicted criticality < 0.95, increase phi_depth proactively
+                    elif predicted_criticality < 0.95:
+                        undershoot_expected = 1.0 - predicted_criticality
+                        preemptive_bias = 0.3 * undershoot_expected * confidence
+
+                        self.auto_phi_learner.external_bias = preemptive_bias
+
+                        if self.enable_logging:
+                            print(f"[PredictiveModel] Preemptive adjustment: bias={preemptive_bias:.3f}, "
+                                  f"predicted_crit={predicted_criticality:.2f}, conf={confidence:.2f}")
+
+            self.predictive_model.forecast_callback = forecast_callback
+
+        # Initialize Session Recorder (Feature 017)
+        if enable_session_recorder:
+            print("\n[Main] Initializing Session Recorder...")
+            session_recorder_config = SessionRecorderConfig(
+                sessions_dir="sessions",
+                sample_rate=self.audio_server.SAMPLE_RATE,
+                enable_logging=enable_logging
+            )
+            self.session_recorder = SessionRecorder(session_recorder_config)
+        else:
+            self.session_recorder = None
+
+        # Initialize Timeline Player (Feature 018)
+        if enable_timeline_player:
+            print("\n[Main] Initializing Timeline Player...")
+            timeline_player_config = TimelinePlayerConfig(
+                update_rate=30,
+                enable_logging=enable_logging
+            )
+            self.timeline_player = TimelinePlayer(timeline_player_config)
+        else:
+            self.timeline_player = None
+
+        # Initialize Data Exporter (Feature 019)
+        if enable_data_exporter:
+            print("\n[Main] Initializing Data Exporter...")
+            data_exporter_config = ExportConfig(
+                output_dir="exports",
+                enable_compression=True,
+                enable_checksum=True,
+                enable_logging=enable_logging
+            )
+            self.data_exporter = DataExporter(data_exporter_config)
+        else:
+            self.data_exporter = None
+
+        # Initialize Node Synchronizer (Feature 020)
+        if enable_node_sync:
+            print("\n[Main] Initializing Node Synchronizer...")
+            node_role = NodeRole.MASTER if node_sync_role.lower() == "master" else NodeRole.CLIENT
+            node_sync_config = NodeSyncConfig(
+                role=node_role,
+                master_url=node_sync_master_url,
+                sync_rate=30,
+                enable_logging=enable_logging
+            )
+            self.node_sync = NodeSynchronizer(node_sync_config)
+        else:
+            self.node_sync = None
+
+        # Initialize PhaseNet Protocol (Feature 021)
+        if enable_phasenet:
+            print("\n[Main] Initializing PhaseNet Protocol...")
+            phasenet_config = PhaseNetConfig(
+                bind_port=phasenet_port,
+                network_key=phasenet_key,
+                enable_encryption=phasenet_key is not None,
+                enable_logging=enable_logging
+            )
+            self.phasenet = PhaseNetNode(phasenet_config)
+        else:
+            self.phasenet = None
+
+        # Initialize Cluster Monitor (Feature 022)
+        if enable_cluster_monitor:
+            print("\n[Main] Initializing Cluster Monitor...")
+            cluster_monitor_config = ClusterMonitorConfig(
+                update_interval=1.0,
+                history_samples=600,
+                enable_rbac=True,
+                enable_logging=enable_logging
+            )
+            self.cluster_monitor = ClusterMonitor(cluster_monitor_config)
+            # Wire references to node_sync and phasenet (FR-001, FR-002)
+            self.cluster_monitor.node_sync = self.node_sync
+            self.cluster_monitor.phasenet = self.phasenet
+        else:
+            self.cluster_monitor = None
+
+        # Initialize Hardware Interface (Feature 023)
+        if enable_hardware_bridge:
+            print("\n[Main] Initializing Hardware I²S Bridge...")
+            self.hw_interface = HardwareInterface(
+                port=hardware_port,
+                baudrate=hardware_baudrate,
+                enable_logging=enable_logging
+            )
+            # Wire to cluster monitor (FR-008)
+            if self.cluster_monitor:
+                self.cluster_monitor.hw_interface = self.hw_interface
+        else:
+            self.hw_interface = None
+
+        # Initialize Hybrid Analog-DSP Bridge (Feature 024)
+        if enable_hybrid_bridge:
+            print("\n[Main] Initializing Hybrid Analog-DSP Bridge...")
+            self.hybrid_bridge = HybridBridge(
+                port=hybrid_port,
+                baudrate=hybrid_baudrate
+            )
+            # Wire to cluster monitor (FR-009)
+            if self.cluster_monitor:
+                self.cluster_monitor.hybrid_bridge = self.hybrid_bridge
+        else:
+            self.hybrid_bridge = None
+
+        # Initialize Hybrid Node Integration (Feature 025)
+        if enable_hybrid_node:
+            print("\n[Main] Initializing Hybrid Node (Analog-Digital Bridge)...")
+            hybrid_config = HybridNodeConfig(
+                input_device=hybrid_node_input_device,
+                output_device=hybrid_node_output_device,
+                phi_source=PhiSource.INTERNAL,
+                enable_logging=enable_logging
+            )
+            self.hybrid_node = HybridNode(hybrid_config)
+
+            # Register metrics callback to stream to metrics_streamer
+            def hybrid_metrics_callback(metrics: HybridMetrics):
+                # Convert HybridMetrics to metrics frame format for streaming
+                asyncio.run(self.metrics_streamer.broadcast_event({
+                    'type': 'hybrid_metrics',
+                    'timestamp': metrics.timestamp,
+                    'ici': metrics.ici,
+                    'phase_coherence': metrics.phase_coherence,
+                    'spectral_centroid': metrics.spectral_centroid,
+                    'consciousness_level': metrics.consciousness_level,
+                    'phi_phase': metrics.phi_phase,
+                    'phi_depth': metrics.phi_depth,
+                    'cpu_load': metrics.cpu_load,
+                    'latency_ms': metrics.latency_ms
+                }))
+
+            self.hybrid_node.register_metrics_callback(hybrid_metrics_callback)
+        else:
+            self.hybrid_node = None
+
+        # Initialize Analytics Components (Feature 015)
+        print("\n[Main] Initializing Multi-Session Analytics...")
+        self.session_comparator = SessionComparator()
+        self.correlation_analyzer = CorrelationAnalyzer()
+
+        # Initialize Chromatic Visualizer (Feature 016)
+        print("\n[Main] Initializing Chromatic Consciousness Visualizer...")
+        visualizer_config = VisualizerConfig(
+            num_channels=8,
+            target_fps=60,
+            enable_logging=enable_logging
+        )
+        self.chromatic_visualizer = ChromaticVisualizer(visualizer_config)
+
+        # Initialize State Sync Manager (Feature 017)
+        print("\n[Main] Initializing State Sync Manager...")
+        sync_config = SyncConfig(
+            max_latency_ms=100.0,
+            max_desync_ms=100.0,
+            websocket_timeout_ms=50.0,
+            enable_logging=enable_logging
+        )
+        self.state_sync_manager = StateSyncManager(sync_config)
+        self.state_sync_manager.start_monitoring()
+
+        # Initialize latency streamer (will be created by latency API)
+        self.latency_streamer: Optional[LatencyStreamer] = None
+
+        # Create FastAPI application
+        print("\n[Main] Creating FastAPI application...")
+        self.app = FastAPI(
+            title="Soundlab API",
+            version="1.0.0",
+            description="Real-time audio processing with Φ-modulation and consciousness metrics"
+        )
+
+        # Enable CORS if requested
+        if enable_cors:
+            self.app.add_middleware(
+                CORSMiddleware,
+                allow_origins=["*"],  # Configure appropriately for production
+                allow_credentials=True,
+                allow_methods=["*"],
+                allow_headers=["*"],
+            )
+
+        # Mount sub-applications
+        self._mount_apis()
+
+        # Add root endpoints
+        self._add_root_endpoints()
+
+        # Serve static files (frontend)
+        self._mount_static_files()
+
+        # Shutdown handler
+        self.is_shutting_down = False
+
+        print("\n[Main] ✓ Server initialization complete")
+
+    def _mount_apis(self):
+        """Mount all API sub-applications"""
+
+        # Preset API
+        preset_app = create_preset_api(self.preset_store, self.ab_snapshot)
+        self.app.mount("/", preset_app)
+
+        # Latency API
+        latency_app = create_latency_api(self.audio_server.latency_manager)
+        self.app.mount("/", latency_app)
+
+        # Get reference to latency streamer
+        from latency_api import latency_streamer
+        self.latency_streamer = latency_streamer
+
+        # Wire audio server latency to streamer
+        if self.latency_streamer:
+            self.audio_server.latency_callback = lambda frame: asyncio.run(
+                self.latency_streamer.broadcast_frame(frame)
+            )
+
+    def _add_root_endpoints(self):
+        """Add root-level endpoints"""
+
+        @self.app.get("/")
+        async def root():
+            """Serve main HTML page"""
+            frontend_path = Path(__file__).parent.parent / "soundlab_v2.html"
+
+            if frontend_path.exists():
+                return FileResponse(frontend_path)
+            else:
+                return {
+                    "message": "Soundlab API Server",
+                    "version": "1.0.0",
+                    "status": "running",
+                    "docs": "/docs",
+                    "audio_running": self.audio_server.is_running
+                }
+
+        # Health check endpoints (Feature 019: FR-003)
+        @self.app.get("/healthz")
+        async def healthz():
+            """
+            Health check endpoint - basic liveness check
+
+            Returns 200 if server is alive and responding
+            """
+            return {
+                "status": "healthy",
+                "service": "soundlab-phi-matrix",
+                "version": getattr(self, 'version', '0.9.0-rc1')
+            }
 
+        @self.app.get("/readyz")
+        async def readyz():
+            """
+            Readiness check endpoint - checks if server is ready to serve traffic
+
+            Returns:
+                200 if ready, 503 if not ready
+            """
+            import psutil
+
+            # Check critical components
+            checks = {
+                "audio_server": self.audio_server is not None,
+                "metrics_streamer": self.metrics_streamer is not None,
+                "state_sync_manager": hasattr(self, 'state_sync_manager') and self.state_sync_manager is not None,
+                "adaptive_enabled": hasattr(self.auto_phi_learner, 'enabled'),
+                "cpu_available": psutil.cpu_percent() < 95,
+                "memory_available": psutil.virtual_memory().percent < 95
+            }
 
+            ready = all(checks.values())
+
+            if not ready:
+                from fastapi import Response
+                return Response(
+                    content=json.dumps({
+                        "status": "not_ready",
+                        "checks": checks
+                    }),
+                    status_code=503,
+                    media_type="application/json"
+                )
+
+            return {
+                "status": "ready",
+                "checks": checks
+            }
 
+        @self.app.get("/metrics")
+        async def prometheus_metrics():
+            """
+            Prometheus-compatible metrics endpoint
+
+            Returns metrics in Prometheus text format
+            """
+            import time
+
+            metrics = []
+
+            # Audio server metrics
+            if self.audio_server:
+                metrics.append(f'soundlab_audio_running {{}} {1 if self.audio_server.is_running else 0}')
+                metrics.append(f'soundlab_callback_count {{}} {self.audio_server.callback_count}')
+                metrics.append(f'soundlab_sample_rate {{}} {self.audio_server.SAMPLE_RATE}')
+                metrics.append(f'soundlab_buffer_size {{}} {self.audio_server.BUFFER_SIZE}')
+
+            # Client connections
+            metrics.append(f'soundlab_metrics_clients {{}} {len(self.metrics_streamer.clients)}')
+            if self.latency_streamer:
+                metrics.append(f'soundlab_latency_clients {{}} {len(self.latency_streamer.clients)}')
+
+            # State sync metrics
+            if hasattr(self, 'state_sync_manager') and self.state_sync_manager:
+                metrics.append(f'soundlab_websocket_clients {{}} {len(self.state_sync_manager.ws_clients)}')
+
+                # Latency stats
+                latency_stats = self.state_sync_manager.get_latency_stats()
+                metrics.append(f'soundlab_websocket_latency_avg_ms {{}} {latency_stats.get("avg_latency_ms", 0)}')
+                metrics.append(f'soundlab_websocket_latency_max_ms {{}} {latency_stats.get("max_latency_ms", 0)}')
+
+            # System metrics
+            import psutil
+            metrics.append(f'soundlab_cpu_percent {{}} {psutil.cpu_percent()}')
+            metrics.append(f'soundlab_memory_percent {{}} {psutil.virtual_memory().percent}')
+
+            # Phi metrics
+            if self.auto_phi_learner:
+                metrics.append(f'soundlab_phi_depth {{}} {self.auto_phi_learner.state.phi_depth}')
+                metrics.append(f'soundlab_phi_phase {{}} {self.auto_phi_learner.state.phi_phase}')
+
+            return Response(content='\n'.join(metrics) + '\n', media_type='text/plain')
+
+        @self.app.get("/version")
+        async def version():
+            """
+            Version information endpoint
+
+            Returns version, commit, and build information
+            """
+            import os
+            from pathlib import Path
+
+            version_file = Path(__file__).parent.parent / "version.txt"
+            version_info = {
+                "version": getattr(self, 'version', '0.9.0-rc1'),
+                "commit": "unknown",
+                "build_date": "unknown"
+            }
 
+            if version_file.exists():
+                with open(version_file, 'r') as f:
+                    for line in f:
+                        if '=' in line:
+                            key, value = line.strip().split('=', 1)
+                            version_info[key.lower()] = value
+
+            return version_info
+
+        @self.app.get("/api/status")
+        async def get_status():
+            """Get server status"""
+            return {
+                "audio_running": self.audio_server.is_running,
+                "sample_rate": self.audio_server.SAMPLE_RATE,
+                "buffer_size": self.audio_server.BUFFER_SIZE,
+                "callback_count": self.audio_server.callback_count,
+                "latency_calibrated": self.audio_server.latency_manager.is_calibrated,
+                "preset_loaded": self.audio_server.current_preset is not None,
+                "metrics_clients": len(self.metrics_streamer.clients),
+                "latency_clients": len(self.latency_streamer.clients) if self.latency_streamer else 0
+            }
 
-    # --------------------------------------------------------
-    # 2. State Memory Bias Callback
-    # --------------------------------------------------------
-    def state_memory_bias_callback(self, bias) :
-        """Feed predictive bias from State Memory to Auto-Φ Learner."""
-        if not self.auto_phi_learner:
-            print("⚠️ Warning)
-            return
+        @self.app.post("/api/audio/start")
+        async def start_audio(calibrate: bool = False):
+            """Start audio processing"""
+            if self.audio_server.is_running:
+                return {"ok": False, "message": "Audio already running"}
 
-        try:
-            self.auto_phi_learner.external_bias = bias
-        except Exception as e:
-            print(f"❌ Error applying bias)
+            success = self.audio_server.start(calibrate_latency=calibrate)
 
+            return {
+                "ok": success,
+                "message": "Audio started" if success else "Failed to start audio"
+            }
 
+        @self.app.post("/api/audio/stop")
+        async def stop_audio():
+            """Stop audio processing"""
+            if not self.audio_server.is_running:
+                return {"ok": False, "message": "Audio not running"}
+
+            self.audio_server.stop()
+
+            return {"ok": True, "message": "Audio stopped"}
+
+        @self.app.get("/api/audio/performance")
+        async def get_performance():
+            """Get audio processing performance metrics"""
+            import numpy as np
+
+            if not self.audio_server.processing_time_history:
+                return {
+                    "message": "No performance data available",
+                    "callback_count": self.audio_server.callback_count
+                }
+
+            history = self.audio_server.processing_time_history
+            buffer_duration_ms = (self.audio_server.BUFFER_SIZE / self.audio_server.SAMPLE_RATE) * 1000.0
+
+            return {
+                "callback_count": self.audio_server.callback_count,
+                "buffer_duration_ms": buffer_duration_ms,
+                "processing_time_ms": {
+                    "current": history[-1] if history else 0,
+                    "average": float(np.mean(history)),
+                    "min": float(np.min(history)),
+                    "max": float(np.max(history)),
+                    "std": float(np.std(history))
+                },
+                "cpu_load": {
+                    "current": history[-1] / buffer_duration_ms if history else 0,
+                    "average": float(np.mean(history)) / buffer_duration_ms,
+                    "peak": float(np.max(history)) / buffer_duration_ms if history else 0
+                }
+            }
 
+        @self.app.post("/api/preset/apply")
+        async def apply_preset_from_api(preset_data: dict):
+            """Apply preset to audio server"""
+            try:
+                self.audio_server.apply_preset(preset_data)
+                return {"ok": True, "message": "Preset applied"}
+            except Exception as e:
+                return {"ok": False, "message": str(e)}
+
+        # Auto-Φ Learner API endpoints (Feature 011)
+        @self.app.get("/api/auto-phi/status")
+        async def get_auto_phi_status():
+            """Get Auto-Φ Learner status"""
+            return {
+                "enabled": self.auto_phi_learner.config.enabled,
+                "phi_depth": self.auto_phi_learner.state.phi_depth,
+                "phi_phase": self.auto_phi_learner.state.phi_phase,
+                "criticality": self.auto_phi_learner.state.criticality,
+                "coherence": self.auto_phi_learner.state.coherence,
+                "settled": self.auto_phi_learner.state.settled
+            }
 
+        @self.app.post("/api/auto-phi/enable")
+        async def set_auto_phi_enabled(enabled: bool):
+            """Enable or disable Auto-Φ Learner (FR-006, SC-004)"""
+            self.auto_phi_learner.set_enabled(enabled)
+            return {
+                "ok": True,
+                "enabled": self.auto_phi_learner.config.enabled,
+                "message": f"Auto-Φ Learner {'enabled' if enabled else 'disabled'}"
+            }
 
-    # --------------------------------------------------------
-    # 3. Hybrid Metrics Callback (Async Safe)
-    # --------------------------------------------------------
-    async def hybrid_metrics_callback(self, metrics) :
-        """Stream HybridMetrics frames to WebSocket clients."""
-        if not self.metrics_streamer:
-            print("⚠️ Warning)
-            return
+        @self.app.get("/api/auto-phi/stats")
+        async def get_auto_phi_stats():
+            """Get Auto-Φ Learner performance statistics (FR-007)"""
+            return self.auto_phi_learner.get_statistics()
+
+        @self.app.post("/api/auto-phi/reset")
+        async def reset_auto_phi_stats():
+            """Reset Auto-Φ Learner statistics"""
+            self.auto_phi_learner.reset_statistics()
+            return {"ok": True, "message": "Statistics reset"}
+
+        @self.app.get("/api/auto-phi/logs")
+        async def export_auto_phi_logs():
+            """Export Auto-Φ Learner performance logs"""
+            return self.auto_phi_learner.export_logs()
+
+        # Criticality Balancer API endpoints (Feature 012)
+        @self.app.get("/api/criticality-balancer/status")
+        async def get_criticality_balancer_status():
+            """Get Criticality Balancer status"""
+            return self.criticality_balancer.get_current_state()
+
+        @self.app.post("/api/criticality-balancer/enable")
+        async def set_criticality_balancer_enabled(enabled: bool):
+            """Enable or disable Criticality Balancer (FR-007, SC-004)"""
+            self.criticality_balancer.set_enabled(enabled)
+            return {
+                "ok": True,
+                "enabled": self.criticality_balancer.config.enabled,
+                "message": f"Criticality Balancer {'enabled' if enabled else 'disabled'}"
+            }
 
-        try:
-            await self.metrics_streamer.broadcast_event({
-                'type',
-                'timestamp', 'timestamp', None),
-                'ici', 'ici', None),
-                'phase_coherence', 'phase_coherence', None),
-                'spectral_centroid', 'spectral_centroid', None),
-                'consciousness_level', 'consciousness_level', None),
-                'phi_phase', 'phi_phase', None),
-                'phi_depth', 'phi_depth', None),
-                'cpu_load', 'cpu_load', None),
-                'latency_ms', 'latency_ms', None)
-            })
-        except Exception as e:
-            print(f"❌ Error broadcasting metrics)
-# ============================================================
-# ROUTES AND WEBSOCKET ENDPOINTS
-# ============================================================
+        @self.app.get("/api/criticality-balancer/stats")
+        async def get_criticality_balancer_stats():
+            """Get Criticality Balancer performance statistics"""
+            return self.criticality_balancer.get_statistics()
+
+        @self.app.post("/api/criticality-balancer/reset")
+        async def reset_criticality_balancer_stats():
+            """Reset Criticality Balancer statistics"""
+            self.criticality_balancer.reset_statistics()
+            return {"ok": True, "message": "Statistics reset"}
+
+        @self.app.get("/api/criticality-balancer/logs")
+        async def export_criticality_balancer_logs():
+            """Export Criticality Balancer performance logs"""
+            return self.criticality_balancer.export_logs()
+
+        # State Memory API endpoints (Feature 013)
+        @self.app.get("/api/state-memory/status")
+        async def get_state_memory_status():
+            """Get State Memory status (FR-005)"""
+            return {
+                "enabled": self.state_memory.config.enabled,
+                "buffer_size": len(self.state_memory.buffer),
+                "max_buffer_size": self.state_memory.config.buffer_size,
+                "total_frames": self.state_memory.total_frames,
+                "current_bias": self.state_memory.current_bias,
+                "smoothed_values": self.state_memory.get_smoothed_values()
+            }
 
+        @self.app.post("/api/state-memory/enable")
+        async def set_state_memory_enabled(enabled: bool):
+            """Enable or disable State Memory (FR-006, SC-004)"""
+            self.state_memory.set_enabled(enabled)
+            return {
+                "ok": True,
+                "enabled": self.state_memory.config.enabled,
+                "message": f"State Memory {'enabled' if enabled else 'disabled'}"
+            }
 
+        @self.app.get("/api/state-memory/stats")
+        async def get_state_memory_stats():
+            """Get State Memory statistics (FR-005)"""
+            return self.state_memory.get_statistics()
+
+        @self.app.get("/api/state-memory/trend")
+        async def get_state_memory_trend():
+            """Get trend summary (FR-003, FR-005)"""
+            return self.state_memory.get_trend_summary()
+
+        @self.app.post("/api/state-memory/reset")
+        async def reset_state_memory_buffer():
+            """Reset State Memory buffer (FR-007)"""
+            self.state_memory.reset_buffer()
+            return {"ok": True, "message": "Buffer reset"}
+
+        @self.app.get("/api/state-memory/buffer")
+        async def export_state_memory_buffer():
+            """Export State Memory buffer for analysis"""
+            return self.state_memory.export_buffer()
+
+        # State Classifier API endpoints (Feature 015)
+        @self.app.get("/api/state-classifier/status")
+        async def get_state_classifier_status():
+            """Get current consciousness state (FR-002)"""
+            return self.state_classifier.get_current_state()
+
+        @self.app.get("/api/state-classifier/stats")
+        async def get_state_classifier_stats():
+            """Get State Classifier statistics (FR-005)"""
+            return self.state_classifier.get_statistics()
+
+        @self.app.get("/api/state-classifier/transitions")
+        async def get_state_classifier_transitions(n: int = 512):
+            """Get recent transition history (FR-004)"""
+            return {
+                "transitions": self.state_classifier.get_transition_history(n)
+            }
 
+        @self.app.get("/api/state-classifier/matrix")
+        async def get_state_classifier_matrix():
+            """Get transition probability matrix (FR-004)"""
+            matrix = self.state_classifier.get_transition_matrix()
+            return {"matrix": matrix.tolist()}
+
+        @self.app.post("/api/state-classifier/reset")
+        async def reset_state_classifier():
+            """Reset State Classifier state"""
+            self.state_classifier.reset()
+            return {"ok": True, "message": "State classifier reset"}
+
+        # Predictive Model API endpoints (Feature 016)
+        @self.app.get("/api/predictive-model/forecast")
+        async def get_predictive_model_forecast():
+            """Get latest forecast (FR-005)"""
+            if not self.predictive_model:
+                return {"ok": False, "message": "Predictive Model not enabled"}
+
+            forecast = self.predictive_model.get_last_forecast()
+            if forecast:
+                return forecast
+            else:
+                return {"ok": False, "message": "No forecast available yet"}
+
+        @self.app.get("/api/predictive-model/stats")
+        async def get_predictive_model_stats():
+            """Get prediction statistics (FR-007, SC-001, SC-002, SC-004)"""
+            if not self.predictive_model:
+                return {"ok": False, "message": "Predictive Model not enabled"}
+
+            return self.predictive_model.get_statistics()
+
+        @self.app.post("/api/predictive-model/reset")
+        async def reset_predictive_model():
+            """Reset Predictive Model state"""
+            if not self.predictive_model:
+                return {"ok": False, "message": "Predictive Model not enabled"}
+
+            self.predictive_model.reset()
+            return {"ok": True, "message": "Predictive model reset"}
+
+        # Session Recorder API endpoints (Feature 017)
+        @self.app.post("/api/record/start")
+        async def start_recording():
+            """Start session recording (FR-006)"""
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            success = self.session_recorder.start_recording()
+            if success:
+                status = self.session_recorder.get_status()
+                return {
+                    "ok": True,
+                    "message": "Recording started",
+                    "session_name": status['session_name'],
+                    "session_path": status['session_path']
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": self.session_recorder.last_error or "Failed to start recording"
+                }
+
+        @self.app.post("/api/record/stop")
+        async def stop_recording():
+            """Stop session recording (FR-006)"""
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            success = self.session_recorder.stop_recording()
+            if success:
+                status = self.session_recorder.get_status()
+                return {
+                    "ok": True,
+                    "message": "Recording stopped",
+                    "statistics": status.get('statistics', {})
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": self.session_recorder.last_error or "Failed to stop recording"
+                }
+
+        @self.app.get("/api/record/status")
+        async def get_recording_status():
+            """Get recording status (FR-006)"""
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            return self.session_recorder.get_status()
+
+        @self.app.get("/api/record/sessions")
+        async def list_sessions():
+            """List all recorded sessions (FR-006)"""
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            sessions = self.session_recorder.list_sessions()
+            return {
+                "ok": True,
+                "sessions": sessions,
+                "count": len(sessions)
+            }
 
+        @self.app.get("/api/record/estimate")
+        async def estimate_size(duration: float = 60.0):
+            """Estimate recording size (FR-007)"""
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            estimate = self.session_recorder.get_size_estimate(duration)
+            return {
+                "ok": True,
+                "duration_seconds": duration,
+                **estimate
+            }
 
-main_app = MainApp()
+        # Timeline Player API endpoints (Feature 018)
+        @self.app.post("/api/playback/load")
+        async def load_session_for_playback(session_path: str):
+            """Load recorded session for playback (FR-002)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.load_session(session_path)
+            if success:
+                status = self.timeline_player.get_status()
+                return {
+                    "ok": True,
+                    "message": "Session loaded",
+                    "duration": status.get('total_duration', 0),
+                    "session_path": session_path
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": self.timeline_player.last_error or "Failed to load session"
+                }
+
+        @self.app.post("/api/playback/play")
+        async def start_playback():
+            """Start playback (FR-003, SC-001)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.play()
+            if success:
+                return {"ok": True, "message": "Playback started"}
+            else:
+                return {"ok": False, "message": "Failed to start playback"}
+
+        @self.app.post("/api/playback/pause")
+        async def pause_playback():
+            """Pause playback (FR-003, SC-001)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.pause()
+            if success:
+                return {"ok": True, "message": "Playback paused"}
+            else:
+                return {"ok": False, "message": "Failed to pause playback"}
+
+        @self.app.post("/api/playback/stop")
+        async def stop_playback():
+            """Stop playback (FR-003, SC-001)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.stop()
+            if success:
+                return {"ok": True, "message": "Playback stopped"}
+            else:
+                return {"ok": False, "message": "Failed to stop playback"}
+
+        @self.app.post("/api/playback/seek")
+        async def seek_playback(time: float):
+            """Seek to specific time (FR-003, SC-002)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.seek(time)
+            if success:
+                return {"ok": True, "message": f"Seeked to {time:.2f}s"}
+            else:
+                return {"ok": False, "message": "Failed to seek"}
+
+        @self.app.post("/api/playback/speed")
+        async def set_playback_speed(speed: float):
+            """Set playback speed (FR-003, SC-003)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.set_speed(speed)
+            if success:
+                return {"ok": True, "message": f"Speed set to {speed}x"}
+            else:
+                return {"ok": False, "message": "Failed to set speed"}
+
+        @self.app.post("/api/playback/range")
+        async def set_playback_range(start: float, end: float):
+            """Set playback range (FR-003)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.set_range(start, end)
+            if success:
+                return {"ok": True, "message": f"Range set to [{start:.2f}, {end:.2f}]"}
+            else:
+                return {"ok": False, "message": "Failed to set range"}
+
+        @self.app.post("/api/playback/loop")
+        async def set_playback_loop(enabled: bool):
+            """Enable or disable loop (FR-003)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            success = self.timeline_player.set_loop(enabled)
+            if success:
+                return {"ok": True, "message": f"Loop {'enabled' if enabled else 'disabled'}"}
+            else:
+                return {"ok": False, "message": "Failed to set loop"}
+
+        @self.app.get("/api/playback/status")
+        async def get_playback_status():
+            """Get playback status (FR-003, SC-004)"""
+            if not self.timeline_player:
+                return {"ok": False, "message": "Timeline Player not enabled"}
+
+            return self.timeline_player.get_status()
+
+        # Data Exporter API endpoints (Feature 019)
+        @self.app.post("/api/export")
+        async def export_session(
+            session_path: str,
+            format: str,
+            output_name: Optional[str] = None,
+            time_range_start: Optional[float] = None,
+            time_range_end: Optional[float] = None,
+            compress: bool = True
+        ):
+            """
+            Export session to specified format (FR-005)
+
+            Args:
+                session_path: Path to session folder
+                format: Export format (csv, json, hdf5, mp4)
+                output_name: Optional output filename (without extension)
+                time_range_start: Optional start time in seconds (FR-004)
+                time_range_end: Optional end time in seconds (FR-004)
+                compress: Enable ZIP compression (FR-007)
+            """
+            if not self.data_exporter:
+                return {"ok": False, "message": "Data Exporter not enabled"}
+
+            try:
+                # Parse format
+                export_format = ExportFormat(format.lower())
+
+                # Create time range tuple if specified
+                time_range = None
+                if time_range_start is not None and time_range_end is not None:
+                    time_range = (time_range_start, time_range_end)
+
+                # Create export request
+                request = ExportRequest(
+                    session_path=session_path,
+                    format=export_format,
+                    output_name=output_name,
+                    time_range=time_range,
+                    compress=compress
+                )
+
+                # Perform export
+                result = self.data_exporter.export_session(request)
+
+                return result
+
+            except ValueError as e:
+                return {"ok": False, "message": f"Invalid format: {format}"}
+            except Exception as e:
+                return {"ok": False, "message": str(e)}
+
+        @self.app.get("/api/export/list")
+        async def list_exported_files():
+            """List all exported files"""
+            if not self.data_exporter:
+                return {"ok": False, "message": "Data Exporter not enabled"}
+
+            exports = self.data_exporter.list_exports()
+            return {
+                "ok": True,
+                "exports": exports,
+                "count": len(exports)
+            }
 
+        @self.app.get("/api/export/stats")
+        async def get_export_statistics():
+            """Get export statistics"""
+            if not self.data_exporter:
+                return {"ok": False, "message": "Data Exporter not enabled"}
 
-@app.get("/")
-async def root():
-    """Serve default HTML or status summary."""
-    try)
-    except Exception as e:
-        return {"error": f"Startup HTML failed)
-async def websocket_ui(ws))
-    print("🎧 WebSocket UI connected")
+            stats = self.data_exporter.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
-    try:
-        while True)
-            param = data.get("param")
-            value = data.get("value")
+        # Node Synchronizer API endpoints (Feature 020)
+        @self.app.post("/api/node/register")
+        async def register_node(node_id: str, node_info: Dict):
+            """Register client node (master only) (FR-007)"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
-            # Example: dynamic routing to callback
-            if param and value is not None, value)
+            if self.node_sync.role != NodeRole.MASTER:
+                return {"ok": False, "message": "Only master can register clients"}
 
-            await ws.send_json({"ack", "param", "value")
+            self.node_sync.register_client(node_id, node_info)
+            return {
+                "ok": True,
+                "message": f"Node {node_id} registered"
+            }
 
-    except WebSocketDisconnect)
-    except Exception as e:
-        print(f"❌ WebSocket error)
-    finally)
+        @self.app.post("/api/node/unregister")
+        async def unregister_node(node_id: str):
+            """Unregister client node (master only) (FR-007)"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
+            if self.node_sync.role != NodeRole.MASTER:
+                return {"ok": False, "message": "Only master can unregister clients"}
 
+            self.node_sync.unregister_client(node_id)
+            return {
+                "ok": True,
+                "message": f"Node {node_id} unregistered"
+            }
 
+        @self.app.get("/api/node/status")
+        async def get_node_status():
+            """Get node synchronizer status (FR-007, FR-008)"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
+            status = self.node_sync.get_status()
+            return {
+                "ok": True,
+                **status
+            }
 
-@app.websocket("/ws/metrics")
-async def websocket_metrics(ws))
-    print("📊 WebSocket Metrics connected")
+        @self.app.get("/api/node/stats")
+        async def get_node_statistics():
+            """Get node synchronizer statistics (SC-001, SC-002)"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
-    try:
-        while True)
-            fake_metrics = {
-                "ici",
-                "phase_coherence",
-                "phi_phase",
-                "phi_depth",
-                "cpu_load",
-                "latency_ms",
+            stats = self.node_sync.get_statistics()
+            return {
+                "ok": True,
+                **stats
             }
-            await ws.send_json(fake_metrics)
-    except WebSocketDisconnect)
-    except Exception as e:
-        print(f"❌ Metrics stream error)
-# ============================================================
-# STARTUP & SHUTDOWN EVENTS
-# ============================================================
 
+        @self.app.post("/api/node/start")
+        async def start_node_sync():
+            """Start node synchronizer"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
+            await self.node_sync.start()
+            return {
+                "ok": True,
+                "message": "Node synchronizer started"
+            }
 
+        @self.app.post("/api/node/stop")
+        async def stop_node_sync():
+            """Stop node synchronizer"""
+            if not self.node_sync:
+                return {"ok": False, "message": "Node Synchronizer not enabled"}
 
+            await self.node_sync.stop()
+            return {
+                "ok": True,
+                "message": "Node synchronizer stopped"
+            }
 
-@app.on_event("startup")
-async def startup_event())
+        # PhaseNet Protocol API endpoints (Feature 021)
+        @self.app.post("/api/network/start")
+        async def start_phasenet():
+            """Start PhaseNet node (FR-008)"""
+            if not self.phasenet:
+                return {"ok": False, "message": "PhaseNet not enabled"}
+
+            self.phasenet.start()
+            return {
+                "ok": True,
+                "message": "PhaseNet started",
+                "node_id": self.phasenet.node_id
+            }
 
-    try:
-        if main_app.audio_server)
-            print("🎚️ AudioServer started.")
-        if main_app.metrics_streamer)
-            print("📡 MetricsStreamer started.")
-        if main_app.auto_phi_learner)
-            print("🧠 AutoPhiLearner started.")
-    except Exception as e:
-        print(f"❌ Startup sequence error)
+        @self.app.post("/api/network/stop")
+        async def stop_phasenet():
+            """Stop PhaseNet node (FR-008)"""
+            if not self.phasenet:
+                return {"ok": False, "message": "PhaseNet not enabled"}
 
+            self.phasenet.stop()
+            return {
+                "ok": True,
+                "message": "PhaseNet stopped"
+            }
 
+        @self.app.get("/api/network/status")
+        async def get_network_status():
+            """Get network status (FR-008)"""
+            if not self.phasenet:
+                return {"ok": False, "message": "PhaseNet not enabled"}
 
+            status = self.phasenet.get_status()
+            return {
+                "ok": True,
+                **status
+            }
 
+        @self.app.get("/api/network/stats")
+        async def get_network_statistics():
+            """Get network statistics (SC-001, SC-002)"""
+            if not self.phasenet:
+                return {"ok": False, "message": "PhaseNet not enabled"}
 
-@app.on_event("shutdown")
-async def shutdown_event())
+            stats = self.phasenet.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
-    try:
-        if main_app.audio_server)
-        if main_app.metrics_streamer)
-        if main_app.auto_phi_learner)
-    except Exception as e:
-        print(f"❌ Shutdown error)
+        @self.app.post("/api/network/update-phase")
+        async def update_network_phase(
+            phi_phase: float,
+            phi_depth: float,
+            criticality: float,
+            coherence: float,
+            ici: float
+        ):
+            """Manually update phase on network (FR-004)"""
+            if not self.phasenet:
+                return {"ok": False, "message": "PhaseNet not enabled"}
+
+            self.phasenet.update_phase(phi_phase, phi_depth, criticality, coherence, ici)
+            return {
+                "ok": True,
+                "message": "Phase updated"
+            }
 
+        # Cluster Monitor API endpoints (Feature 022)
+        @self.app.get("/api/cluster/nodes")
+        async def get_cluster_nodes():
+            """Get list of all nodes in cluster (FR-003, SC-001)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            nodes = self.cluster_monitor.get_nodes_list()
+            return {
+                "ok": True,
+                "nodes": nodes,
+                "count": len(nodes)
+            }
 
+        @self.app.get("/api/cluster/nodes/{node_id}")
+        async def get_cluster_node_detail(node_id: str):
+            """Get detailed node info with history (FR-003, SC-003)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            detail = self.cluster_monitor.get_node_detail(node_id)
+            if detail:
+                return {
+                    "ok": True,
+                    **detail
+                }
+            else:
+                return {"ok": False, "message": f"Node {node_id} not found"}
+
+        @self.app.post("/api/cluster/nodes/{node_id}/promote")
+        async def promote_cluster_node(node_id: str, token: Optional[str] = None):
+            """Promote node to master (FR-003, SC-002, SC-005)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            result = self.cluster_monitor.promote_node(node_id, token)
+            return result
+
+        @self.app.post("/api/cluster/nodes/{node_id}/quarantine")
+        async def quarantine_cluster_node(node_id: str, token: Optional[str] = None):
+            """Quarantine node (FR-003, SC-002, SC-005)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            result = self.cluster_monitor.quarantine_node(node_id, token)
+            return result
+
+        @self.app.post("/api/cluster/nodes/{node_id}/restart")
+        async def restart_cluster_node(node_id: str, token: Optional[str] = None):
+            """Restart node (FR-003, SC-002, SC-005)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            result = self.cluster_monitor.restart_node(node_id, token)
+            return result
+
+        @self.app.get("/api/cluster/stats")
+        async def get_cluster_statistics():
+            """Get cluster statistics (FR-001)"""
+            if not self.cluster_monitor:
+                return {"ok": False, "message": "Cluster Monitor not enabled"}
+
+            stats = self.cluster_monitor.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
+        # Hardware Interface API endpoints (Feature 023)
+        @self.app.get("/api/hardware/devices")
+        async def list_hardware_devices():
+            """List available serial devices (FR-007)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            devices = self.hw_interface.list_devices()
+            return {
+                "ok": True,
+                "devices": devices,
+                "count": len(devices)
+            }
 
+        @self.app.post("/api/hardware/connect")
+        async def connect_hardware(port: Optional[str] = None):
+            """Connect to hardware device (FR-007)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            # Update port if provided
+            if port:
+                self.hw_interface.port = port
+
+            success = self.hw_interface.connect()
+            if success:
+                version = self.hw_interface.get_version()
+                return {
+                    "ok": True,
+                    "message": "Connected to hardware",
+                    "port": self.hw_interface.port,
+                    "baudrate": self.hw_interface.baudrate,
+                    "firmware_version": version
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to connect to hardware"
+                }
+
+        @self.app.post("/api/hardware/disconnect")
+        async def disconnect_hardware():
+            """Disconnect from hardware device"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            self.hw_interface.disconnect()
+            return {
+                "ok": True,
+                "message": "Disconnected from hardware"
+            }
 
-# ============================================================
-# MAIN ENTRY POINT
-# ============================================================
+        @self.app.post("/api/hardware/start")
+        async def start_hardware():
+            """Start hardware bridge (FR-002)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            success = self.hw_interface.start()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hardware bridge started"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to start hardware bridge"
+                }
+
+        @self.app.post("/api/hardware/stop")
+        async def stop_hardware():
+            """Stop hardware bridge"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            success = self.hw_interface.stop()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hardware bridge stopped"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to stop hardware bridge"
+                }
+
+        @self.app.get("/api/hardware/status")
+        async def get_hardware_status():
+            """Get hardware link status (FR-008)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            return {
+                "ok": True,
+                "is_connected": self.hw_interface.is_connected,
+                "is_running": self.hw_interface.is_running,
+                "port": self.hw_interface.port,
+                "baudrate": self.hw_interface.baudrate,
+                "link_status": self.hw_interface.stats.link_status
+            }
 
-if __name__ == "__main__":
-    import uvicorn
-    uvicorn.run(
-        "server.main,
-        host="127.0.0.1",
-        port=8000,
-        reload=True,
-        log_level="info"
+        @self.app.get("/api/hardware/stats")
+        async def get_hardware_statistics():
+            """Get hardware statistics (SC-001, SC-002)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
 
-# ============================================================
-# END OF FILE
-# ============================================================
-# ============================================================
-# STARTUP & SHUTDOWN EVENTS
-# ============================================================
+            stats = self.hw_interface.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
+        @self.app.post("/api/hardware/self-test")
+        async def run_hardware_self_test():
+            """Run hardware self-test (FR-010, SC-001)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            passed, latency_us, jitter_us = self.hw_interface.self_test()
+            return {
+                "ok": True,
+                "passed": passed,
+                "latency_us": latency_us,
+                "jitter_us": jitter_us,
+                "meets_sc001": latency_us <= 40 and jitter_us <= 5
+            }
 
+        @self.app.post("/api/hardware/calibrate")
+        async def calibrate_hardware_drift():
+            """Calibrate clock drift (FR-005)"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            drift_ppm = self.hw_interface.calibrate_drift()
+            return {
+                "ok": True,
+                "drift_ppm": drift_ppm,
+                "message": f"Clock drift: {drift_ppm:.3f} ppm"
+            }
 
+        @self.app.post("/api/hardware/reset-stats")
+        async def reset_hardware_statistics():
+            """Reset hardware statistics counters"""
+            if not self.hw_interface:
+                return {"ok": False, "message": "Hardware Interface not enabled"}
+
+            success = self.hw_interface.reset_statistics()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Statistics reset"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to reset statistics"
+                }
+
+        # Hybrid Analog-DSP Node API endpoints (Feature 024)
+        @self.app.get("/api/hybrid/devices")
+        async def list_hybrid_devices():
+            """List available serial devices (FR-005)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            devices = self.hybrid_bridge.list_devices()
+            return {
+                "ok": True,
+                "devices": devices,
+                "count": len(devices)
+            }
 
+        @self.app.post("/api/hybrid/connect")
+        async def connect_hybrid(port: Optional[str] = None):
+            """Connect to hybrid node (FR-005)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            # Update port if provided
+            if port:
+                self.hybrid_bridge.port = port
+
+            success = self.hybrid_bridge.connect()
+            if success:
+                version = self.hybrid_bridge.get_version()
+                return {
+                    "ok": True,
+                    "message": "Connected to hybrid node",
+                    "port": self.hybrid_bridge.port,
+                    "baudrate": self.hybrid_bridge.baudrate,
+                    "firmware_version": version
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to connect to hybrid node"
+                }
+
+        @self.app.post("/api/hybrid/disconnect")
+        async def disconnect_hybrid():
+            """Disconnect from hybrid node"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            self.hybrid_bridge.disconnect()
+            return {
+                "ok": True,
+                "message": "Disconnected from hybrid node"
+            }
 
-@app.on_event("startup")
-async def startup_event())
+        @self.app.post("/api/hybrid/start")
+        async def start_hybrid():
+            """Start hybrid node processing (FR-001)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            success = self.hybrid_bridge.start()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hybrid node started"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to start hybrid node"
+                }
+
+        @self.app.post("/api/hybrid/stop")
+        async def stop_hybrid():
+            """Stop hybrid node processing"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            success = self.hybrid_bridge.stop()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hybrid node stopped"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to stop hybrid node"
+                }
+
+        @self.app.get("/api/hybrid/status")
+        async def get_hybrid_status():
+            """Get hybrid node status (FR-009)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            status = self.hybrid_bridge.get_status()
+            return {
+                "ok": True,
+                "is_connected": self.hybrid_bridge.is_connected,
+                "is_running": self.hybrid_bridge.is_running,
+                "port": self.hybrid_bridge.port,
+                "baudrate": self.hybrid_bridge.baudrate,
+                **status
+            }
 
-    try:
-        if main_app.audio_server)
-            print("🎚️ AudioServer started.")
-        if main_app.metrics_streamer)
-            print("📡 MetricsStreamer started.")
-        if main_app.auto_phi_learner)
-            print("🧠 AutoPhiLearner started.")
-    except Exception as e:
-        print(f"❌ Startup sequence error)
+        @self.app.get("/api/hybrid/dsp-metrics")
+        async def get_hybrid_dsp_metrics():
+            """Get DSP metrics (ICI, coherence, spectral analysis) (FR-003)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            metrics = self.hybrid_bridge.get_dsp_metrics()
+            if metrics:
+                return {
+                    "ok": True,
+                    **metrics
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to get DSP metrics"
+                }
+
+        @self.app.get("/api/hybrid/safety")
+        async def get_hybrid_safety():
+            """Get safety telemetry (FR-007)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            safety = self.hybrid_bridge.get_safety()
+            if safety:
+                return {
+                    "ok": True,
+                    **safety
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to get safety telemetry"
+                }
+
+        @self.app.get("/api/hybrid/stats")
+        async def get_hybrid_statistics():
+            """Get hybrid node statistics (SC-001, SC-002, SC-003)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            stats = self.hybrid_bridge.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
+        @self.app.post("/api/hybrid/set-preamp-gain")
+        async def set_hybrid_preamp_gain(gain: float):
+            """Set analog preamp gain (FR-002)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            success = self.hybrid_bridge.set_preamp_gain(gain)
+            if success:
+                return {
+                    "ok": True,
+                    "message": f"Preamp gain set to {gain:.2f}"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to set preamp gain"
+                }
+
+        @self.app.post("/api/hybrid/set-control-voltage")
+        async def set_hybrid_control_voltage(cv1: float, cv2: float, phi_phase: float = 0.0, phi_depth: float = 0.0):
+            """Set control voltage outputs (FR-004)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            from hybrid_bridge import ControlVoltage
+            cv = ControlVoltage(cv1=cv1, cv2=cv2, phi_phase=phi_phase, phi_depth=phi_depth)
+            success = self.hybrid_bridge.set_control_voltage(cv)
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Control voltage set",
+                    "cv1": cv1,
+                    "cv2": cv2,
+                    "phi_phase": phi_phase,
+                    "phi_depth": phi_depth
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to set control voltage"
+                }
+
+        @self.app.post("/api/hybrid/calibrate")
+        async def calibrate_hybrid():
+            """Run calibration routine (FR-008, SC-001)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            calibration = self.hybrid_bridge.calibrate()
+            if calibration:
+                return {
+                    "ok": True,
+                    "message": "Calibration complete",
+                    **calibration,
+                    "meets_sc001": calibration.get('total_latency_us', 9999) <= 2000
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Calibration failed"
+                }
+
+        @self.app.post("/api/hybrid/emergency-shutdown")
+        async def hybrid_emergency_shutdown(reason: str = "Manual shutdown"):
+            """Emergency shutdown hybrid node (FR-007)"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            success = self.hybrid_bridge.emergency_shutdown(reason)
+            if success:
+                return {
+                    "ok": True,
+                    "message": f"Emergency shutdown executed: {reason}"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to execute emergency shutdown"
+                }
+
+        @self.app.post("/api/hybrid/reset-stats")
+        async def reset_hybrid_statistics():
+            """Reset hybrid node statistics counters"""
+            if not self.hybrid_bridge:
+                return {"ok": False, "message": "Hybrid Bridge not enabled"}
+
+            success = self.hybrid_bridge.reset_statistics()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Statistics reset"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to reset statistics"
+                }
+
+        # Hybrid Node Integration API endpoints (Feature 025)
+        @self.app.get("/api/hybrid-node/devices")
+        async def list_hybrid_node_audio_devices():
+            """List available audio devices for hybrid node (FR-005)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            devices = HybridNode.list_audio_devices()
+            return {
+                "ok": True,
+                "devices": devices,
+                "count": len(devices)
+            }
 
+        @self.app.post("/api/hybrid-node/start")
+        async def start_hybrid_node():
+            """Start hybrid mode processing (FR-003, SC-001)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            success = self.hybrid_node.start()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hybrid mode started"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to start hybrid mode"
+                }
+
+        @self.app.post("/api/hybrid-node/stop")
+        async def stop_hybrid_node():
+            """Stop hybrid mode processing"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            success = self.hybrid_node.stop()
+            if success:
+                return {
+                    "ok": True,
+                    "message": "Hybrid mode stopped"
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to stop hybrid mode"
+                }
+
+        @self.app.get("/api/hybrid-node/status")
+        async def get_hybrid_node_status():
+            """Get hybrid node status (FR-007)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            metrics = self.hybrid_node.get_current_metrics()
+            stats = self.hybrid_node.get_statistics()
+
+            if metrics:
+                return {
+                    "ok": True,
+                    "is_running": self.hybrid_node.is_running,
+                    "metrics": {
+                        "timestamp": metrics.timestamp,
+                        "ici": metrics.ici,
+                        "phase_coherence": metrics.phase_coherence,
+                        "spectral_centroid": metrics.spectral_centroid,
+                        "consciousness_level": metrics.consciousness_level,
+                        "phi_phase": metrics.phi_phase,
+                        "phi_depth": metrics.phi_depth,
+                        "cpu_load": metrics.cpu_load,
+                        "latency_ms": metrics.latency_ms,
+                        "dropouts": metrics.dropouts
+                    },
+                    "statistics": stats
+                }
+            else:
+                return {
+                    "ok": True,
+                    "is_running": self.hybrid_node.is_running,
+                    "metrics": None,
+                    "statistics": stats
+                }
+
+        @self.app.post("/api/hybrid-node/phi-source")
+        async def set_hybrid_node_phi_source(source: str):
+            """Set Φ modulation source (FR-004, User Story 2)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            try:
+                phi_source = PhiSource(source)
+                self.hybrid_node.set_phi_source(phi_source)
+                return {
+                    "ok": True,
+                    "message": f"Φ source set to {source}",
+                    "source": source
+                }
+            except ValueError:
+                return {
+                    "ok": False,
+                    "message": f"Invalid Φ source: {source}. Valid: manual, microphone, sensor, internal"
+                }
+
+        @self.app.post("/api/hybrid-node/phi-manual")
+        async def set_hybrid_node_phi_manual(phase: float, depth: float):
+            """Set manual Φ values (FR-004, User Story 2)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            self.hybrid_node.set_phi_manual(phase, depth)
+            return {
+                "ok": True,
+                "message": "Manual Φ values set",
+                "phase": phase,
+                "depth": depth
+            }
 
+        @self.app.get("/api/hybrid-node/stats")
+        async def get_hybrid_node_statistics():
+            """Get hybrid node performance statistics (SC-004)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            stats = self.hybrid_node.get_statistics()
+            return {
+                "ok": True,
+                **stats
+            }
 
-@app.on_event("shutdown")
-async def shutdown_event())
+        @self.app.post("/api/hybrid-node/reset-stats")
+        async def reset_hybrid_node_statistics():
+            """Reset hybrid node statistics"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
-    try:
-        if main_app.audio_server)
-        if main_app.metrics_streamer)
-        if main_app.auto_phi_learner)
-    except Exception as e:
-        print(f"❌ Shutdown error)
+            self.hybrid_node.reset_statistics()
+            return {
+                "ok": True,
+                "message": "Statistics reset"
+            }
 
+        # Sensor Binding API endpoints (Feature 011)
+        @self.app.post("/api/sensor-binding/enable")
+        async def enable_sensor_binding():
+            """Enable sensor binding with PhiRouter (Feature 011)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            success = self.hybrid_node.enable_sensor_binding()
+            return {
+                "ok": success,
+                "message": "Sensor binding enabled" if success else "Failed to enable sensor binding"
+            }
 
+        @self.app.post("/api/sensor-binding/disable")
+        async def disable_sensor_binding():
+            """Disable sensor binding"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            self.hybrid_node.disable_sensor_binding()
+            return {
+                "ok": True,
+                "message": "Sensor binding disabled"
+            }
 
+        @self.app.get("/api/sensor-binding/status")
+        async def get_sensor_binding_status():
+            """Get sensor binding status (FR-004)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
-# ============================================================
-# MAIN ENTRY POINT
-# ============================================================
+            status = self.hybrid_node.get_sensor_status()
+            return {
+                "ok": True,
+                **status
+            }
 
-if __name__ == "__main__":
-    import uvicorn
-    uvicorn.run(
-        "server.main,
-        host="127.0.0.1",
-        port=8000,
-        reload=True,
-        log_level="info"
+        @self.app.get("/api/sensor-binding/midi-devices")
+        async def list_midi_devices():
+            """List available MIDI devices (User Story 1)"""
+            devices = HybridNode.list_midi_devices()
+            return {
+                "ok": True,
+                "devices": devices,
+                "count": len(devices)
+            }
 
-# ============================================================
-# END OF FILE
-# ============================================================
-# ============================================================
-# STARTUP & SHUTDOWN EVENTS
-# ============================================================
+        @self.app.get("/api/sensor-binding/serial-devices")
+        async def list_serial_devices():
+            """List available serial devices (User Story 1)"""
+            devices = HybridNode.list_serial_devices()
+            return {
+                "ok": True,
+                "devices": devices,
+                "count": len(devices)
+            }
 
+        @self.app.post("/api/sensor-binding/bind-midi")
+        async def bind_midi_sensor(device_id: Optional[str] = None, cc_number: int = 1, channel: int = 0):
+            """Bind MIDI controller as Φ source (FR-001, User Story 1)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            success = self.hybrid_node.bind_midi_sensor(
+                device_id=device_id,
+                cc_number=cc_number,
+                channel=channel
+            )
+
+            return {
+                "ok": success,
+                "message": f"MIDI CC{cc_number} bound" if success else "Failed to bind MIDI",
+                "sensor_id": f"midi_cc{cc_number}"
+            }
 
+        @self.app.post("/api/sensor-binding/bind-serial")
+        async def bind_serial_sensor(device_id: Optional[str] = None, baudrate: int = 9600,
+                                     input_min: float = 0.0, input_max: float = 1.0):
+            """Bind serial sensor as Φ source (FR-001, User Story 1)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            success = self.hybrid_node.bind_serial_sensor(
+                device_id=device_id,
+                baudrate=baudrate,
+                input_range=(input_min, input_max)
+            )
+
+            return {
+                "ok": success,
+                "message": f"Serial sensor bound: {device_id}" if success else "Failed to bind serial sensor",
+                "sensor_id": f"serial_{device_id or 'auto'}"
+            }
 
+        @self.app.post("/api/sensor-binding/bind-audio-beat")
+        async def bind_audio_beat_detector():
+            """Bind audio beat detector as Φ source (FR-001)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            success = self.hybrid_node.bind_audio_beat_detector()
 
-@app.on_event("startup")
-async def startup_event())
+            return {
+                "ok": success,
+                "message": "Audio beat detector bound" if success else "Failed to bind audio beat detector",
+                "sensor_id": "audio_beat"
+            }
 
-    try:
-        if main_app.audio_server)
-            print("🎚️ AudioServer started.")
-        if main_app.metrics_streamer)
-            print("📡 MetricsStreamer started.")
-        if main_app.auto_phi_learner)
-            print("🧠 AutoPhiLearner started.")
-    except Exception as e:
-        print(f"❌ Startup sequence error)
+        @self.app.post("/api/sensor-binding/unbind")
+        async def unbind_sensor(sensor_id: str):
+            """Unbind a sensor (User Story 2)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            success = self.hybrid_node.unbind_sensor(sensor_id)
 
+            return {
+                "ok": success,
+                "message": f"Sensor unbound: {sensor_id}" if success else f"Failed to unbind sensor: {sensor_id}"
+            }
 
+        # Adaptive Control API endpoints (Feature 012)
+        @self.app.post("/api/adaptive-control/enable")
+        async def enable_adaptive_control(mode: str = "reactive"):
+            """Enable adaptive Phi control (Feature 012)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            # Ensure sensor binding is enabled first
+            if not self.hybrid_node.phi_router:
+                self.hybrid_node.enable_sensor_binding()
 
-@app.on_event("shutdown")
-async def shutdown_event())
+            success = self.hybrid_node.enable_adaptive_control(mode=mode)
 
-    try:
-        if main_app.audio_server)
-        if main_app.metrics_streamer)
-        if main_app.auto_phi_learner)
-    except Exception as e:
-        print(f"❌ Shutdown error)
+            return {
+                "ok": success,
+                "message": f"Adaptive control enabled ({mode} mode)" if success else "Failed to enable adaptive control",
+                "mode": mode
+            }
 
+        @self.app.post("/api/adaptive-control/disable")
+        async def disable_adaptive_control():
+            """Disable adaptive Phi control"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            self.hybrid_node.disable_adaptive_control()
 
+            return {
+                "ok": True,
+                "message": "Adaptive control disabled"
+            }
 
+        @self.app.get("/api/adaptive-control/status")
+        async def get_adaptive_control_status():
+            """Get adaptive control status (FR-004, SC-001, SC-002)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
+
+            status = self.hybrid_node.get_adaptive_status()
+
+            if status is None:
+                return {
+                    "ok": True,
+                    "adaptive_enabled": False,
+                    "status": None
+                }
+
+            return {
+                "ok": True,
+                "adaptive_enabled": True,
+                "status": status
+            }
 
-# ============================================================
-# MAIN ENTRY POINT
-# ============================================================
+        @self.app.post("/api/adaptive-control/manual-override")
+        async def set_adaptive_manual_override(enabled: bool):
+            """Set manual override for adaptive control (User Story 3, SC-004)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
-if __name__ == "__main__":
-    import uvicorn
-    uvicorn.run(
-        "server.main,
-        host="127.0.0.1",
-        port=8000,
-        reload=True,
-        log_level="info"
+            self.hybrid_node.set_adaptive_manual_override(enabled)
 
-# ============================================================
-# END OF FILE
-# ============================================================
-# ============================================================
-# STARTUP & SHUTDOWN EVENTS
-# ============================================================
+            return {
+                "ok": True,
+                "message": f"Manual override {'enabled' if enabled else 'disabled'}",
+                "override_active": enabled
+            }
 
+        @self.app.post("/api/adaptive-control/trigger-learning")
+        async def trigger_adaptive_learning():
+            """Trigger learning from current session (User Story 2, FR-004)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            success = self.hybrid_node.trigger_adaptive_learning()
 
+            return {
+                "ok": success,
+                "message": "Learning triggered" if success else "Failed to trigger learning"
+            }
 
+        @self.app.post("/api/adaptive-control/save-session")
+        async def save_adaptive_session(filepath: str):
+            """Save adaptive session to file (FR-003)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
-@app.on_event("startup")
-async def startup_event())
+            success = self.hybrid_node.save_adaptive_session(filepath)
 
-    try:
-        if main_app.audio_server)
-            print("🎚️ AudioServer started.")
-        if main_app.metrics_streamer)
-            print("📡 MetricsStreamer started.")
-        if main_app.auto_phi_learner)
-            print("🧠 AutoPhiLearner started.")
-    except Exception as e:
-        print(f"❌ Startup sequence error)
+            return {
+                "ok": success,
+                "message": f"Session saved to {filepath}" if success else "Failed to save session",
+                "filepath": filepath
+            }
 
+        @self.app.post("/api/adaptive-control/load-session")
+        async def load_adaptive_session(filepath: str):
+            """Load adaptive session from file (FR-004)"""
+            if not self.hybrid_node:
+                return {"ok": False, "message": "Hybrid Node not enabled"}
 
+            success = self.hybrid_node.load_adaptive_session(filepath)
 
+            return {
+                "ok": success,
+                "message": f"Session loaded from {filepath}" if success else "Failed to load session",
+                "filepath": filepath
+            }
 
+        # Multi-Session Analytics API endpoints (Feature 015)
+        @self.app.post("/api/analytics/sessions/load")
+        async def load_analysis_session(filename: str):
+            """
+            Load a recorded session for analysis (FR-001, User Story 1)
+
+            Args:
+                filename: Name of session file to load
+
+            Returns:
+                Session load status and basic stats
+            """
+            if not self.session_recorder:
+                return {"ok": False, "message": "Session Recorder not enabled"}
+
+            # Load session data
+            session_data = self.session_recorder.load_session(filename)
+
+            if not session_data:
+                return {"ok": False, "message": f"Failed to load session: {filename}"}
+
+            # Extract session ID from metadata
+            metadata = session_data.get('metadata', {})
+            session_id = metadata.get('session_name', filename)
+
+            # Load into comparator
+            success_comparator = self.session_comparator.load_session(session_id, session_data)
+
+            # Load into correlation analyzer
+            self.correlation_analyzer.load_session(session_id, session_data)
+
+            if success_comparator:
+                # Get basic stats
+                stats = self.session_comparator.get_all_stats()
+                session_stats = stats.get(session_id)
+
+                return {
+                    "ok": True,
+                    "message": f"Session {session_id} loaded successfully",
+                    "session_id": session_id,
+                    "stats": {
+                        "duration": session_stats.duration,
+                        "sample_count": session_stats.sample_count,
+                        "mean_ici": session_stats.mean_ici,
+                        "mean_phi": session_stats.mean_phi
+                    } if session_stats else {}
+                }
+            else:
+                return {"ok": False, "message": "Failed to load session into analytics"}
+
+        @self.app.post("/api/analytics/sessions/unload")
+        async def unload_analysis_session(session_id: str):
+            """
+            Unload a session from analysis (SC-001: Memory management)
+
+            Args:
+                session_id: Session identifier to unload
+            """
+            self.session_comparator.unload_session(session_id)
+            return {
+                "ok": True,
+                "message": f"Session {session_id} unloaded"
+            }
 
-@app.on_event("shutdown")
-async def shutdown_event())
+        @self.app.get("/api/analytics/sessions")
+        async def get_loaded_sessions():
+            """
+            Get list of loaded sessions with statistics (FR-002, User Story 1)
+
+            Returns:
+                List of loaded sessions with their statistics
+            """
+            stats = self.session_comparator.get_all_stats()
+
+            sessions_list = []
+            for session_id, session_stats in stats.items():
+                sessions_list.append({
+                    "session_id": session_id,
+                    "duration": session_stats.duration,
+                    "sample_count": session_stats.sample_count,
+                    "mean_ici": session_stats.mean_ici,
+                    "std_ici": session_stats.std_ici,
+                    "mean_coherence": session_stats.mean_coherence,
+                    "mean_criticality": session_stats.mean_criticality,
+                    "mean_phi": session_stats.mean_phi
+                })
+
+            # Get memory usage
+            memory_usage = self.session_comparator.get_memory_usage()
+
+            return {
+                "ok": True,
+                "sessions": sessions_list,
+                "count": len(sessions_list),
+                "memory_usage": memory_usage
+            }
 
-    try:
-        if main_app.audio_server)
-        if main_app.metrics_streamer)
-        if main_app.auto_phi_learner)
-    except Exception as e:
-        print(f"❌ Shutdown error)
+        @self.app.post("/api/analytics/compare")
+        async def compare_sessions(session_a_id: str, session_b_id: str):
+            """
+            Compare two loaded sessions (FR-002, User Story 1)
+
+            Args:
+                session_a_id: First session ID
+                session_b_id: Second session ID
+
+            Returns:
+                Comparison results including deltas and correlations
+            """
+            result = self.session_comparator.compare_sessions(session_a_id, session_b_id)
+
+            if result:
+                return {
+                    "ok": True,
+                    "comparison": {
+                        "session_a": result.session_a_id,
+                        "session_b": result.session_b_id,
+                        "deltas": {
+                            "ici": result.delta_mean_ici,
+                            "coherence": result.delta_mean_coherence,
+                            "criticality": result.delta_mean_criticality,
+                            "phi": result.delta_mean_phi
+                        },
+                        "correlations": {
+                            "ici": result.ici_correlation,
+                            "coherence": result.coherence_correlation,
+                            "criticality": result.criticality_correlation,
+                            "phi": result.phi_correlation
+                        },
+                        "statistical_significance": {
+                            "ici_ttest_pvalue": result.ici_ttest_pvalue,
+                            "coherence_ttest_pvalue": result.coherence_ttest_pvalue
+                        }
+                    }
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to compare sessions. Ensure both sessions are loaded."
+                }
+
+        @self.app.get("/api/analytics/correlations")
+        async def get_correlation_matrices(metric: str = "ici"):
+            """
+            Get correlation matrix for a metric across all sessions (FR-004, User Story 3)
+
+            Args:
+                metric: Metric to correlate (ici, coherence, criticality, phi)
+
+            Returns:
+                Correlation matrix with validation flags
+            """
+            corr_matrix = self.correlation_analyzer.compute_correlation_matrix(metric)
+
+            if corr_matrix:
+                return {
+                    "ok": True,
+                    "metric": corr_matrix.metric_name,
+                    "session_ids": corr_matrix.session_ids,
+                    "matrix": corr_matrix.matrix,
+                    "is_symmetric": corr_matrix.is_symmetric,
+                    "diagonal_ones": corr_matrix.diagonal_ones,
+                    "session_count": len(corr_matrix.session_ids)
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to compute correlation matrix. Need at least 2 loaded sessions."
+                }
+
+        @self.app.get("/api/analytics/correlations/all")
+        async def get_all_correlations():
+            """
+            Get correlation matrices for all metrics (FR-004)
+
+            Returns:
+                Correlation matrices for all metric types
+            """
+            all_correlations = self.correlation_analyzer.compute_all_correlations()
+
+            result = {}
+            for metric, corr_matrix in all_correlations.items():
+                result[metric] = {
+                    "session_ids": corr_matrix.session_ids,
+                    "matrix": corr_matrix.matrix,
+                    "is_symmetric": corr_matrix.is_symmetric,
+                    "diagonal_ones": corr_matrix.diagonal_ones
+                }
+
+            return {
+                "ok": True,
+                "correlations": result,
+                "metrics_count": len(result)
+            }
 
+        @self.app.get("/api/analytics/heatmap")
+        async def get_correlation_heatmap(metric: str = "ici"):
+            """
+            Get heatmap visualization data (FR-004, UI support)
+
+            Args:
+                metric: Metric for heatmap (ici, coherence, criticality, phi)
+
+            Returns:
+                Heatmap data with statistics for visualization
+            """
+            heatmap = self.correlation_analyzer.get_heatmap_data(metric)
+
+            if heatmap:
+                return {
+                    "ok": True,
+                    **heatmap
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "Failed to generate heatmap data"
+                }
+
+        @self.app.get("/api/analytics/summary")
+        async def get_analytics_summary():
+            """
+            Get summary table for all session pairs (FR-004)
+
+            Returns:
+                Summary statistics for all pairwise comparisons
+            """
+            summary = self.correlation_analyzer.get_summary_table()
+
+            return {
+                "ok": True,
+                "summary": summary,
+                "pair_count": len(summary)
+            }
 
+        # Chromatic Visualizer API endpoints (Feature 016)
+        @self.app.get("/api/chromatic/state")
+        async def get_chromatic_state():
+            """
+            Get current chromatic visualization state (FR-001, FR-002)
+
+            Returns:
+                Current chromatic state with channel colors and Phi modulation
+            """
+            state = self.chromatic_visualizer.get_current_state()
+
+            if state:
+                return {
+                    "ok": True,
+                    **state
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "No chromatic state available"
+                }
+
+        @self.app.get("/api/chromatic/performance")
+        async def get_chromatic_performance():
+            """
+            Get chromatic visualizer performance stats (SC-003, SC-005)
+
+            Returns:
+                Performance statistics including FPS and frame time
+            """
+            stats = self.chromatic_visualizer.get_performance_stats()
+
+            return {
+                "ok": True,
+                **stats
+            }
 
+        @self.app.get("/api/chromatic/config")
+        async def get_chromatic_config():
+            """
+            Get current visualizer configuration
+
+            Returns:
+                Current configuration settings
+            """
+            config = self.chromatic_visualizer.config
+
+            return {
+                "ok": True,
+                "config": {
+                    "num_channels": config.num_channels,
+                    "min_frequency": config.min_frequency,
+                    "max_frequency": config.max_frequency,
+                    "frequency_scale": config.frequency_scale,
+                    "phi_rotation_enabled": config.phi_rotation_enabled,
+                    "phi_breathing_enabled": config.phi_breathing_enabled,
+                    "phi_breathing_frequency": config.phi_breathing_frequency,
+                    "target_fps": config.target_fps
+                }
+            }
 
+        @self.app.post("/api/chromatic/config/phi-rotation")
+        async def set_phi_rotation(enabled: bool):
+            """
+            Enable or disable Phi golden angle rotation (FR-002)
 
-# ============================================================
-# MAIN ENTRY POINT
-# ============================================================
+            Args:
+                enabled: Whether to enable Phi rotation
 
-if __name__ == "__main__":
-    import uvicorn
-    uvicorn.run(
-        "server.main,
-        host="127.0.0.1",
-        port=8000,
-        reload=True,
-        log_level="info"
+            Returns:
+                Success status
+            """
+            self.chromatic_visualizer.config.phi_rotation_enabled = enabled
 
-# ============================================================
-# END OF FILE
-# ============================================================
-# ============================================================
-# STARTUP & SHUTDOWN EVENTS
-# ============================================================
+            return {
+                "ok": True,
+                "message": f"Phi rotation {'enabled' if enabled else 'disabled'}",
+                "phi_rotation_enabled": enabled
+            }
 
+        @self.app.post("/api/chromatic/config/phi-breathing")
+        async def set_phi_breathing(enabled: bool, frequency: Optional[float] = None):
+            """
+            Enable or disable Phi-breathing visualization (User Story 2, FR-002)
 
+            Args:
+                enabled: Whether to enable Phi breathing
+                frequency: Optional breathing frequency in Hz (1-2 Hz typical)
 
+            Returns:
+                Success status
+            """
+            self.chromatic_visualizer.config.phi_breathing_enabled = enabled
 
+            if frequency is not None:
+                self.chromatic_visualizer.config.phi_breathing_frequency = frequency
 
-@app.on_event("startup")
-async def startup_event())
+            return {
+                "ok": True,
+                "message": f"Phi breathing {'enabled' if enabled else 'disabled'}",
+                "phi_breathing_enabled": enabled,
+                "phi_breathing_frequency": self.chromatic_visualizer.config.phi_breathing_frequency
+            }
 
-    try:
-        if main_app.audio_server)
-            print("🎚️ AudioServer started.")
-        if main_app.metrics_streamer)
-            print("📡 MetricsStreamer started.")
-        if main_app.auto_phi_learner)
-            print("🧠 AutoPhiLearner started.")
-    except Exception as e:
-        print(f"❌ Startup sequence error)
+        @self.app.post("/api/chromatic/config/frequency-scale")
+        async def set_frequency_scale(scale: str):
+            """
+            Set frequency to hue mapping scale (FR-001)
+
+            Args:
+                scale: "linear" or "log"
+
+            Returns:
+                Success status
+            """
+            if scale not in ["linear", "log"]:
+                return {
+                    "ok": False,
+                    "message": "Invalid scale. Must be 'linear' or 'log'"
+                }
+
+            self.chromatic_visualizer.config.frequency_scale = scale
+
+            return {
+                "ok": True,
+                "message": f"Frequency scale set to {scale}",
+                "frequency_scale": scale
+            }
 
+        # Phi-Matrix Dashboard API endpoints (Feature 017)
+        @self.app.get("/api/dashboard/state")
+        async def get_dashboard_state():
+            """
+            Get current synchronized dashboard state (FR-002, FR-003)
+
+            Returns:
+                Current synchronized state across all modules
+            """
+            state = self.state_sync_manager.get_state()
+
+            if state:
+                return {
+                    "ok": True,
+                    **state
+                }
+            else:
+                return {
+                    "ok": False,
+                    "message": "No dashboard state available"
+                }
+
+        @self.app.post("/api/dashboard/pause")
+        async def pause_dashboard():
+            """
+            Pause all synchronized modules (User Story 2)
+
+            Returns:
+                Success status with master time
+            """
+            self.state_sync_manager.pause()
+
+            return {
+                "ok": True,
+                "message": "Dashboard paused",
+                "master_time": self.state_sync_manager.get_master_time(),
+                "is_paused": True
+            }
 
+        @self.app.post("/api/dashboard/resume")
+        async def resume_dashboard():
+            """
+            Resume all synchronized modules (User Story 2)
+
+            Returns:
+                Success status with master time
+            """
+            self.state_sync_manager.resume()
+
+            return {
+                "ok": True,
+                "message": "Dashboard resumed",
+                "master_time": self.state_sync_manager.get_master_time(),
+                "is_paused": False
+            }
 
+        @self.app.get("/api/dashboard/latency")
+        async def get_dashboard_latency():
+            """
+            Get WebSocket latency statistics (FR-003, SC-001)
 
+            Returns:
+                Latency statistics and success criteria compliance
+            """
+            stats = self.state_sync_manager.get_latency_stats()
 
-@app.on_event("shutdown")
-async def shutdown_event())
+            return {
+                "ok": True,
+                **stats
+            }
 
-    try:
-        if main_app.audio_server)
-        if main_app.metrics_streamer)
-        if main_app.auto_phi_learner)
-    except Exception as e:
-        print(f"❌ Shutdown error)
+        @self.app.get("/api/dashboard/sync-health")
+        async def get_sync_health():
+            """
+            Get synchronization health report (SC-004)
 
+            Returns:
+                Sync health status and desync detection
+            """
+            health = self.state_sync_manager.check_sync_health()
 
+            return {
+                "ok": True,
+                **health
+            }
 
+        # Dashboard WebSocket endpoint (FR-003)
+        @self.app.websocket("/ws/dashboard")
+        async def websocket_dashboard(websocket):
+            """
+            WebSocket endpoint for unified dashboard synchronization (Feature 017)
+
+            Provides:
+            - Bidirectional communication < 50ms (FR-003)
+            - State synchronization across all modules (FR-002)
+            - Pause/resume coordination (User Story 2)
+            - Interactive control surface (User Story 3)
+            """
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+
+            await websocket.accept()
+            await self.state_sync_manager.register_client(websocket)
+
+            try:
+                while True:
+                    # Receive message from client
+                    data = await websocket.receive_text()
+                    message = json.loads(data)
+
+                    # Handle message and get response
+                    response = await self.state_sync_manager.handle_client_message(websocket, message)
+
+                    # Send response
+                    if response:
+                        await websocket.send_json(response)
+
+            except WebSocketDisconnect:
+                await self.state_sync_manager.unregister_client(websocket)
+            except Exception as e:
+                print(f"[Dashboard WebSocket] Error: {e}")
+                await self.state_sync_manager.unregister_client(websocket)
+
+        # Metrics WebSocket endpoint
+        @self.app.websocket("/ws/metrics")
+        async def websocket_metrics(websocket):
+            """WebSocket endpoint for real-time metrics (30 Hz)"""
+            await self.metrics_streamer.handle_websocket(websocket)
+
+        # UI Control WebSocket endpoint
+        @self.app.websocket("/ws/ui")
+        async def websocket_ui_control(websocket):
+            """WebSocket endpoint for real-time UI parameter control"""
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+            import time
+
+            await websocket.accept()
+            print("[Main] UI control WebSocket connected")
+
+            # Track last update time for rate limiting (10 Hz max)
+            last_update_time = 0.0
+            MIN_UPDATE_INTERVAL = 0.1  # 100ms = 10 Hz
+
+            try:
+                # Send initial parameter state
+                initial_params = self.audio_server.get_current_parameters()
+                await websocket.send_json({
+                    "type": "state",
+                    "data": initial_params
+                })
+
+                # Message loop
+                while True:
+                    # Receive message from client
+                    message = await websocket.receive_text()
+                    data = json.loads(message)
+
+                    msg_type = data.get("type")
+
+                    if msg_type == "set_param":
+                        # Rate limiting check
+                        current_time = time.time()
+                        if current_time - last_update_time < MIN_UPDATE_INTERVAL:
+                            # Skip update if too frequent
+                            continue
+
+                        # Extract parameters
+                        param_type = data.get("param_type", "channel")  # 'channel', 'global', 'phi'
+                        channel = data.get("channel")  # None for global params
+                        param_name = data.get("param")
+                        value = data.get("value")
+
+                        # Update parameter
+                        success = self.audio_server.update_parameter(
+                            param_type=param_type,
+                            channel=channel,
+                            param_name=param_name,
+                            value=value
+                        )
+
+                        # Send acknowledgment
+                        await websocket.send_json({
+                            "type": "param_updated",
+                            "success": success,
+                            "param_type": param_type,
+                            "channel": channel,
+                            "param": param_name,
+                            "value": value
+                        })
+
+                        last_update_time = current_time
+
+                    elif msg_type == "get_state":
+                        # Send current parameter state
+                        current_params = self.audio_server.get_current_parameters()
+                        await websocket.send_json({
+                            "type": "state",
+                            "parameters": current_params
+                        })
+
+                    elif msg_type == "apply_preset":
+                        # Apply entire preset (Feature 010: Preset Browser)
+                        preset_name = data.get("preset_name", "Unknown")
+                        parameters = data.get("parameters", {})
+
+                        try:
+                            # Apply all channel parameters
+                            if "channels" in parameters:
+                                for channel_idx, channel_params in parameters["channels"].items():
+                                    channel_idx = int(channel_idx)
+                                    for param_name, value in channel_params.items():
+                                        self.audio_server.update_parameter(
+                                            param_type="channel",
+                                            channel=channel_idx,
+                                            param_name=param_name,
+                                            value=value
+                                        )
+
+                            # Apply global parameters
+                            if "global" in parameters:
+                                for param_name, value in parameters["global"].items():
+                                    self.audio_server.update_parameter(
+                                        param_type="global",
+                                        channel=None,
+                                        param_name=param_name,
+                                        value=value
+                                    )
+
+                            # Apply phi parameters
+                            if "phi" in parameters:
+                                for param_name, value in parameters["phi"].items():
+                                    self.audio_server.update_parameter(
+                                        param_type="phi",
+                                        channel=None,
+                                        param_name=param_name,
+                                        value=value
+                                    )
+
+                            # Send confirmation
+                            await websocket.send_json({
+                                "type": "preset_applied",
+                                "success": True,
+                                "preset_name": preset_name
+                            })
+
+                            print(f"[Main] Applied preset: {preset_name}")
+
+                        except Exception as e:
+                            print(f"[Main] Error applying preset: {e}")
+                            await websocket.send_json({
+                                "type": "preset_applied",
+                                "success": False,
+                                "preset_name": preset_name,
+                                "error": str(e)
+                            })
+
+                    elif msg_type == "ping":
+                        # Respond to ping
+                        await websocket.send_json({"type": "pong"})
+
+            except WebSocketDisconnect:
+                print("[Main] UI control WebSocket disconnected")
+            except Exception as e:
+                print(f"[Main] UI control WebSocket error: {e}")
+                import traceback
+                traceback.print_exc()
+
+        # Playback WebSocket endpoint (Feature 018)
+        @self.app.websocket("/ws/playback")
+        async def websocket_playback(websocket):
+            """WebSocket endpoint for playback frame streaming (FR-004)"""
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+
+            if not self.timeline_player:
+                await websocket.close(code=1000, reason="Timeline Player not enabled")
+                return
+
+            await websocket.accept()
+            print("[Main] Playback WebSocket connected")
+
+            # List to store this client's frames
+            playback_frames = []
+            playback_lock = asyncio.Lock()
+
+            # Define frame callback to capture frames for this client
+            def frame_callback(frame):
+                """Capture playback frames for streaming"""
+                async def enqueue():
+                    async with playback_lock:
+                        playback_frames.append(frame)
+                        # Keep only last 10 frames to prevent memory buildup
+                        if len(playback_frames) > 10:
+                            playback_frames.pop(0)
+
+                asyncio.create_task(enqueue())
+
+            # Set the frame callback
+            old_callback = self.timeline_player.frame_callback
+            self.timeline_player.frame_callback = frame_callback
+
+            try:
+                # Stream frames to client
+                while True:
+                    # Check if there are frames to send
+                    async with playback_lock:
+                        if playback_frames:
+                            frame = playback_frames.pop(0)
+                            await websocket.send_json(frame)
+                        else:
+                            # No frames, wait a bit
+                            await asyncio.sleep(0.033)  # ~30 Hz
+
+            except WebSocketDisconnect:
+                print("[Main] Playback WebSocket disconnected")
+            except Exception as e:
+                print(f"[Main] Playback WebSocket error: {e}")
+                import traceback
+                traceback.print_exc()
+            finally:
+                # Restore old callback
+                self.timeline_player.frame_callback = old_callback
+
+        # Node Sync WebSocket endpoint (Feature 020)
+        @self.app.websocket("/ws/sync")
+        async def websocket_node_sync(websocket):
+            """WebSocket endpoint for node synchronization (FR-003)"""
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+
+            if not self.node_sync:
+                await websocket.close(code=1000, reason="Node Synchronizer not enabled")
+                return
+
+            await websocket.accept()
+            print("[Main] Node sync WebSocket connected")
+
+            # Determine if this is master or client
+            if self.node_sync.role == NodeRole.MASTER:
+                # Master mode: broadcast sync frames to this client
+                client_id = None
+
+                try:
+                    # Wait for client registration
+                    data = await websocket.receive_json()
+                    if data.get("type") == "register":
+                        client_id = data.get("node_id", f"client_{int(time.time() * 1000)}")
+                        client_info = data.get("info", {})
+                        self.node_sync.register_client(client_id, client_info)
+
+                        # Send confirmation
+                        await websocket.send_json({
+                            "type": "registered",
+                            "node_id": client_id
+                        })
+
+                        # Broadcast loop: send sync frames to this client
+                        while True:
+                            # Get current state from auto-phi learner
+                            if self.auto_phi_learner:
+                                phi_phase = self.auto_phi_learner.state.phi_phase
+                                phi_depth = self.auto_phi_learner.state.phi_depth
+                                criticality = self.auto_phi_learner.state.criticality
+                                coherence = self.auto_phi_learner.state.coherence
+                                ici = getattr(self.audio_server, 'last_ici', 0.0)
+
+                                # Process and broadcast
+                                self.node_sync.process_local_state(
+                                    phi_phase, phi_depth, criticality, coherence, ici
+                                )
+
+                                # Send sync frame
+                                frame_data = {
+                                    "type": "sync_frame",
+                                    "phi_phase": phi_phase,
+                                    "phi_depth": phi_depth,
+                                    "criticality": criticality,
+                                    "coherence": coherence,
+                                    "ici": ici,
+                                    "timestamp": time.time(),
+                                    "master_timestamp": time.time(),
+                                    "sequence": self.node_sync.sequence_counter - 1
+                                }
+
+                                await websocket.send_json(frame_data)
+
+                            # Wait for next sync interval (30 Hz)
+                            await asyncio.sleep(1.0 / self.node_sync.config.sync_rate)
+
+                except WebSocketDisconnect:
+                    print(f"[Main] Node sync WebSocket disconnected (client={client_id})")
+                    if client_id:
+                        self.node_sync.unregister_client(client_id)
+
+            else:
+                # Client mode: receive sync frames from master
+                try:
+                    # Register with master
+                    await websocket.send_json({
+                        "type": "register",
+                        "node_id": self.node_sync.node_id,
+                        "info": {}
+                    })
+
+                    # Receive loop
+                    while True:
+                        data = await websocket.receive_json()
+
+                        if data.get("type") == "sync_frame":
+                            # Process received frame
+                            await self.node_sync.receive_sync_frame(data)
+
+                            # Apply sync frame to local state
+                            if self.auto_phi_learner:
+                                # Update phi parameters from master
+                                self.audio_server.update_parameter(
+                                    param_type='phi',
+                                    channel=None,
+                                    param_name='depth',
+                                    value=data['phi_depth']
+                                )
+                                self.audio_server.update_parameter(
+                                    param_type='phi',
+                                    channel=None,
+                                    param_name='phase',
+                                    value=data['phi_phase']
+                                )
+
+                except WebSocketDisconnect:
+                    print("[Main] Node sync WebSocket disconnected from master")
+
+        # Cluster Monitor WebSocket endpoint (Feature 022)
+        @self.app.websocket("/ws/cluster")
+        async def websocket_cluster(websocket):
+            """WebSocket endpoint for cluster monitoring (FR-004, SC-001)"""
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+
+            if not self.cluster_monitor:
+                await websocket.close(code=1000, reason="Cluster Monitor not enabled")
+                return
+
+            await websocket.accept()
+            print("[Main] Cluster monitor WebSocket connected")
+
+            # Add client to cluster monitor's client list
+            with self.cluster_monitor.ws_client_lock:
+                self.cluster_monitor.ws_clients.append(websocket)
+
+            try:
+                # Broadcast loop: send cluster updates at configured interval
+                while True:
+                    # Get current cluster state
+                    update = {
+                        "type": "cluster_update",
+                        "timestamp": time.time(),
+                        "nodes": self.cluster_monitor.get_nodes_list(),
+                        "stats": self.cluster_monitor.get_statistics()
+                    }
+
+                    await websocket.send_json(update)
+
+                    # Wait for next update interval (1-2 Hz)
+                    await asyncio.sleep(self.cluster_monitor.config.update_interval)
+
+            except WebSocketDisconnect:
+                print("[Main] Cluster monitor WebSocket disconnected")
+            except Exception as e:
+                print(f"[Main] Cluster monitor WebSocket error: {e}")
+            finally:
+                # Remove client from cluster monitor's client list
+                with self.cluster_monitor.ws_client_lock:
+                    if websocket in self.cluster_monitor.ws_clients:
+                        self.cluster_monitor.ws_clients.remove(websocket)
+
+        # Hybrid Node WebSocket endpoint (Feature 025)
+        @self.app.websocket("/ws/hybrid")
+        async def websocket_hybrid(websocket):
+            """WebSocket endpoint for hybrid node Φ and gain control (FR-004)"""
+            from fastapi import WebSocket, WebSocketDisconnect
+            import json
+
+            if not self.hybrid_node:
+                await websocket.close(code=1000, reason="Hybrid Node not enabled")
+                return
+
+            await websocket.accept()
+            print("[Main] Hybrid node WebSocket connected")
+
+            # Track metrics streaming
+            hybrid_metrics_queue = asyncio.Queue(maxsize=100)
+
+            # Register callback for this client
+            def metrics_callback(metrics: HybridMetrics):
+                try:
+                    hybrid_metrics_queue.put_nowait({
+                        "type": "hybrid_metrics",
+                        "timestamp": metrics.timestamp,
+                        "ici": metrics.ici,
+                        "phase_coherence": metrics.phase_coherence,
+                        "spectral_centroid": metrics.spectral_centroid,
+                        "consciousness_level": metrics.consciousness_level,
+                        "phi_phase": metrics.phi_phase,
+                        "phi_depth": metrics.phi_depth,
+                        "cpu_load": metrics.cpu_load,
+                        "latency_ms": metrics.latency_ms,
+                        "dropouts": metrics.dropouts
+                    })
+                except asyncio.QueueFull:
+                    pass  # Drop if queue full
+
+            self.hybrid_node.register_metrics_callback(metrics_callback)
+
+            try:
+                # Bidirectional communication loop
+                while True:
+                    # Create tasks for both sending and receiving
+                    receive_task = asyncio.create_task(websocket.receive_json())
+                    send_task = asyncio.create_task(hybrid_metrics_queue.get())
+
+                    # Wait for either task to complete
+                    done, pending = await asyncio.wait(
+                        {receive_task, send_task},
+                        return_when=asyncio.FIRST_COMPLETED
+                    )
+
+                    # Cancel pending tasks
+                    for task in pending:
+                        task.cancel()
+
+                    # Handle received message (Φ control commands)
+                    if receive_task in done:
+                        try:
+                            data = receive_task.result()
+                            msg_type = data.get("type")
+
+                            if msg_type == "set_phi_source":
+                                # Change Φ source
+                                source = data.get("source")
+                                try:
+                                    phi_source = PhiSource(source)
+                                    self.hybrid_node.set_phi_source(phi_source)
+                                    await websocket.send_json({
+                                        "type": "phi_source_changed",
+                                        "source": source,
+                                        "ok": True
+                                    })
+                                except ValueError:
+                                    await websocket.send_json({
+                                        "type": "error",
+                                        "message": f"Invalid Φ source: {source}",
+                                        "ok": False
+                                    })
+
+                            elif msg_type == "set_phi_manual":
+                                # Set manual Φ values (SC-002: latency < 2ms)
+                                phase = data.get("phase", 0.0)
+                                depth = data.get("depth", 0.5)
+                                self.hybrid_node.set_phi_manual(phase, depth)
+                                await websocket.send_json({
+                                    "type": "phi_manual_updated",
+                                    "phase": phase,
+                                    "depth": depth,
+                                    "ok": True
+                                })
+
+                            elif msg_type == "ping":
+                                # Ping response
+                                await websocket.send_json({"type": "pong"})
+
+                        except Exception as e:
+                            print(f"[Main] Hybrid WebSocket receive error: {e}")
+
+                    # Handle metrics to send
+                    if send_task in done:
+                        try:
+                            metrics_data = send_task.result()
+                            await websocket.send_json(metrics_data)
+                        except Exception as e:
+                            print(f"[Main] Hybrid WebSocket send error: {e}")
+
+            except WebSocketDisconnect:
+                print("[Main] Hybrid node WebSocket disconnected")
+            except Exception as e:
+                print(f"[Main] Hybrid node WebSocket error: {e}")
+            finally:
+                # Unregister callback
+                self.hybrid_node.unregister_metrics_callback(metrics_callback)
+
+    def _mount_static_files(self):
+        """Mount static file directories"""
+        # Mount frontend files if they exist
+        frontend_dir = Path(__file__).parent.parent
+
+        if (frontend_dir / "static").exists():
+            self.app.mount("/static", StaticFiles(directory=str(frontend_dir / "static")), name="static")
+
+    async def startup(self):
+        """Server startup tasks"""
+        print("\n" + "=" * 60)
+        print("STARTING SOUNDLAB SERVER")
+        print("=" * 60)
+
+        # Start metrics streamer
+        print("\n[Main] Starting metrics streamer...")
+        await self.metrics_streamer.start()
+
+        # Start latency streamer
+        if self.latency_streamer:
+            print("[Main] Starting latency streamer...")
+            await self.latency_streamer.start()
+
+        # Start cluster monitor (Feature 022)
+        if self.cluster_monitor:
+            print("[Main] Starting cluster monitor...")
+            self.cluster_monitor.start()
+            if self.cluster_monitor.config.enable_rbac:
+                print(f"[Main] Cluster Monitor Admin Token: {self.cluster_monitor.admin_token}")
+
+        print("\n[Main] ✓ All services started")
+        print("\n" + "=" * 60)
+        print(f"Server running at: http://{self.host}:{self.port}")
+        print(f"API docs: http://{self.host}:{self.port}/docs")
+        print("=" * 60)
+        print("\nEndpoints:")
+        print("  GET  /                              - Frontend UI")
+        print("  GET  /api/status                    - Server status")
+        print("  POST /api/audio/start               - Start audio processing")
+        print("  POST /api/audio/stop                - Stop audio processing")
+        print("  GET  /api/audio/performance         - Performance metrics")
+        print("  POST /api/preset/apply              - Apply preset")
+        print("")
+        print("  GET  /api/presets                   - List presets")
+        print("  GET  /api/presets/{id}              - Get preset")
+        print("  POST /api/presets                   - Create preset")
+        print("  PUT  /api/presets/{id}              - Update preset")
+        print("  DELETE /api/presets/{id}            - Delete preset")
+        print("  POST /api/presets/export            - Export all presets")
+        print("  POST /api/presets/import            - Import preset bundle")
+        print("  POST /api/presets/ab/store/{A|B}    - Store A/B snapshot")
+        print("  POST /api/presets/ab/toggle         - Toggle A/B")
+        print("")
+        print("  GET  /api/latency/current           - Current latency")
+        print("  GET  /api/latency/stats             - Latency statistics")
+        print("  POST /api/latency/calibrate         - Run calibration")
+        print("  POST /api/latency/compensation/set  - Set compensation")
+        print("")
+        print("  WS   /ws/metrics                    - Metrics stream (30 Hz)")
+        print("  WS   /ws/latency                    - Latency stream (10 Hz)")
+        print("  WS   /ws/ui                         - UI control (bidirectional)")
+        print("=" * 60)
+        print("\nPress Ctrl+C to stop server")
+        print("=" * 60)
+
+    async def shutdown(self):
+        """Server shutdown tasks"""
+        if self.is_shutting_down:
+            return
 
+        self.is_shutting_down = True
+
+        print("\n" + "=" * 60)
+        print("SHUTTING DOWN SOUNDLAB SERVER")
+        print("=" * 60)
+
+        # Stop audio server
+        if self.audio_server.is_running:
+            print("\n[Main] Stopping audio server...")
+            self.audio_server.stop()
+
+        # Stop metrics streamer
+        print("[Main] Stopping metrics streamer...")
+        await self.metrics_streamer.stop()
+
+        # Stop latency streamer
+        if self.latency_streamer:
+            print("[Main] Stopping latency streamer...")
+            await self.latency_streamer.stop()
+
+        # Stop cluster monitor (Feature 022)
+        if self.cluster_monitor:
+            print("[Main] Stopping cluster monitor...")
+            self.cluster_monitor.stop()
+
+        print("\n[Main] ✓ Shutdown complete")
+        print("=" * 60)
+
+    def run(self, auto_start_audio: bool = False, calibrate_on_start: bool = False):
+        """
+        Run the server
+
+        Args:
+            auto_start_audio: Automatically start audio processing
+            calibrate_on_start: Run latency calibration on startup
+        """
+
+        # Setup signal handlers for graceful shutdown
+        def signal_handler(sig, frame):
+            print("\n[Main] Interrupt received, shutting down...")
+            asyncio.run(self.shutdown())
+            sys.exit(0)
+
+        signal.signal(signal.SIGINT, signal_handler)
+        signal.signal(signal.SIGTERM, signal_handler)
+
+        # Register startup/shutdown events
+        @self.app.on_event("startup")
+        async def on_startup():
+            await self.startup()
+
+            # Auto-start audio if requested
+            if auto_start_audio:
+                print("\n[Main] Auto-starting audio processing...")
+                self.audio_server.start(calibrate_latency=calibrate_on_start)
+
+        @self.app.on_event("shutdown")
+        async def on_shutdown():
+            await self.shutdown()
+
+        # Run server with uvicorn
+        uvicorn.run(
+            self.app,
+            host=self.host,
+            port=self.port,
+            log_level="info"
+        )
+
+
+def main():
+    """Main entry point"""
+    import argparse
+
+    parser = argparse.ArgumentParser(description="Soundlab Audio Server")
+    parser.add_argument("--host", default="0.0.0.0", help="Server host address")
+    parser.add_argument("--port", type=int, default=8000, help="Server port")
+    parser.add_argument("--input-device", type=int, default=None, help="Audio input device index")
+    parser.add_argument("--output-device", type=int, default=None, help="Audio output device index")
+    parser.add_argument("--auto-start-audio", action="store_true", help="Automatically start audio processing")
+    parser.add_argument("--calibrate", action="store_true", help="Run latency calibration on startup")
+    parser.add_argument("--no-logging", action="store_true", help="Disable metrics/latency logging")
+    parser.add_argument("--enable-auto-phi", action="store_true", help="Enable Auto-Phi Learner (adaptive criticality control)")
+    parser.add_argument("--enable-criticality-balancer", action="store_true", help="Enable Criticality Balancer (adaptive coupling and amplitude)")
+    parser.add_argument("--enable-state-memory", action="store_true", help="Enable State Memory (temporal memory and prediction)")
+    parser.add_argument("--enable-state-classifier", action="store_true", help="Enable State Classifier (consciousness state classification)")
+    parser.add_argument("--enable-predictive-model", action="store_true", help="Enable Predictive Model (next-state forecasting)")
+    parser.add_argument("--disable-session-recorder", action="store_true", help="Disable Session Recorder (recording enabled by default)")
+    parser.add_argument("--disable-timeline-player", action="store_true", help="Disable Timeline Player (playback enabled by default)")
+    parser.add_argument("--disable-data-exporter", action="store_true", help="Disable Data Exporter (export enabled by default)")
+    parser.add_argument("--enable-node-sync", action="store_true", help="Enable Node Synchronizer (distributed phase-locked operation)")
+    parser.add_argument("--node-sync-role", default="master", choices=["master", "client"], help="Node role: master (authority) or client (subscriber)")
+    parser.add_argument("--node-sync-master-url", help="Master WebSocket URL for client nodes (e.g., ws://192.168.1.100:8000/ws/sync)")
+    parser.add_argument("--enable-phasenet", action="store_true", help="Enable PhaseNet Protocol (mesh network for distributed sync)")
+    parser.add_argument("--phasenet-port", type=int, default=9000, help="PhaseNet UDP port (default: 9000)")
+    parser.add_argument("--phasenet-key", help="PhaseNet encryption key (enables AES-128 encryption)")
+    parser.add_argument("--enable-cluster-monitor", action="store_true", help="Enable Cluster Monitor (centralized node monitoring and management)")
+    parser.add_argument("--enable-hardware-bridge", action="store_true", help="Enable Hardware I²S Bridge (sync with embedded microcontrollers via serial)")
+    parser.add_argument("--hardware-port", help="Serial port for hardware bridge (auto-detect if not specified)")
+    parser.add_argument("--hardware-baudrate", type=int, default=115200, help="Serial baudrate for hardware bridge (default: 115200)")
+    parser.add_argument("--enable-hybrid-bridge", action="store_true", help="Enable Hybrid Analog-DSP Bridge (analog signal processing with DSP analysis)")
+    parser.add_argument("--hybrid-port", help="Serial port for hybrid bridge (auto-detect if not specified)")
+    parser.add_argument("--hybrid-baudrate", type=int, default=115200, help="Serial baudrate for hybrid bridge (default: 115200)")
+    parser.add_argument("--enable-hybrid-node", action="store_true", help="Enable Hybrid Node Integration (Feature 025: analog I/O with D-ASE engine)")
+    parser.add_argument("--hybrid-node-input-device", type=int, help="Audio input device index for hybrid node (auto-detect if not specified)")
+    parser.add_argument("--hybrid-node-output-device", type=int, help="Audio output device index for hybrid node (auto-detect if not specified)")
+    parser.add_argument("--list-devices", action="store_true", help="List available audio devices and exit")
+
+    args = parser.parse_args()
+
+    # List devices if requested
+    if args.list_devices:
+        import sounddevice as sd
+        print("\n" + "=" * 60)
+        print("Available Audio Devices")
+        print("=" * 60)
+        print(sd.query_devices())
+        print("\nUse --input-device and --output-device with device index")
+        return
+
+    # Create and run server
+    server = SoundlabServer(
+        host=args.host,
+        port=args.port,
+        audio_input_device=args.input_device,
+        audio_output_device=args.output_device,
+        enable_logging=not args.no_logging,
+        enable_auto_phi=args.enable_auto_phi,
+        enable_criticality_balancer=args.enable_criticality_balancer,
+        enable_state_memory=args.enable_state_memory,
+        enable_state_classifier=args.enable_state_classifier,
+        enable_predictive_model=args.enable_predictive_model,
+        enable_session_recorder=not args.disable_session_recorder,
+        enable_timeline_player=not args.disable_timeline_player,
+        enable_data_exporter=not args.disable_data_exporter,
+        enable_node_sync=args.enable_node_sync,
+        node_sync_role=args.node_sync_role,
+        node_sync_master_url=args.node_sync_master_url,
+        enable_phasenet=args.enable_phasenet,
+        phasenet_port=args.phasenet_port,
+        phasenet_key=args.phasenet_key,
+        enable_cluster_monitor=args.enable_cluster_monitor,
+        enable_hardware_bridge=args.enable_hardware_bridge,
+        hardware_port=args.hardware_port,
+        hardware_baudrate=args.hardware_baudrate,
+        enable_hybrid_bridge=args.enable_hybrid_bridge,
+        hybrid_port=args.hybrid_port,
+        hybrid_baudrate=args.hybrid_baudrate,
+        enable_hybrid_node=args.enable_hybrid_node,
+        hybrid_node_input_device=args.hybrid_node_input_device,
+        hybrid_node_output_device=args.hybrid_node_output_device
+    )
+
+    server.run(
+        auto_start_audio=args.auto_start_audio,
+        calibrate_on_start=args.calibrate
+    )
 
-# ============================================================
-# MAIN ENTRY POINT
-# ============================================================
 
 if __name__ == "__main__":
-    import uvicorn
-    uvicorn.run(
-        "server.main,
-        host="127.0.0.1",
-        port=8000,
-        reload=True,
-        log_level="info"
-
-# ============================================================
-# END OF FILE
-# ============================================================
-
-"""  # auto-closed missing docstring
+    main()
diff --git a/server/metrics_computer.py b/server/metrics_computer.py
index cedbb4cebf4db1925101f1889636f61ccc440fbc..669ceebe40d27055cbbacfb7cf463d2ece6508ef 100644
--- a/server/metrics_computer.py
+++ b/server/metrics_computer.py
@@ -1,297 +1,400 @@
 """
 MetricsComputer - Unified Metrics Facade
 
 This module provides a unified interface for computing all Soundlab + D-ASE metrics
-by aggregating functionality from)
-
-
-Created)
-Purpose)
+by aggregating functionality from:
+- ICIEngine (Integrated Chromatic Information)
+- ChromaticFieldProcessor (D-ASE chromatic field metrics)
+- StateClassifierGraph (consciousness state classification)
 
+Created: 2025-10-17 (Priority 1 Remediation)
+Purpose: Fix validate_soundlab_v1_final.py import errors
+"""
 
 import numpy as np
 from typing import Dict, Optional
 
 from .ici_engine import IntegratedChromaticInformation, ICIConfig
 from .chromatic_field_processor import ChromaticFieldProcessor
 from .state_classifier import StateClassifierGraph, StateClassifierConfig
 
 
 class MetricsComputer:
     """
     Unified metrics computation facade
 
-    This class aggregates metrics from multiple computation engines)
+    This class aggregates metrics from multiple computation engines:
+    - ICI (Integrated Chromatic Information)
     - Phase coherence
     - Spectral centroid
     - Consciousness state classification
     - Chromatic field metrics
 
     The class can operate in two modes:
     1. Live mode: Process real audio buffers
-    2. Validation mode, enable_logging: bool, validation_mode: bool) :
+    2. Validation mode: Generate synthetic metrics for testing
+    """
+
+    def __init__(self, enable_logging: bool = False, validation_mode: bool = True):
         """
         Initialize MetricsComputer
 
         Args:
             enable_logging: Enable metrics/performance logging
-            validation_mode, returns synthetic metrics without audio processing
+            validation_mode: If True, returns synthetic metrics without audio processing
         """
         self.enable_logging = enable_logging
         self.validation_mode = validation_mode
 
         # Initialize computation engines
         if not validation_mode:
-            # Live mode,
+            # Live mode: initialize all engines
+            ici_config = ICIConfig(
+                num_channels=8,
                 fft_size=512,
                 smoothing_alpha=0.2,
                 enable_logging=enable_logging
-
+            )
             self.ici_engine = IntegratedChromaticInformation(ici_config)
 
             state_config = StateClassifierConfig(
                 enable_logging=enable_logging
-
+            )
             self.state_classifier = StateClassifierGraph(state_config)
 
             self.chromatic_processor = ChromaticFieldProcessor()
         else:
             # Validation mode: no initialization needed
             self.ici_engine = None
             self.state_classifier = None
             self.chromatic_processor = None
 
         # Current metrics cache
         self._cached_metrics: Dict = {}
 
-        if enable_logging)
-            logger.info("[MetricsComputer]   validation_mode=%s", validation_mode)
+        if enable_logging:
+            print("[MetricsComputer] Initialized")
+            print(f"[MetricsComputer]   validation_mode={validation_mode}")
 
-    @lru_cache(maxsize=128)
-    def compute_all(self, audio_buffer) :
+    def compute_all(self, audio_buffer: Optional[np.ndarray] = None) -> Dict:
         """
         Compute all metrics
 
         Args:
-            audio_buffer, buffer_size)
+            audio_buffer: Optional audio buffer (num_channels, buffer_size)
                          If None, returns cached or synthetic metrics
 
         Returns:
             Dictionary with all computed metrics:
             {
-                'ici',                    # Integrated Chromatic Information [0, 1]
-                'phase_coherence',        # Phase coherence [0, 1]
-                'spectral_centroid',      # Spectral centroid in Hz
-                'consciousness_state',      # Current state (COMA, SLEEP, etc.)
-                'criticality',            # System criticality [0, 1]
-                'chromatic_energy',       # Chromatic field energy
+                'ici': float,                    # Integrated Chromatic Information [0, 1]
+                'phase_coherence': float,        # Phase coherence [0, 1]
+                'spectral_centroid': float,      # Spectral centroid in Hz
+                'consciousness_state': str,      # Current state (COMA, SLEEP, etc.)
+                'criticality': float,            # System criticality [0, 1]
+                'chromatic_energy': float,       # Chromatic field energy
                 'valid': bool                    # Metrics validity flag
             }
         """
-        if self.validation_mode or audio_buffer is None)
+        if self.validation_mode or audio_buffer is None:
+            # Return synthetic metrics for validation
+            return self._generate_synthetic_metrics()
 
-        # Live mode)
+        # Live mode: compute real metrics
+        return self._compute_live_metrics(audio_buffer)
 
-    def _generate_synthetic_metrics(self) :
+    def _generate_synthetic_metrics(self) -> Dict:
         """
         Generate synthetic metrics for validation/testing
 
         Returns:
             Dictionary with synthetic metric values
         """
         # Generate realistic synthetic values
         metrics = {
-            'ici',                      # Mid-range ICI
-            'phase_coherence',          # Good coherence
-            'spectral_centroid',      # Mid-frequency centroid
-            'consciousness_state',   # Default awake state
-            'criticality',              # Near-critical
-            'chromatic_energy',         # Moderate energy
-            'valid')
-    def _compute_live_metrics(self, audio_buffer) :
+            'ici': 0.65,                      # Mid-range ICI
+            'phase_coherence': 0.72,          # Good coherence
+            'spectral_centroid': 2500.0,      # Mid-frequency centroid
+            'consciousness_state': 'AWAKE',   # Default awake state
+            'criticality': 0.58,              # Near-critical
+            'chromatic_energy': 0.45,         # Moderate energy
+            'valid': True
+        }
+
+        self._cached_metrics = metrics
+        return metrics
+
+    def _compute_live_metrics(self, audio_buffer: np.ndarray) -> Dict:
         """
         Compute real metrics from audio buffer
 
         Args:
-            audio_buffer, buffer_size)
+            audio_buffer: Audio buffer (num_channels, buffer_size)
 
         Returns:
             Dictionary with computed metrics
         """
-        try, ici_matrix = self.ici_engine.process_block(audio_buffer)
+        try:
+            # 1. Compute ICI and phase coherence
+            ici_value, ici_matrix = self.ici_engine.process_block(audio_buffer)
 
             # 2. Compute spectral centroid (simplified)
             spectral_centroid = self._compute_spectral_centroid(audio_buffer)
 
             # 3. Get phase coherence from ICI matrix
             # (coherence is the normalized off-diagonal correlation)
             phase_coherence = self._extract_phase_coherence(ici_matrix)
 
             # 4. Classify consciousness state
             state_changed = self.state_classifier.classify_state(
                 ici_value, phase_coherence, spectral_centroid
-
+            )
             state_info = self.state_classifier.get_current_state()
 
             # 5. Compute criticality metric (simplified)
             # Criticality measures proximity to phase transition
             criticality = self._compute_criticality(ici_value, phase_coherence)
 
             # 6. Compute chromatic field energy (from D-ASE processor)
             chromatic_energy = self._compute_chromatic_energy(audio_buffer)
 
             # Aggregate metrics
             metrics = {
-                'ici'),
-                'phase_coherence'),
-                'spectral_centroid'),
-                'consciousness_state',
-                'criticality'),
-                'chromatic_energy'),
+                'ici': float(ici_value),
+                'phase_coherence': float(phase_coherence),
+                'spectral_centroid': float(spectral_centroid),
+                'consciousness_state': state_info['current_state'],
+                'criticality': float(criticality),
+                'chromatic_energy': float(chromatic_energy),
                 'valid': True
             }
 
             self._cached_metrics = metrics
             return metrics
 
         except Exception as e:
             if self.enable_logging:
-                logger.error("[MetricsComputer] ERROR computing metrics, e)
+                print(f"[MetricsComputer] ERROR computing metrics: {e}")
 
             # Return last valid metrics or zeros
             if self._cached_metrics:
                 return self._cached_metrics
 
             return {
-                'ici',
-                'phase_coherence',
-                'spectral_centroid',
-                'consciousness_state',
-                'criticality',
-                'chromatic_energy',
-                'valid')
-    def _compute_spectral_centroid(self, audio_buffer) :
-            audio_buffer, buffer_size)
-
-        Returns, axis=0)
+                'ici': 0.0,
+                'phase_coherence': 0.0,
+                'spectral_centroid': 0.0,
+                'consciousness_state': 'COMA',
+                'criticality': 0.0,
+                'chromatic_energy': 0.0,
+                'valid': False
+            }
+
+    def _compute_spectral_centroid(self, audio_buffer: np.ndarray) -> float:
+        """
+        Compute spectral centroid (center of mass of spectrum)
+
+        Args:
+            audio_buffer: Audio buffer (num_channels, buffer_size)
+
+        Returns:
+            Spectral centroid in Hz
+        """
+        # Average across all channels
+        avg_signal = np.mean(audio_buffer, axis=0)
 
         # Compute FFT
         spectrum = np.abs(np.fft.rfft(avg_signal))
         freqs = np.fft.rfftfreq(len(avg_signal), d=1.0/48000.0)
 
-        # Compute centroid)|) / Σ|X(f)|
-        if np.sum(spectrum) > 1e-10) / np.sum(spectrum)
-        else)
-    def _extract_phase_coherence(self, ici_matrix) :
+        # Compute centroid: Σ(f * |X(f)|) / Σ|X(f)|
+        if np.sum(spectrum) > 1e-10:
+            centroid = np.sum(freqs * spectrum) / np.sum(spectrum)
+        else:
+            centroid = 0.0
+
+        return centroid
+
+    def _extract_phase_coherence(self, ici_matrix: Optional[np.ndarray]) -> float:
         """
         Extract phase coherence from ICI matrix
 
         Args:
-            ici_matrix)
+            ici_matrix: 8x8 ICI matrix (or None)
 
-        Returns, 1]
+        Returns:
+            Phase coherence [0, 1]
         """
-        if ici_matrix is None)
+        if ici_matrix is None:
+            return 0.5
+
+        # Phase coherence is the average of off-diagonal ICI values
+        # (already normalized by ICI engine)
         n = ici_matrix.shape[0]
 
         # Sum off-diagonal elements
         total = np.sum(ici_matrix) - np.trace(ici_matrix)
 
         # Normalize by number of pairs
         num_pairs = n * (n - 1)
         coherence = total / num_pairs if num_pairs > 0 else 0.0
 
         # Map from [-1, 1] to [0, 1]
         coherence = (coherence + 1.0) / 2.0
 
         return float(np.clip(coherence, 0.0, 1.0))
 
-    @lru_cache(maxsize=128)
-    def _compute_criticality(self, ici, coherence) :
+    def _compute_criticality(self, ici: float, coherence: float) -> float:
         """
         Compute system criticality metric
 
         Criticality measures proximity to phase transition point.
         Peak criticality occurs when ICI and coherence are balanced.
 
         Args:
-            ici, 1]
-            coherence, 1]
+            ici: Integrated Chromatic Information [0, 1]
+            coherence: Phase coherence [0, 1]
 
-        Returns, 1]
+        Returns:
+            Criticality [0, 1]
         """
         # Criticality peaks when both ICI and coherence are moderate
         # (around 0.5-0.7 range indicates "edge of chaos")
         optimal_ici = 0.6
         optimal_coherence = 0.6
 
         # Distance from optimal point
         ici_dist = abs(ici - optimal_ici)
         coh_dist = abs(coherence - optimal_coherence)
 
         # Combined distance (scaled to [0, 1])
         distance = np.sqrt(ici_dist**2 + coh_dist**2) / np.sqrt(2)
 
         # Criticality is inverse of distance
         criticality = 1.0 - distance
 
         return float(np.clip(criticality, 0.0, 1.0))
 
-    @lru_cache(maxsize=128)
-    def _compute_chromatic_energy(self, audio_buffer) :
+    def _compute_chromatic_energy(self, audio_buffer: np.ndarray) -> float:
         """
         Compute chromatic field energy
 
         Args:
-            audio_buffer, buffer_size)
+            audio_buffer: Audio buffer (num_channels, buffer_size)
 
-        Returns, 1]
+        Returns:
+            Chromatic energy [0, 1]
         """
-        # Simplified chromatic energy, axis=1))
+        # Simplified chromatic energy: RMS energy across channels
+        rms_per_channel = np.sqrt(np.mean(audio_buffer**2, axis=1))
         avg_energy = np.mean(rms_per_channel)
 
         # Normalize to [0, 1] range (assuming typical audio levels)
         normalized_energy = np.clip(avg_energy * 10.0, 0.0, 1.0)
 
         return float(normalized_energy)
 
-    def get_detailed_metrics(self) :
+    def get_detailed_metrics(self) -> Dict:
         """
         Get detailed metrics including performance stats
 
+        Returns:
+            Dictionary with detailed metrics and statistics
+        """
+        detailed = self._cached_metrics.copy()
+
         if not self.validation_mode:
             # Add performance statistics from engines
-            if self.ici_engine)
+            if self.ici_engine:
+                ici_stats = self.ici_engine.get_statistics()
                 detailed['ici_stats'] = ici_stats
 
-            if self.state_classifier)
+            if self.state_classifier:
+                state_stats = self.state_classifier.get_statistics()
                 detailed['state_stats'] = state_stats
 
         return detailed
 
-    def reset(self) :
+    def reset(self):
         """Reset all computation engines"""
         if not self.validation_mode:
-            if self.ici_engine)
+            if self.ici_engine:
+                self.ici_engine.reset()
 
-            if self.state_classifier)
+            if self.state_classifier:
+                self.state_classifier.reset()
 
         self._cached_metrics.clear()
 
-        if self.enable_logging)
+        if self.enable_logging:
+            print("[MetricsComputer] Reset complete")
 
 
 # Self-test function
-@lru_cache(maxsize=128)
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test MetricsComputer"""
+    print("=" * 60)
+    print("MetricsComputer Self-Test")
+    print("=" * 60)
+
+    try:
+        print("\n1. Testing validation mode (synthetic metrics)...")
+        mc = MetricsComputer(validation_mode=True)
+
+        metrics = mc.compute_all()
+
+        assert 'ici' in metrics
+        assert 'phase_coherence' in metrics
+        assert 'spectral_centroid' in metrics
+        assert 'consciousness_state' in metrics
+        assert 'criticality' in metrics
+        assert 'chromatic_energy' in metrics
+        assert metrics['valid'] == True
+
+        print(f"   ✓ Metrics computed: {', '.join(metrics.keys())}")
+        print(f"   ✓ ICI: {metrics['ici']:.3f}")
+        print(f"   ✓ State: {metrics['consciousness_state']}")
+
+        print("\n2. Testing live mode...")
+        mc_live = MetricsComputer(validation_mode=False, enable_logging=False)
+
+        # Create synthetic audio buffer
+        audio_buffer = np.random.randn(8, 512) * 0.1
+
+        metrics_live = mc_live.compute_all(audio_buffer)
+
+        assert 'ici' in metrics_live
+        assert 0.0 <= metrics_live['ici'] <= 1.0
+        assert 0.0 <= metrics_live['phase_coherence'] <= 1.0
+        assert metrics_live['spectral_centroid'] >= 0.0
+
+        print(f"   ✓ Live metrics computed")
+        print(f"   ✓ ICI: {metrics_live['ici']:.3f}")
+        print(f"   ✓ Coherence: {metrics_live['phase_coherence']:.3f}")
+        print(f"   ✓ Centroid: {metrics_live['spectral_centroid']:.1f} Hz")
+
+        print("\n3. Testing detailed metrics...")
+        detailed = mc.get_detailed_metrics()
+
+        assert len(detailed) >= len(metrics)
+        print(f"   ✓ Detailed metrics: {len(detailed)} fields")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-    if not success)
+if __name__ == "__main__":
+    success = _self_test()
 
-"""  # auto-closed missing docstring
+    if not success:
+        import sys
+        sys.exit(1)
diff --git a/server/metrics_frame.py b/server/metrics_frame.py
index d4023f630618001977b1afc8390badb18b0adf35..ab4748e7f78d45985de7586a058b9b499ccbe7e7 100644
--- a/server/metrics_frame.py
+++ b/server/metrics_frame.py
@@ -1,239 +1,332 @@
 """
 MetricsFrame - Real-Time D-ASE Metrics Data Structure
 
 Represents a single snapshot of D-ASE consciousness metrics synchronized
 with audio processing blocks and Φ-modulation state.
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import time
 from typing import Optional, Dict, Literal
 from dataclasses import dataclass, asdict
 import json
 
 
 StateType = Literal["AWAKE", "DREAMING", "DEEP_SLEEP", "REM", "TRANSITION", "CRITICAL", "IDLE"]
 
 
 @dataclass
 class MetricsFrame:
     """
     Complete metrics snapshot for one audio processing cycle
 
     Conforms to FR-002 schema requirements
     """
 
     # Timestamps
-    timestamp)
+    timestamp: float  # Unix timestamp (seconds since epoch)
     audio_callback_time: Optional[float] = None  # Audio callback start time
 
     # D-ASE Core Metrics
-    ici, 1]
-    phase_coherence, 1]
+    ici: float = 0.0  # Inter-Channel Interference [0, 1]
+    phase_coherence: float = 0.0  # Phase alignment across channels [0, 1]
     spectral_centroid: float = 0.0  # Frequency in Hz
-    criticality, 1]
-    consciousness_level, 1]
+    criticality: float = 0.0  # System criticality measure [0, 1]
+    consciousness_level: float = 0.0  # Composite consciousness metric [0, 1]
     state: StateType = "IDLE"  # Categorical consciousness state
 
     # Φ-Modulation State
-    phi_phase, 2π]
-    phi_depth, 1.618]
+    phi_phase: float = 0.0  # Φ-phase in radians [0, 2π]
+    phi_depth: float = 0.618  # Φ-depth [0, 1.618]
     phi_source: str = "internal"  # Active Φ source
 
     # Performance Metrics
     latency_ms: Optional[float] = None  # Processing latency in milliseconds
-    cpu_load, 1]
+    cpu_load: Optional[float] = None  # CPU utilization [0, 1]
 
     # Quality Indicators
-    valid)
-    frame_id) :
+    valid: bool = True  # False if any metric is invalid (NaN/Inf)
+    frame_id: Optional[int] = None  # Sequential frame number
+
+    def to_dict(self) -> Dict:
+        """Convert to dictionary for JSON serialization"""
+        return asdict(self)
+
+    def to_json(self, pretty: bool = False) -> str:
         """
         Convert to JSON string
 
         Args:
-            pretty, format with indentation
+            pretty: If True, format with indentation
 
-        if pretty, indent=2)
-        else)
+        Returns:
+            JSON string representation
+        """
+        data = self.to_dict()
+        if pretty:
+            return json.dumps(data, indent=2)
+        else:
+            return json.dumps(data)
 
     @classmethod
-    def from_dict(cls, data) :
+    def from_dict(cls, data: Dict) -> 'MetricsFrame':
         """
         Create MetricsFrame from dictionary
 
         Args:
             data: Dictionary with metric values
 
         Returns:
             MetricsFrame instance
         """
         # Filter to only valid fields
-        valid_fields = {k, v in data.items() if k in cls.__dataclass_fields__}
+        valid_fields = {k: v for k, v in data.items() if k in cls.__dataclass_fields__}
         return cls(**valid_fields)
 
     @classmethod
-    def from_json(cls, json_str) :
+    def from_json(cls, json_str: str) -> 'MetricsFrame':
         """
         Create MetricsFrame from JSON string
 
         Args:
             json_str: JSON representation
 
+        Returns:
+            MetricsFrame instance
+        """
+        data = json.loads(json_str)
         return cls.from_dict(data)
 
-    def validate(self) :
+    def validate(self) -> bool:
+        """
+        Validate metrics for NaN/Inf values (FR-010)
+
+        Returns:
+            True if all metrics are valid
+        """
+        import math
+
+        numeric_fields = [
+            'ici', 'phase_coherence', 'spectral_centroid', 'criticality',
+            'consciousness_level', 'phi_phase', 'phi_depth'
+        ]
+
+        for field in numeric_fields:
+            value = getattr(self, field)
+            if not isinstance(value, (int, float)):
+                continue
+            if math.isnan(value) or math.isinf(value):
+                self.valid = False
+                return False
+
+        self.valid = True
+        return True
+
+    def sanitize(self) -> 'MetricsFrame':
+        """
+        Replace invalid values (NaN/Inf) with None
+
+        Returns:
+            Self (modified in place)
+        """
+        import math
+
+        numeric_fields = [
+            'ici', 'phase_coherence', 'spectral_centroid', 'criticality',
+            'consciousness_level', 'phi_phase', 'phi_depth', 'latency_ms', 'cpu_load'
+        ]
+
+        for field in numeric_fields:
+            value = getattr(self, field)
+            if isinstance(value, (int, float)):
+                if math.isnan(value) or math.isinf(value):
+                    setattr(self, field, 0.0)  # Replace with safe default
+                    self.valid = False
+
+        return self
+
+    def classify_state(self) -> StateType:
         """
         Classify consciousness state based on metrics
 
         State classification logic:
         - IDLE: consciousness < 0.1
-        - DEEP_SLEEP, coherence > 0.7
-        - DREAMING, coherence < 0.5
-        - REM, criticality > 0.7
+        - DEEP_SLEEP: consciousness < 0.3, coherence > 0.7
+        - DREAMING: consciousness 0.3-0.5, coherence < 0.5
+        - REM: consciousness 0.4-0.6, criticality > 0.7
         - AWAKE: consciousness > 0.6
         - CRITICAL: criticality > 0.9
         - TRANSITION: Otherwise
 
         Returns:
             Classified state
         """
         c = self.consciousness_level
         coh = self.phase_coherence
         crit = self.criticality
 
         if crit > 0.9:
             self.state = "CRITICAL"
         elif c < 0.1:
             self.state = "IDLE"
         elif c > 0.6:
             self.state = "AWAKE"
         elif c < 0.3 and coh > 0.7:
             self.state = "DEEP_SLEEP"
         elif 0.3 <= c < 0.5 and coh < 0.5:
             self.state = "DREAMING"
         elif 0.4 <= c < 0.6 and crit > 0.7:
             self.state = "REM"
-        else) :
+        else:
+            self.state = "TRANSITION"
+
+        return self.state
+
+    def __repr__(self) -> str:
         return (
-            f"MetricsFrame(t={self.timestamp, "
-            f"consciousness={self.consciousness_level, "
+            f"MetricsFrame(t={self.timestamp:.3f}, "
+            f"consciousness={self.consciousness_level:.2f}, "
             f"state={self.state}, "
-            f"φ={self.phi_depth:.2f}@{self.phi_phase)"
+            f"φ={self.phi_depth:.2f}@{self.phi_phase:.2f}rad)"
+        )
+
+
+def create_idle_frame() -> MetricsFrame:
+    """
+    Create an idle/paused frame
 
-@lru_cache(maxsize=128)
-def create_idle_frame() :
+    Used when engine is not processing audio
+    """
+    return MetricsFrame(
+        timestamp=time.time(),
+        state="IDLE",
+        consciousness_level=0.0,
+        valid=True
+    )
+
+
+def create_test_frame(frame_id: int = 0) -> MetricsFrame:
     """
     Create a test frame with synthetic data
 
     Args:
         frame_id: Sequential frame number
 
+    Returns:
+        Test MetricsFrame with valid synthetic metrics
+    """
+    import math
+
+    t = time.time()
     phase = (frame_id * 0.1) % (2 * math.pi)
 
     return MetricsFrame(
         timestamp=t,
         frame_id=frame_id,
         ici=0.5 + 0.2 * math.sin(phase),
         phase_coherence=0.7 + 0.2 * math.cos(phase),
         spectral_centroid=1000 + 500 * math.sin(phase * 2),
         criticality=0.6 + 0.3 * math.sin(phase * 0.5),
         consciousness_level=0.5 + 0.3 * math.sin(phase),
         phi_phase=phase,
         phi_depth=0.618 + 0.3 * math.sin(phase * 0.3),
         phi_source="internal",
         latency_ms=5.0 + 2.0 * abs(math.sin(phase)),
         cpu_load=0.4 + 0.1 * abs(math.sin(phase)),
         valid=True
+    )
 
-# Self-test function
-def _self_test())
-    logger.info("MetricsFrame Self-Test")
-    logger.info("=" * 60)
 
-    try)
+# Self-test function
+def _self_test():
+    """Test MetricsFrame functionality"""
+    print("=" * 60)
+    print("MetricsFrame Self-Test")
+    print("=" * 60)
+
+    try:
+        # Test basic creation
+        print("\n1. Creating basic frame...")
         frame = MetricsFrame(
             timestamp=time.time(),
             ici=0.5,
             phase_coherence=0.8,
             spectral_centroid=2000.0,
             consciousness_level=0.6
-
-        logger.info("   %s", frame)
-        logger.info("   ✓ Basic creation OK")
+        )
+        print(f"   {frame}")
+        print("   ✓ Basic creation OK")
 
         # Test JSON serialization
-        logger.info("\n2. Testing JSON serialization...")
+        print("\n2. Testing JSON serialization...")
         json_str = frame.to_json()
-        logger.info("   JSON length, len(json_str))
+        print(f"   JSON length: {len(json_str)} bytes")
         frame_restored = MetricsFrame.from_json(json_str)
         assert frame_restored.ici == frame.ici
-        logger.info("   ✓ JSON round-trip OK")
+        print("   ✓ JSON round-trip OK")
 
         # Test validation
-        logger.info("\n3. Testing validation...")
+        print("\n3. Testing validation...")
         import math
         invalid_frame = MetricsFrame(
             timestamp=time.time(),
             ici=math.nan,
             consciousness_level=math.inf
-
+        )
         is_valid = invalid_frame.validate()
-        logger.info("   Invalid frame detected, not is_valid)
+        print(f"   Invalid frame detected: {not is_valid}")
         assert not is_valid
-        logger.info("   ✓ Validation OK")
+        print("   ✓ Validation OK")
 
         # Test sanitization
-        logger.info("\n4. Testing sanitization...")
+        print("\n4. Testing sanitization...")
         invalid_frame.sanitize()
-        logger.info("   Sanitized ICI, invalid_frame.ici)
+        print(f"   Sanitized ICI: {invalid_frame.ici}")
         assert invalid_frame.ici == 0.0  # Should be replaced
-        logger.info("   ✓ Sanitization OK")
+        print("   ✓ Sanitization OK")
 
         # Test state classification
-        logger.info("\n5. Testing state classification...")
+        print("\n5. Testing state classification...")
         test_states = [
             (0.05, "IDLE"),
             (0.25, "DEEP_SLEEP"),
             (0.7, "AWAKE"),
         ]
 
-        for consciousness, expected_state in test_states),
+        for consciousness, expected_state in test_states:
+            frame = MetricsFrame(
+                timestamp=time.time(),
                 consciousness_level=consciousness,
                 phase_coherence=0.8,
                 criticality=0.5
-
+            )
             frame.classify_state()
-            logger.info("   consciousness=%s → %s (expected %s)", consciousness, frame.state, expected_state)
+            print(f"   consciousness={consciousness} → {frame.state} (expected {expected_state})")
 
-        logger.info("   ✓ State classification OK")
+        print("   ✓ State classification OK")
 
         # Test helper functions
-        logger.info("\n6. Testing helper functions...")
+        print("\n6. Testing helper functions...")
         idle = create_idle_frame()
-        logger.info("   Idle frame, idle.state)
+        print(f"   Idle frame: {idle.state}")
         assert idle.state == "IDLE"
 
         test = create_test_frame(frame_id=42)
-        logger.info("   Test frame #%s, test.frame_id, test)
+        print(f"   Test frame #{test.frame_id}: {test}")
         assert test.frame_id == 42
 
-        logger.info("   ✓ Helper functions OK")
+        print("   ✓ Helper functions OK")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/metrics_logger.py b/server/metrics_logger.py
index e3bb1bf145b9d3cde70c4372ec612b2cd40e3fcf..4e0f790dcc4ee9cde870677b632afc018fd20c0a 100644
--- a/server/metrics_logger.py
+++ b/server/metrics_logger.py
@@ -1,317 +1,460 @@
 """
 MetricsLogger - Session Recording and Archival
 
 Handles CSV and JSON logging of D-ASE metrics streams with rotation and compression.
 Implements FR-007 requirements for session recording.
 """
-import logging
-logger = logging.getLogger(__name__)
-
 
 import os
 import csv
 import json
 import gzip
 from typing import List, Optional
 from datetime import datetime
 from pathlib import Path
 
 from .metrics_frame import MetricsFrame
 
 
 class MetricsLogger:
     """
     Records metrics streams to CSV and JSON files
 
-    Features, JSON for replay)
+    Features:
+    - Dual-format logging (CSV for analysis, JSON for replay)
     - Automatic file rotation
     - Gzip compression for archived sessions
-
+    - Frame gap detection (FR-007)
     """
 
     def __init__(self,
-                 log_dir,
-                 session_name,
-                 enable_csv,
-                 enable_json,
-                 compress_on_close):
+                 log_dir: Optional[str] = None,
+                 session_name: Optional[str] = None,
+                 enable_csv: bool = True,
+                 enable_json: bool = True,
+                 compress_on_close: bool = True):
         """
         Initialize metrics logger
 
         Args:
-            log_dir)
-            session_name)
+            log_dir: Directory for log files (None = logs/metrics/)
+            session_name: Session identifier (None = timestamp)
             enable_csv: Enable CSV logging
             enable_json: Enable JSON logging
             compress_on_close: Compress files with gzip when session ends
         """
         # Setup logging directory
-        if log_dir is None), "..", "logs", "metrics")
+        if log_dir is None:
+            log_dir = os.path.join(os.path.dirname(__file__), "..", "logs", "metrics")
         self.log_dir = Path(log_dir)
         self.log_dir.mkdir(parents=True, exist_ok=True)
 
         # Session naming
-        if session_name is None).strftime("%Y%m%d_%H%M%S")
+        if session_name is None:
+            session_name = datetime.now().strftime("%Y%m%d_%H%M%S")
         self.session_name = session_name
 
         # Configuration
         self.enable_csv = enable_csv
         self.enable_json = enable_json
         self.compress_on_close = compress_on_close
 
         # File handles
         self.csv_file = None
         self.csv_writer = None
         self.json_file = None
 
         # Statistics
         self.frames_logged = 0
         self.last_timestamp = None
         self.gaps_detected = 0
         self.session_start_time = datetime.now()
 
         # Initialize log files
         self._init_csv()
         self._init_json()
 
-        logger.info("[MetricsLogger] Session '%s' started", session_name)
-        logger.info("[MetricsLogger] Logging to %s", self.log_dir)
+        print(f"[MetricsLogger] Session '{session_name}' started")
+        print(f"[MetricsLogger] Logging to {self.log_dir}")
 
-    def _init_csv(self) :
+    def _init_csv(self):
         """Initialize CSV log file"""
         if not self.enable_csv:
             return
 
         csv_path = self.log_dir / f"{self.session_name}_metrics.csv"
 
-        try, 'w', newline='')
+        try:
+            self.csv_file = open(csv_path, 'w', newline='')
             self.csv_writer = csv.DictWriter(
                 self.csv_file,
                 fieldnames=[
                     'timestamp', 'frame_id', 'state',
                     'ici', 'phase_coherence', 'spectral_centroid',
                     'criticality', 'consciousness_level',
                     'phi_phase', 'phi_depth', 'phi_source',
                     'latency_ms', 'cpu_load', 'valid'
                 ]
-
+            )
             self.csv_writer.writeheader()
             self.csv_file.flush()
 
-            logger.info("[MetricsLogger] CSV logging to %s", csv_path)
+            print(f"[MetricsLogger] CSV logging to {csv_path}")
 
         except Exception as e:
-            logger.error("[MetricsLogger] ERROR: Could not init CSV, e)
+            print(f"[MetricsLogger] ERROR: Could not init CSV: {e}")
             self.csv_file = None
             self.csv_writer = None
 
-    def _init_json(self) :
+    def _init_json(self):
         """Initialize JSON log file"""
         if not self.enable_json:
             return
 
         json_path = self.log_dir / f"{self.session_name}_metrics.jsonl"
 
-        try, 'w')
+        try:
+            self.json_file = open(json_path, 'w')
             # Write session header
             header = {
-                "session_name",
-                "start_time"),
-                "format_version") + '\n')
+                "session_name": self.session_name,
+                "start_time": self.session_start_time.isoformat(),
+                "format_version": "1.0"
+            }
+            self.json_file.write(json.dumps(header) + '\n')
             self.json_file.flush()
 
-            logger.info("[MetricsLogger] JSON logging to %s", json_path)
+            print(f"[MetricsLogger] JSON logging to {json_path}")
 
         except Exception as e:
-            logger.error("[MetricsLogger] ERROR: Could not init JSON, e)
+            print(f"[MetricsLogger] ERROR: Could not init JSON: {e}")
             self.json_file = None
 
-    def log_frame(self, frame: MetricsFrame) :
+    def log_frame(self, frame: MetricsFrame):
         """
         Log a single metrics frame
 
         Args:
             frame: MetricsFrame to log
         """
-        # Detect gaps (FR-007)
-        if self.last_timestamp is not None) * 1000
+        # Detect gaps (FR-007: no gaps > 50ms)
+        if self.last_timestamp is not None:
+            gap_ms = (frame.timestamp - self.last_timestamp) * 1000
             if gap_ms > 50.0:
                 self.gaps_detected += 1
-                logger.warning("[MetricsLogger] WARNING: Gap detected, gap_ms)
+                print(f"[MetricsLogger] WARNING: Gap detected: {gap_ms:.1f}ms")
 
         self.last_timestamp = frame.timestamp
 
         # Log to CSV
         if self.csv_writer and self.csv_file:
-            try)
+            try:
+                row = frame.to_dict()
                 self.csv_writer.writerow(row)
             except Exception as e:
-                logger.error("[MetricsLogger] ERROR writing CSV, e)
+                print(f"[MetricsLogger] ERROR writing CSV: {e}")
 
         # Log to JSON (JSONL format - one frame per line)
         if self.json_file:
-            try) + '\n')
+            try:
+                self.json_file.write(frame.to_json() + '\n')
             except Exception as e:
-                logger.error("[MetricsLogger] ERROR writing JSON, e)
+                print(f"[MetricsLogger] ERROR writing JSON: {e}")
 
         self.frames_logged += 1
 
         # Flush periodically (every 100 frames)
-        if self.frames_logged % 100 == 0)
+        if self.frames_logged % 100 == 0:
+            self.flush()
 
-    def log_batch(self, frames: List[MetricsFrame]) :
+    def log_batch(self, frames: List[MetricsFrame]):
         """
         Log multiple frames at once
 
         Args:
             frames: List of MetricsFrames
         """
-        for frame in frames)
+        for frame in frames:
+            self.log_frame(frame)
 
-    def flush(self) :
+    def flush(self):
         """Flush all buffers to disk"""
         if self.csv_file:
-            try)
+            try:
+                self.csv_file.flush()
             except:
                 pass
 
         if self.json_file:
-            try)
-            except) :
+            try:
+                self.json_file.flush()
+            except:
+                pass
+
+    def get_statistics(self) -> dict:
         """
         Get logging statistics
 
-        Returns) - self.session_start_time).total_seconds()
+        Returns:
+            Dictionary with session statistics
+        """
+        duration = (datetime.now() - self.session_start_time).total_seconds()
 
         return {
-            'session_name',
-            'start_time'),
-            'duration_seconds',
-            'frames_logged',
-            'gaps_detected',
-            'average_fps',
-            'csv_enabled',
-            'json_enabled') :
-            logger.warning("[MetricsLogger] WARNING, stats['gaps_detected'])
+            'session_name': self.session_name,
+            'start_time': self.session_start_time.isoformat(),
+            'duration_seconds': duration,
+            'frames_logged': self.frames_logged,
+            'gaps_detected': self.gaps_detected,
+            'average_fps': self.frames_logged / duration if duration > 0 else 0,
+            'csv_enabled': self.enable_csv,
+            'json_enabled': self.enable_json
+        }
+
+    def close(self):
+        """Close log files and optionally compress"""
+        print(f"[MetricsLogger] Closing session '{self.session_name}'")
+
+        # Get final statistics
+        stats = self.get_statistics()
+        print(f"[MetricsLogger] Logged {stats['frames_logged']} frames over {stats['duration_seconds']:.1f}s")
+        print(f"[MetricsLogger] Average: {stats['average_fps']:.1f} fps")
+        if stats['gaps_detected'] > 0:
+            print(f"[MetricsLogger] WARNING: {stats['gaps_detected']} gaps detected")
 
         # Close CSV
         if self.csv_file:
-            try)
+            try:
+                self.csv_file.close()
                 csv_path = self.log_dir / f"{self.session_name}_metrics.csv"
 
-                if self.compress_on_close)
+                if self.compress_on_close:
+                    self._compress_file(csv_path)
 
             except Exception as e:
-                logger.error("[MetricsLogger] ERROR closing CSV, e)
+                print(f"[MetricsLogger] ERROR closing CSV: {e}")
 
         # Close JSON
         if self.json_file:
             try:
                 # Write session footer
                 footer = {
-                    "session_end").isoformat(),
-                    "statistics") + '\n')
+                    "session_end": datetime.now().isoformat(),
+                    "statistics": stats
+                }
+                self.json_file.write(json.dumps(footer) + '\n')
                 self.json_file.close()
 
                 json_path = self.log_dir / f"{self.session_name}_metrics.jsonl"
 
-                if self.compress_on_close)
+                if self.compress_on_close:
+                    self._compress_file(json_path)
 
             except Exception as e:
-                logger.error("[MetricsLogger] ERROR closing JSON, e)
+                print(f"[MetricsLogger] ERROR closing JSON: {e}")
 
-        logger.info("[MetricsLogger] Session closed")
+        print(f"[MetricsLogger] Session closed")
 
-    def _compress_file(self, filepath: Path) :
+    def _compress_file(self, filepath: Path):
         """
         Compress file with gzip and remove original
 
         Args:
-            filepath))
+            filepath: Path to file to compress
+        """
+        if not filepath.exists():
+            return
+
+        gz_path = filepath.with_suffix(filepath.suffix + '.gz')
 
-        try, 'rb') as f_in, 'wb') as f_out)
+        try:
+            with open(filepath, 'rb') as f_in:
+                with gzip.open(gz_path, 'wb') as f_out:
+                    f_out.writelines(f_in)
 
             # Remove original
             filepath.unlink()
 
             original_size = filepath.stat().st_size if filepath.exists() else 0
             compressed_size = gz_path.stat().st_size
             ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
 
-            logger.info("[MetricsLogger] Compressed %s → %s (%s% reduction)", filepath.name, gz_path.name, ratio)
+            print(f"[MetricsLogger] Compressed {filepath.name} → {gz_path.name} ({ratio:.1f}% reduction)")
 
         except Exception as e:
-            logger.error("[MetricsLogger] ERROR compressing %s, filepath, e)
+            print(f"[MetricsLogger] ERROR compressing {filepath}: {e}")
 
-    def __del__(self))
+    def __del__(self):
+        """Ensure cleanup on destruction"""
+        self.close()
 
 
 class MetricsArchive:
+    """
+    Read and query archived metrics sessions
+
+    Allows loading historical data for analysis
+    """
+
+    def __init__(self, log_dir: Optional[str] = None):
         """
         Initialize archive reader
 
         Args:
             log_dir: Directory containing log files
         """
-        if log_dir is None), "..", "logs", "metrics")
+        if log_dir is None:
+            log_dir = os.path.join(os.path.dirname(__file__), "..", "logs", "metrics")
         self.log_dir = Path(log_dir)
 
-    def list_sessions(self) :
+    def list_sessions(self) -> List[dict]:
         """
         List all available sessions
 
-        Returns):
+        Returns:
+            List of session info dictionaries
+        """
+        sessions = []
+
+        if not self.log_dir.exists():
+            return sessions
+
+        # Find all CSV/JSONL files
+        for file_path in self.log_dir.glob("*_metrics.*"):
             # Skip compressed files for now
-            if file_path.suffix == '.gz', '')
+            if file_path.suffix == '.gz':
+                continue
+
+            session_name = file_path.stem.replace('_metrics', '')
             file_type = file_path.suffix[1:]  # Remove leading dot
 
             sessions.append({
-                'session_name',
-                'file_path'),
-                'file_type',
-                'size_bytes').st_size,
-                'modified_time').st_mtime)
+                'session_name': session_name,
+                'file_path': str(file_path),
+                'file_type': file_type,
+                'size_bytes': file_path.stat().st_size,
+                'modified_time': datetime.fromtimestamp(file_path.stat().st_mtime)
             })
 
-        return sorted(sessions, key=lambda x, reverse=True)
+        return sorted(sessions, key=lambda x: x['modified_time'], reverse=True)
 
-    def load_session_jsonl(self, session_name) :
+    def load_session_jsonl(self, session_name: str) -> List[MetricsFrame]:
         """
         Load metrics from JSONL file
 
         Args:
             session_name: Session identifier
 
-        Returns))
+        Returns:
+            List of MetricsFrames
+        """
+        json_path = self.log_dir / f"{session_name}_metrics.jsonl"
+
+        if not json_path.exists():
+            # Try compressed version
+            json_path = json_path.with_suffix('.jsonl.gz')
             if not json_path.exists():
-                raise FileNotFoundError(f"Session not found)
+                raise FileNotFoundError(f"Session not found: {session_name}")
 
         frames = []
 
         try:
             if json_path.suffix == '.gz':
                 open_func = gzip.open
-            else, 'rt') as f:
+            else:
+                open_func = open
+
+            with open_func(json_path, 'rt') as f:
                 for line in f:
-                    try)
+                    try:
+                        data = json.loads(line)
                         # Skip header/footer lines
-                        if 'timestamp' in data)
+                        if 'timestamp' in data:
+                            frame = MetricsFrame.from_dict(data)
                             frames.append(frame)
                     except:
                         pass  # Skip invalid lines
 
         except Exception as e:
-            logger.error("[MetricsArchive] ERROR loading session, e)
+            print(f"[MetricsArchive] ERROR loading session: {e}")
 
         return frames
 
 
 # Self-test function
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test MetricsLogger functionality"""
+    print("=" * 60)
+    print("MetricsLogger Self-Test")
+    print("=" * 60)
+
+    try:
+        from metrics_frame import create_test_frame
+        import tempfile
+        import shutil
+
+        # Create temporary directory for testing
+        temp_dir = tempfile.mkdtemp()
+        print(f"\n1. Using temp directory: {temp_dir}")
+
+        # Initialize logger
+        print("\n2. Initializing logger...")
+        logger = MetricsLogger(
+            log_dir=temp_dir,
+            session_name="test_session",
+            compress_on_close=False  # Disable for testing
+        )
+        print("   ✓ Logger initialized")
+
+        # Log some test frames
+        print("\n3. Logging test frames...")
+        for i in range(10):
+            frame = create_test_frame(frame_id=i)
+            logger.log_frame(frame)
+
+        print(f"   ✓ Logged {logger.frames_logged} frames")
+
+        # Get statistics
+        print("\n4. Checking statistics...")
+        stats = logger.get_statistics()
+        print(f"   Frames: {stats['frames_logged']}")
+        print(f"   Duration: {stats['duration_seconds']:.2f}s")
+        print(f"   FPS: {stats['average_fps']:.1f}")
+        print("   ✓ Statistics OK")
+
+        # Close logger
+        print("\n5. Closing logger...")
+        logger.close()
+        print("   ✓ Logger closed")
+
+        # Test archive reading
+        print("\n6. Testing archive reading...")
+        archive = MetricsArchive(log_dir=temp_dir)
+        sessions = archive.list_sessions()
+        print(f"   Found {len(sessions)} sessions")
+
+        if len(sessions) > 0:
+            loaded_frames = archive.load_session_jsonl("test_session")
+            print(f"   Loaded {len(loaded_frames)} frames")
+            assert len(loaded_frames) == 10
+            print("   ✓ Archive reading OK")
+
+        # Cleanup
+        print("\n7. Cleanup...")
+        shutil.rmtree(temp_dir)
+        print("   ✓ Cleanup complete")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/metrics_streamer.py b/server/metrics_streamer.py
index f26c3d2ae20a06ede2426f605546053bd67f0640..6f9a801eabfa9166f14977e9ac43f9d5a53b714c 100644
--- a/server/metrics_streamer.py
+++ b/server/metrics_streamer.py
@@ -1,312 +1,467 @@
 """
 MetricsStreamer - Real-Time WebSocket Metrics Broadcaster
 
-Implements FR-001 through FR-006)
+Implements FR-001 through FR-006:
+- WebSocket server at /ws/metrics
+- ≥30 Hz broadcast rate
+- Multi-client support (≥5 concurrent)
 - Frame buffering with <100ms latency
 - REST endpoint /api/metrics/latest
 - Graceful reconnection handling
 """
 
 import asyncio
 import json
 import time
 from typing import Set, Optional, Dict
 from collections import deque
 from fastapi import FastAPI, WebSocket, WebSocketDisconnect
 from fastapi.responses import JSONResponse
 import logging
 
 from .metrics_frame import MetricsFrame, create_idle_frame
 from .metrics_logger import MetricsLogger
 
 
 class MetricsStreamer:
     """
     WebSocket-based metrics broadcaster
 
     Manages multiple client connections and broadcasts metrics at ≥30 Hz
     """
 
     TARGET_FPS = 30  # FR-003: ≥30 Hz
     FRAME_INTERVAL = 1.0 / TARGET_FPS  # 0.033 seconds
-    MAX_BUFFER_SIZE = 2  # FR-004,
-                 enable_logging,
-                 log_dir,
-                 session_name):
+    MAX_BUFFER_SIZE = 2  # FR-004: Buffer ≤2 frames
+    MAX_CLIENTS = 10  # Support more than minimum 5
+
+    def __init__(self,
+                 enable_logging: bool = True,
+                 log_dir: Optional[str] = None,
+                 session_name: Optional[str] = None):
         """
         Initialize metrics streamer
 
         Args:
             enable_logging: Enable session logging to disk
-            log_dir)
-            session_name)
+            log_dir: Log directory (None = logs/metrics/)
+            session_name: Session identifier (None = timestamp)
         """
         # WebSocket clients
-        self.active_connections)
+        self.active_connections: Set[WebSocket] = set()
         self.connection_count = 0
 
         # Frame buffering
         self.frame_buffer = deque(maxlen=self.MAX_BUFFER_SIZE)
         self.latest_frame: Optional[MetricsFrame] = None
         self.frame_counter = 0
 
         # Broadcasting control
         self.broadcasting = False
         self.broadcast_task = None
         self.last_broadcast_time = 0.0
 
         # Session logging
         self.logger: Optional[MetricsLogger] = None
-        if enable_logging,
+        if enable_logging:
+            self.logger = MetricsLogger(
+                log_dir=log_dir,
                 session_name=session_name
+            )
 
         # Statistics
         self.stats = {
-            'total_frames_sent',
-            'total_bytes_sent',
-            'dropped_frames',
-            'avg_latency_ms',
-            'clients_connected',
-            'clients_disconnected',
+            'total_frames_sent': 0,
+            'total_bytes_sent': 0,
+            'dropped_frames': 0,
+            'avg_latency_ms': 0.0,
+            'clients_connected': 0,
+            'clients_disconnected': 0,
         }
 
         # Logging
         self.log = logging.getLogger(__name__)
         self.log.setLevel(logging.INFO)
 
-        logger.info("[MetricsStreamer] Initialized")
+        print("[MetricsStreamer] Initialized")
 
-    async def connect(self, websocket):
+    async def connect(self, websocket: WebSocket):
         """
         Connect a new WebSocket client
 
         Args:
             websocket: FastAPI WebSocket connection
 
         Raises:
-            RuntimeError) >= self.MAX_CLIENTS, reason="Max clients reached")
+            RuntimeError: If max clients exceeded
+        """
+        if len(self.active_connections) >= self.MAX_CLIENTS:
+            await websocket.close(code=1008, reason="Max clients reached")
             raise RuntimeError(f"Max clients ({self.MAX_CLIENTS}) exceeded")
 
         await websocket.accept()
         self.active_connections.add(websocket)
         self.connection_count += 1
         self.stats['clients_connected'] += 1
 
         client_id = id(websocket)
-        self.log.info(f"Client {client_id} connected. Total)}")
+        self.log.info(f"Client {client_id} connected. Total: {len(self.active_connections)}")
 
         # Send current state immediately
         if self.latest_frame:
-            try))
-            except, websocket):
+            try:
+                await websocket.send_text(self.latest_frame.to_json())
+            except:
+                pass
+
+    async def disconnect(self, websocket: WebSocket):
         """
         Disconnect a WebSocket client
 
         Args:
             websocket: Client to disconnect
         """
-        if websocket in self.active_connections)
+        if websocket in self.active_connections:
+            self.active_connections.remove(websocket)
             self.stats['clients_disconnected'] += 1
 
             client_id = id(websocket)
-            self.log.info(f"Client {client_id} disconnected. Remaining)}")
+            self.log.info(f"Client {client_id} disconnected. Remaining: {len(self.active_connections)}")
 
-    def submit_frame(self, frame: MetricsFrame) :
-            frame)
+    def submit_frame(self, frame: MetricsFrame):
+        """
+        Submit a new metrics frame for broadcasting
+
+        Called from audio callback thread (must be thread-safe)
+
+        Args:
+            frame: MetricsFrame to broadcast
+        """
+        # Validate and sanitize
+        frame.sanitize()
         frame.frame_id = self.frame_counter
         self.frame_counter += 1
 
         # Update latest frame
         self.latest_frame = frame
 
         # Add to buffer (deque is thread-safe for single producer/consumer)
         self.frame_buffer.append(frame)
 
         # Log to disk if enabled
-        if self.logger)
+        if self.logger:
+            self.logger.log_frame(frame)
 
-    async def broadcast_frame(self, frame):
+    async def broadcast_frame(self, frame: MetricsFrame):
         """
         Broadcast frame to all connected clients
 
         Args:
             frame: Frame to broadcast
         """
-        if not self.active_connections)
+        if not self.active_connections:
+            return
+
+        json_data = frame.to_json()
         data_size = len(json_data)
 
         # Broadcast to all clients
         disconnected_clients = set()
 
         for websocket in self.active_connections:
-            try)
+            try:
+                await websocket.send_text(json_data)
                 self.stats['total_bytes_sent'] += data_size
-            except WebSocketDisconnect)
-            except Exception as e)})
+            except WebSocketDisconnect:
+                disconnected_clients.add(websocket)
+            except Exception as e:
+                self.log.error(f"Error sending to client {id(websocket)}: {e}")
                 disconnected_clients.add(websocket)
 
         # Remove disconnected clients
-        for ws in disconnected_clients)
+        for ws in disconnected_clients:
+            await self.disconnect(ws)
 
         self.stats['total_frames_sent'] += 1
 
-    async def broadcast_loop(self))
+    async def broadcast_loop(self):
+        """
+        Main broadcasting loop (runs as async task)
 
         Broadcasts at ≥30 Hz from frame buffer
         """
         self.log.info("Broadcast loop started")
         self.broadcasting = True
 
         idle_frame_timer = 0.0
         last_time = time.time()
 
-        while self.broadcasting)
+        while self.broadcasting:
+            current_time = time.time()
             dt = current_time - last_time
             last_time = current_time
 
             # Check if we have frames to broadcast
-            if self.frame_buffer)
+            if self.frame_buffer:
+                frame = self.frame_buffer.popleft()
 
                 # Calculate latency
-                if frame.timestamp > 0) * 1000
+                if frame.timestamp > 0:
+                    latency_ms = (current_time - frame.timestamp) * 1000
                     self.stats['avg_latency_ms'] = (
                         0.9 * self.stats['avg_latency_ms'] + 0.1 * latency_ms
+                    )
 
                     # Warn if latency exceeds threshold
                     if latency_ms > 100.0:
-                        self.log.warning(f"High latency: {latency_ms)
+                        self.log.warning(f"High latency: {latency_ms:.1f}ms")
 
                 await self.broadcast_frame(frame)
                 idle_frame_timer = 0.0
 
-            else)
+            else:
+                # No frames available - send idle frame every 1 second (FR-009)
                 idle_frame_timer += dt
-                if idle_frame_timer >= 1.0)
+                if idle_frame_timer >= 1.0:
+                    idle_frame = create_idle_frame()
                     await self.broadcast_frame(idle_frame)
                     idle_frame_timer = 0.0
 
             # Sleep to maintain target frame rate
             # Account for processing time
             elapsed = time.time() - current_time
             sleep_time = max(0.001, self.FRAME_INTERVAL - elapsed)
             await asyncio.sleep(sleep_time)
 
         self.log.info("Broadcast loop stopped")
 
-    def start_broadcasting(self) :
+    def start_broadcasting(self):
         """Start the broadcast loop"""
-        if not self.broadcasting and self.broadcast_task is None))
-            logger.info("[MetricsStreamer] Broadcasting started")
+        if not self.broadcasting and self.broadcast_task is None:
+            self.broadcast_task = asyncio.create_task(self.broadcast_loop())
+            print("[MetricsStreamer] Broadcasting started")
 
     async def stop_broadcasting(self):
         """Stop the broadcast loop"""
         self.broadcasting = False
 
-        if self.broadcast_task)
+        if self.broadcast_task:
+            await self.broadcast_task
+            self.broadcast_task = None
+
+        print("[MetricsStreamer] Broadcasting stopped")
+
+    def get_latest_frame(self) -> Optional[MetricsFrame]:
+        """
+        Get the most recent frame (for REST API)
+
+        Returns:
+            Latest MetricsFrame or None
+        """
+        return self.latest_frame
 
-    def get_latest_frame(self) :
+    def get_statistics(self) -> Dict:
         """
         Get streaming statistics
 
-        Returns) - self.last_broadcast_time if self.last_broadcast_time > 0 else 0
+        Returns:
+            Dictionary with performance metrics
+        """
+        uptime = time.time() - self.last_broadcast_time if self.last_broadcast_time > 0 else 0
 
         return {
             **self.stats,
-            'active_connections'),
-            'buffer_size'),
-            'broadcasting',
-            'frames_buffered'),
-            'uptime_seconds',
+            'active_connections': len(self.active_connections),
+            'buffer_size': len(self.frame_buffer),
+            'broadcasting': self.broadcasting,
+            'frames_buffered': len(self.frame_buffer),
+            'uptime_seconds': uptime,
         }
 
-    async def close(self))
+    async def close(self):
+        """Cleanup and close all connections"""
+        print("[MetricsStreamer] Shutting down")
 
         # Stop broadcasting
         await self.stop_broadcasting()
 
         # Close all client connections
         for websocket in list(self.active_connections):
-            try)
-            except)
+            try:
+                await websocket.close()
+            except:
+                pass
+        self.active_connections.clear()
 
         # Close logger
-        if self.logger)
+        if self.logger:
+            self.logger.close()
 
-        logger.info("[MetricsStreamer] Shutdown complete")
+        print("[MetricsStreamer] Shutdown complete")
 
     def __del__(self):
         """Ensure cleanup"""
-        if self.logger)
+        if self.logger:
+            self.logger.close()
 
 
 # FastAPI Application Setup
-def create_metrics_app(streamer) :
+def create_metrics_app(streamer: MetricsStreamer) -> FastAPI:
     """
     Create FastAPI application with metrics endpoints
 
     Args:
         streamer: MetricsStreamer instance
 
+    Returns:
+        FastAPI application
+    """
+    app = FastAPI(title="D-ASE Metrics API")
+
     @app.websocket("/ws/metrics")
-    async def websocket_metrics_endpoint(websocket), FR-003, FR-005
+    async def websocket_metrics_endpoint(websocket: WebSocket):
+        """
+        WebSocket endpoint for real-time metrics stream
+
+        Implements FR-001, FR-003, FR-005
         """
         await streamer.connect(websocket)
 
         try:
             # Keep connection alive and handle incoming messages
-            while True)
+            while True:
+                # Wait for client messages (mostly ping/pong)
                 data = await websocket.receive_text()
 
                 # Handle control messages
-                try)
+                try:
+                    msg = json.loads(data)
                     if msg.get('type') == 'ping':
-                        await websocket.send_text(json.dumps({'type'))
+                        await websocket.send_text(json.dumps({'type': 'pong'}))
                 except:
                     pass  # Ignore malformed messages
 
-        except WebSocketDisconnect)
+        except WebSocketDisconnect:
+            await streamer.disconnect(websocket)
         except Exception as e:
-            logging.error(f"WebSocket error)
+            logging.error(f"WebSocket error: {e}")
             await streamer.disconnect(websocket)
 
     @app.get("/api/metrics/latest")
     async def get_latest_metrics():
         """
         REST endpoint for latest metrics frame
 
         Implements FR-006
 
-        if frame is None,
-                content={"error")
+        Returns:
+            JSON response with latest frame or 404 if not available
+        """
+        frame = streamer.get_latest_frame()
+
+        if frame is None:
+            return JSONResponse(
+                status_code=404,
+                content={"error": "No metrics available"}
+            )
 
         return JSONResponse(content=frame.to_dict())
 
     @app.get("/api/metrics/stats")
     async def get_metrics_statistics():
         """
         Get streaming statistics
 
+        Returns:
+            JSON response with performance statistics
+        """
+        stats = streamer.get_statistics()
         return JSONResponse(content=stats)
 
     @app.on_event("startup")
-    async def startup_event())
+    async def startup_event():
+        """Start broadcasting on application startup"""
+        streamer.start_broadcasting()
         logging.info("Metrics API started")
 
     @app.on_event("shutdown")
-    async def shutdown_event())
+    async def shutdown_event():
+        """Cleanup on application shutdown"""
+        await streamer.close()
         logging.info("Metrics API shutdown")
 
     return app
 
 
 # Self-test function
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test MetricsStreamer (requires async runtime)"""
+    print("=" * 60)
+    print("MetricsStreamer Self-Test")
+    print("=" * 60)
+
+    try:
+        from metrics_frame import create_test_frame
+        import asyncio
+
+        async def run_test():
+            # Initialize streamer
+            print("\n1. Initializing streamer...")
+            streamer = MetricsStreamer(enable_logging=False)
+            print("   ✓ Streamer initialized")
+
+            # Start broadcasting
+            print("\n2. Starting broadcast loop...")
+            streamer.start_broadcasting()
+            await asyncio.sleep(0.1)  # Let it start
+            print("   ✓ Broadcasting started")
+
+            # Submit test frames
+            print("\n3. Submitting test frames...")
+            for i in range(5):
+                frame = create_test_frame(frame_id=i)
+                streamer.submit_frame(frame)
+                await asyncio.sleep(0.033)  # ~30 Hz
+
+            print(f"   ✓ Submitted {streamer.frame_counter} frames")
+
+            # Check latest frame
+            print("\n4. Checking latest frame...")
+            latest = streamer.get_latest_frame()
+            print(f"   Latest frame: {latest}")
+            assert latest is not None
+            print("   ✓ Latest frame OK")
+
+            # Get statistics
+            print("\n5. Getting statistics...")
+            stats = streamer.get_statistics()
+            print(f"   Frames sent: {stats['total_frames_sent']}")
+            print(f"   Avg latency: {stats['avg_latency_ms']:.2f}ms")
+            print("   ✓ Statistics OK")
+
+            # Stop broadcasting
+            print("\n6. Stopping broadcast...")
+            await streamer.stop_broadcasting()
+            print("   ✓ Stopped")
+
+            # Cleanup
+            await streamer.close()
+            print("   ✓ Cleanup complete")
+
+        # Run async test
+        asyncio.run(run_test())
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/middleware/__pycache__/__init__.cpython-311.pyc b/server/middleware/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index 09e06d251d60702904c6b729013b6228b4958409..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 311
zcmZ3^%ge<81liv{XDkQOk3k$5V1zP0^8p#t8B!Rc7}6P{7*iO788n%yI=Or^Q&MtL
z%M*)I6_WFF3-a?)^Gb@jxI!{AixmnIld}`kK@xc-iJ5uD3T}zTC5{1}3b}AiR$RJ3
z>7rsQ1!w=DV35kR%=FTt#FEVXJfKu+QBi(Td`4m(&<>CyR}fzT%+JhAM>2=YPm}2u
zdwhIKesX*~h>@6+6Cb~l;WNnjzhsM3i^@`qVi5Mm#K&jmWtPOp>lIY~;;_lhPbtkw
pwJTx=ss{PESPDpdU}j`w{J_M<$nt>!L<lf&^ER-9U=b@&8UU`8Tr&Uw

diff --git a/server/node_sync.py b/server/node_sync.py
index ef790d064c417455bfd00c6dcba7291f7e7d33b8..58b890c1f0ed001b67b9689a320eb7a9fb7565ba 100644
--- a/server/node_sync.py
+++ b/server/node_sync.py
@@ -1,565 +1,689 @@
 """
 Node Synchronizer - Feature 020
 Distributed synchronization for multiple Soundlab nodes in phase-locked operation.
 
-Features, client = subscriber)
+Features:
+- Master/client architecture (master = authority, client = subscriber)
 - WebSocket communication protocol
 - NTP-like clock synchronization with smoothing filter
 - Frame interpolation for missing data
 - Auto-reconnect mechanism
 - Latency compensation and drift correction
+- Multi-node support (scales to 8+ nodes)
 
 Requirements:
 - FR-001: NodeSynchronizer class
 - FR-002: Master/client roles
 - FR-003: WebSocket /ws/sync protocol
-- FR-004, phi_depth, criticality, timestamp}
+- FR-004: Sync packet format {phi_phase, phi_depth, criticality, timestamp}
 - FR-005: NTP-like clock sync with smoothing
 - FR-006: Frame interpolation
-- FR-007, /api/node/status
+- FR-007: REST API /api/node/register, /api/node/status
 
 Success Criteria:
-- SC-001, jitter <0.5ms
+- SC-001: Phase drift <1ms, jitter <0.5ms
 - SC-002: Metrics consistency ≥99%
 - SC-003: Auto-reconnect <3s
+- SC-004: Scales to ≥8 nodes
+"""
 
 import asyncio
 import json
 import time
 import threading
 from typing import Optional, Dict, List, Callable, Any
 from dataclasses import dataclass
 from enum import Enum
 from collections import deque
 import statistics
 
 
-class NodeRole(Enum))"""
+class NodeRole(Enum):
+    """Node role in synchronization network (FR-002)"""
     MASTER = "master"  # Authority node, broadcasts state
     CLIENT = "client"  # Subscriber node, receives state
 
 
 @dataclass
 class NodeSyncConfig:
     """Configuration for node synchronizer"""
     role: NodeRole = NodeRole.MASTER
-    master_url)
+    master_url: Optional[str] = None  # WebSocket URL for master (client only)
     sync_rate: int = 30  # Synchronization rate in Hz
-    clock_sync_interval)
+    clock_sync_interval: float = 5.0  # Clock sync interval in seconds (FR-005)
     drift_window: int = 100  # Samples for drift calculation
-    reconnect_timeout)
-    max_interpolation_gap)
+    reconnect_timeout: float = 3.0  # Auto-reconnect timeout (SC-003)
+    max_interpolation_gap: int = 5  # Max frames to interpolate (FR-006)
     enable_logging: bool = True
 
 
 @dataclass
-class SyncFrame)"""
+class SyncFrame:
+    """Synchronization frame (FR-004)"""
     phi_phase: float
     phi_depth: float
     criticality: float
     coherence: float
     ici: float
     timestamp: float  # Local timestamp
     master_timestamp: float  # Master's timestamp
     sequence: int  # Sequence number for ordering
 
 
 @dataclass
-class ClockOffset)"""
+class ClockOffset:
+    """Clock synchronization data (FR-005)"""
     offset: float  # Time offset between local and master clock
     latency: float  # Round-trip latency
     drift: float  # Clock drift rate
     updated_at: float  # Last update time
 
 
-class NodeSynchronizer)
+class NodeSynchronizer:
+    """
+    Node Synchronizer for distributed phase-locked operation (Feature 020)
 
     Synchronizes Φ-modulation, metrics, and state data across multiple nodes.
     Implements master/client architecture with clock synchronization.
     """
 
-    def __init__(self, config: Optional[NodeSyncConfig]) :
+    def __init__(self, config: Optional[NodeSyncConfig] = None):
         """
         Initialize Node Synchronizer
 
         Args:
-            config)
+            config: Synchronization configuration
+        """
+        self.config = config or NodeSyncConfig()
 
         # Role and state
         self.role = self.config.role
         self.is_running = False
         self.node_id = f"node_{int(time.time() * 1000)}"
 
         # Master state (FR-002)
-        self.connected_clients, Dict] = {}  # node_id : Optional[ClockOffset] = None
-        self.last_received_frame)
+        self.connected_clients: Dict[str, Dict] = {}  # node_id -> client info
+        self.last_broadcast_time = 0.0
+
+        # Client state (FR-002)
+        self.master_connection = None
+        self.last_sync_time = 0.0
+        self.clock_offset: Optional[ClockOffset] = None
+        self.last_received_frame: Optional[SyncFrame] = None
+        self.received_frames = deque(maxlen=self.config.drift_window)
 
         # Frame interpolation (FR-006)
         self.interpolation_buffer = deque(maxlen=self.config.max_interpolation_gap)
         self.missed_frame_count = 0
         self.interpolated_frame_count = 0
 
         # Statistics (SC-001, SC-002)
         self.phase_drift_history = deque(maxlen=self.config.drift_window)
         self.metrics_consistency_history = deque(maxlen=self.config.drift_window)
         self.latency_history = deque(maxlen=self.config.drift_window)
 
         # Callbacks
         self.sync_callback: Optional[Callable] = None  # Called when sync frame received
         self.connect_callback: Optional[Callable] = None  # Called on connect/disconnect
 
         # Background tasks
         self.sync_task = None
         self.clock_sync_task = None
         self.reconnect_task = None
 
         # Sequence tracking
         self.sequence_counter = 0
         self.last_received_sequence = -1
 
-        if self.config.enable_logging)", self.role.value, self.node_id)
+        if self.config.enable_logging:
+            print(f"[NodeSync] Initialized as {self.role.value} (id={self.node_id})")
 
-    async def start(self))"""
+    async def start(self):
+        """Start node synchronizer (FR-001)"""
         if self.is_running:
             return
 
         self.is_running = True
 
-        if self.role == NodeRole.MASTER)
-        else)
+        if self.role == NodeRole.MASTER:
+            await self._start_master()
+        else:
+            await self._start_client()
 
-        if self.config.enable_logging, self.role.value)
+        if self.config.enable_logging:
+            print(f"[NodeSync] Started as {self.role.value}")
 
-    async def _start_master(self))"""
+    async def _start_master(self):
+        """Start master node (FR-002)"""
         # Master doesn't need background tasks for sync
         # It broadcasts when process_local_state is called
         pass
 
-    async def _start_client(self), FR-003)"""
-        if not self.config.master_url)
+    async def _start_client(self):
+        """Start client node (FR-002, FR-003)"""
+        if not self.config.master_url:
+            raise ValueError("Client mode requires master_url")
 
         # Start clock sync task (FR-005)
         self.clock_sync_task = asyncio.create_task(self._clock_sync_loop())
 
         # Start auto-reconnect task (SC-003)
         self.reconnect_task = asyncio.create_task(self._reconnect_loop())
 
     async def stop(self):
         """Stop node synchronizer"""
         if not self.is_running:
             return
 
         self.is_running = False
 
         # Cancel background tasks
-        if self.clock_sync_task)
-        if self.reconnect_task)
+        if self.clock_sync_task:
+            self.clock_sync_task.cancel()
+        if self.reconnect_task:
+            self.reconnect_task.cancel()
 
         # Disconnect from master
-        if self.master_connection)
+        if self.master_connection:
+            await self.master_connection.close()
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[NodeSync] Stopped")
 
-    @lru_cache(maxsize=128)
-    def process_local_state(self, phi_phase, phi_depth,
-                           criticality, coherence, ici))
+    def process_local_state(self, phi_phase: float, phi_depth: float,
+                           criticality: float, coherence: float, ici: float):
+        """
+        Process local state and broadcast if master (FR-004)
 
         Args:
             phi_phase: Current phi phase
             phi_depth: Current phi depth
             criticality: Current criticality
             coherence: Current coherence
             ici: Current ICI
         """
-        if self.role == NodeRole.MASTER,
+        if self.role == NodeRole.MASTER:
+            # Create sync frame
+            frame = SyncFrame(
+                phi_phase=phi_phase,
                 phi_depth=phi_depth,
                 criticality=criticality,
                 coherence=coherence,
                 ici=ici,
                 timestamp=time.time(),
                 master_timestamp=time.time(),
                 sequence=self.sequence_counter
-
+            )
             self.sequence_counter += 1
 
             # Broadcast to all connected clients
             # Try to create task if event loop is running
-            try))
-            except RuntimeError, in self-test)
+            try:
+                asyncio.create_task(self._broadcast_frame(frame))
+            except RuntimeError:
+                # No event loop running (e.g., in self-test)
                 # Store frame for testing
                 self.last_broadcast_time = time.time()
 
-    async def _broadcast_frame(self, frame))
+    async def _broadcast_frame(self, frame: SyncFrame):
+        """
+        Broadcast sync frame to all connected clients (FR-003)
 
         Args:
             frame: Sync frame to broadcast
         """
         # Convert frame to JSON
         frame_data = {
-            "type",
-            "phi_phase",
-            "phi_depth",
-            "criticality",
-            "coherence",
-            "ici",
-            "timestamp",
-            "master_timestamp",
+            "type": "sync_frame",
+            "phi_phase": frame.phi_phase,
+            "phi_depth": frame.phi_depth,
+            "criticality": frame.criticality,
+            "coherence": frame.coherence,
+            "ici": frame.ici,
+            "timestamp": frame.timestamp,
+            "master_timestamp": frame.master_timestamp,
             "sequence": frame.sequence
         }
 
         # Send to all connected clients
-        # Note, we store it for testing
+        # Note: This would be implemented in the WebSocket handler
+        # For now, we store it for testing
         self.last_broadcast_time = time.time()
 
-    async def receive_sync_frame(self, frame_data)) (FR-004, FR-006)
+    async def receive_sync_frame(self, frame_data: Dict):
+        """
+        Receive sync frame from master (client only) (FR-004, FR-006)
 
         Args:
             frame_data: Sync frame data from master
         """
-        if self.role != NodeRole.CLIENT,
+        if self.role != NodeRole.CLIENT:
+            return
+
+        # Parse frame
+        frame = SyncFrame(
+            phi_phase=frame_data['phi_phase'],
             phi_depth=frame_data['phi_depth'],
             criticality=frame_data['criticality'],
             coherence=frame_data.get('coherence', 0.0),
             ici=frame_data.get('ici', 0.0),
             timestamp=time.time(),  # Local receive time
             master_timestamp=frame_data['master_timestamp'],
             sequence=frame_data['sequence']
+        )
 
         # Check for missed frames (FR-006)
         if self.last_received_sequence >= 0:
             gap = frame.sequence - self.last_received_sequence - 1
             if gap > 0:
                 self.missed_frame_count += gap
 
                 # Interpolate if gap is small enough
-                if gap <= self.config.max_interpolation_gap and self.last_received_frame, frame, gap
+                if gap <= self.config.max_interpolation_gap and self.last_received_frame:
+                    interpolated_frames = self._interpolate_frames(
+                        self.last_received_frame, frame, gap
+                    )
 
                     # Process interpolated frames
-                    for interp_frame in interpolated_frames, interpolated=True)
+                    for interp_frame in interpolated_frames:
+                        self._process_received_frame(interp_frame, interpolated=True)
 
         # Update sequence tracking
         self.last_received_sequence = frame.sequence
         self.last_received_frame = frame
 
         # Add to buffer for interpolation
         self.interpolation_buffer.append(frame)
 
         # Process frame
         self._process_received_frame(frame, interpolated=False)
 
-    def _interpolate_frames(self, frame1, frame2, gap) :
+    def _interpolate_frames(self, frame1: SyncFrame, frame2: SyncFrame, gap: int) -> List[SyncFrame]:
+        """
+        Interpolate missing frames using linear interpolation (FR-006)
+
+        Args:
             frame1: Previous frame
             frame2: Current frame
             gap: Number of frames to interpolate
 
-        Returns, gap + 1))
+        Returns:
+            List of interpolated frames
+        """
+        interpolated = []
+
+        for i in range(1, gap + 1):
+            alpha = i / (gap + 1)
 
             interp_frame = SyncFrame(
                 phi_phase=frame1.phi_phase + alpha * (frame2.phi_phase - frame1.phi_phase),
                 phi_depth=frame1.phi_depth + alpha * (frame2.phi_depth - frame1.phi_depth),
                 criticality=frame1.criticality + alpha * (frame2.criticality - frame1.criticality),
                 coherence=frame1.coherence + alpha * (frame2.coherence - frame1.coherence),
                 ici=frame1.ici + alpha * (frame2.ici - frame1.ici),
                 timestamp=frame1.timestamp + alpha * (frame2.timestamp - frame1.timestamp),
                 master_timestamp=frame1.master_timestamp + alpha * (frame2.master_timestamp - frame1.master_timestamp),
                 sequence=frame1.sequence + i
+            )
 
             interpolated.append(interp_frame)
 
         self.interpolated_frame_count += len(interpolated)
         return interpolated
 
-    @lru_cache(maxsize=128)
-    def _process_received_frame(self, frame: SyncFrame, interpolated: bool) :
+    def _process_received_frame(self, frame: SyncFrame, interpolated: bool = False):
+        """
+        Process received sync frame (FR-004)
+
+        Args:
             frame: Sync frame
-            interpolated)
+            interpolated: Whether frame was interpolated
+        """
+        # Add to received frames for drift calculation
+        self.received_frames.append(frame)
 
         # Calculate phase drift (SC-001)
-        if self.clock_offset))
+        if self.clock_offset:
+            # Adjust timestamp with clock offset
+            adjusted_timestamp = frame.timestamp - self.clock_offset.offset
+            drift = adjusted_timestamp - frame.master_timestamp
+            self.phase_drift_history.append(abs(drift))
 
         # Calculate latency
-        if self.clock_offset)
+        if self.clock_offset:
+            latency = self.clock_offset.latency
+            self.latency_history.append(latency)
 
         # Update last sync time
         self.last_sync_time = time.time()
 
         # Call sync callback if registered
-        if self.sync_callback and not interpolated)
+        if self.sync_callback and not interpolated:
+            self.sync_callback(frame)
 
-    async def _clock_sync_loop(self))
+    async def _clock_sync_loop(self):
+        """
+        Clock synchronization loop (FR-005)
 
         Implements NTP-like algorithm with smoothing filter.
         """
         while self.is_running:
-            try)
+            try:
+                # Perform clock sync
+                await self._sync_clock()
 
                 # Wait for next sync interval
                 await asyncio.sleep(self.config.clock_sync_interval)
 
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[NodeSync] Clock sync error, e)
+                    print(f"[NodeSync] Clock sync error: {e}")
 
-    async def _sync_clock(self))
+    async def _sync_clock(self):
+        """
+        Synchronize clock with master (FR-005)
 
-        Uses NTP-like algorithm, t2 (master time), t3 (response time)
+        Uses NTP-like algorithm:
+        1. Send timestamp t1 to master
+        2. Master responds with t1, t2 (master time), t3 (response time)
         3. Calculate offset and latency
         """
-        if self.role != NodeRole.CLIENT or not self.master_connection)
+        if self.role != NodeRole.CLIENT or not self.master_connection:
+            return
+
+        # Send clock sync request
+        t1 = time.time()
 
         request = {
-            "type",
-            "t1", this would send via WebSocket
+            "type": "clock_sync",
+            "t1": t1
+        }
+
+        # In actual implementation, this would send via WebSocket
         # and wait for response with t2, t3
         # For now, simulate with placeholder
 
         # Placeholder for testing
-        # In real implementation)
+        # In real implementation:
+        # response = await self.master_connection.send_and_wait(request)
         # t2 = response['t2']  # Master time when received
         # t3 = response['t3']  # Master time when sent response
         # t4 = time.time()  # Local time when received response
 
         # Calculate offset and latency (NTP algorithm)
         # offset = ((t2 - t1) + (t3 - t4)) / 2
         # latency = (t4 - t1) - (t3 - t2)
 
         # For now, create placeholder offset
-        if not self.clock_offset,
+        if not self.clock_offset:
+            self.clock_offset = ClockOffset(
+                offset=0.0,
                 latency=0.001,  # 1ms placeholder
                 drift=0.0,
                 updated_at=time.time()
+            )
 
-    async def _reconnect_loop(self))
+    async def _reconnect_loop(self):
+        """
+        Auto-reconnect loop for clients (SC-003)
 
         Attempts to reconnect if connection is lost.
         """
         while self.is_running:
             try:
                 # Check if connected
                 if not self.master_connection:
                     # Attempt reconnect
-                    if self.config.enable_logging)
+                    if self.config.enable_logging:
+                        print("[NodeSync] Attempting to reconnect to master...")
 
                     # In actual implementation, this would create WebSocket connection
                     # For now, placeholder
 
                     # Call connect callback if registered
-                    if self.connect_callback)
+                    if self.connect_callback:
+                        self.connect_callback(False)
 
                 # Wait before next check
                 await asyncio.sleep(self.config.reconnect_timeout)
 
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[NodeSync] Reconnect error, e)
+                    print(f"[NodeSync] Reconnect error: {e}")
 
-    def register_client(self, client_id: str, client_info: Dict) :
+    def register_client(self, client_id: str, client_info: Dict):
+        """
+        Register client node (master only) (FR-007)
+
+        Args:
             client_id: Client node ID
             client_info: Client information
         """
-        if self.role != NodeRole.MASTER,
-            "connected_at"),
-            "last_seen")
+        if self.role != NodeRole.MASTER:
+            return
+
+        self.connected_clients[client_id] = {
+            **client_info,
+            "connected_at": time.time(),
+            "last_seen": time.time()
         }
 
         if self.config.enable_logging:
-            logger.info("[NodeSync] Client registered, client_id)
+            print(f"[NodeSync] Client registered: {client_id}")
 
         # Call connect callback
-        if self.connect_callback)
+        if self.connect_callback:
+            self.connect_callback(True)
 
-    def unregister_client(self, client_id: str) :
+    def unregister_client(self, client_id: str):
+        """
+        Unregister client node (master only) (FR-007)
+
+        Args:
             client_id: Client node ID
         """
         if self.role != NodeRole.MASTER:
             return
 
         if client_id in self.connected_clients:
             del self.connected_clients[client_id]
 
             if self.config.enable_logging:
-                logger.info("[NodeSync] Client unregistered, client_id)
+                print(f"[NodeSync] Client unregistered: {client_id}")
 
             # Call connect callback
-            if self.connect_callback)
+            if self.connect_callback:
+                self.connect_callback(False)
 
-    def get_status(self) :
+    def get_status(self) -> Dict[str, Any]:
+        """
+        Get synchronizer status (FR-007)
+
+        Returns:
             Status dictionary with statistics
         """
         status = {
-            "role",
-            "node_id",
+            "role": self.role.value,
+            "node_id": self.node_id,
             "is_running": self.is_running
         }
 
         if self.role == NodeRole.MASTER:
             # Master status
             status.update({
-                "connected_clients"),
+                "connected_clients": len(self.connected_clients),
                 "clients": [
                     {
-                        "id",
-                        "connected_at",
-                        "last_seen", info in self.connected_clients.items()
+                        "id": client_id,
+                        "connected_at": info["connected_at"],
+                        "last_seen": info["last_seen"]
+                    }
+                    for client_id, info in self.connected_clients.items()
                 ],
-                "last_broadcast_time")
+                "last_broadcast_time": self.last_broadcast_time
+            })
         else:
             # Client status
             status.update({
-                "master_url",
-                "connected",
-                "last_sync_time",
-                "clock_offset",
-                "latency",
-                "missed_frames",
-                "interpolated_frames")
+                "master_url": self.config.master_url,
+                "connected": self.master_connection is not None,
+                "last_sync_time": self.last_sync_time,
+                "clock_offset": self.clock_offset.offset if self.clock_offset else None,
+                "latency": self.clock_offset.latency if self.clock_offset else None,
+                "missed_frames": self.missed_frame_count,
+                "interpolated_frames": self.interpolated_frame_count
+            })
 
         return status
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict[str, Any]:
+        """
+        Get synchronization statistics (SC-001, SC-002)
+
+        Returns:
             Statistics dictionary
         """
         stats = {
-            "role",
+            "role": self.role.value,
             "node_id": self.node_id
         }
 
-        if self.role == NodeRole.CLIENT)
+        if self.role == NodeRole.CLIENT:
+            # Phase drift statistics (SC-001)
             if self.phase_drift_history:
                 stats["phase_drift"] = {
-                    "average_ms") * 1000,
-                    "max_ms") * 1000,
-                    "min_ms") * 1000,
-                    "stdev_ms") * 1000 if len(self.phase_drift_history) > 1 else 0,
+                    "average_ms": statistics.mean(self.phase_drift_history) * 1000,
+                    "max_ms": max(self.phase_drift_history) * 1000,
+                    "min_ms": min(self.phase_drift_history) * 1000,
+                    "stdev_ms": statistics.stdev(self.phase_drift_history) * 1000 if len(self.phase_drift_history) > 1 else 0,
                     "target_ms": 1.0  # SC-001 target
                 }
 
             # Latency statistics
             if self.latency_history:
                 stats["latency"] = {
-                    "average_ms") * 1000,
-                    "max_ms") * 1000,
-                    "min_ms") * 1000
+                    "average_ms": statistics.mean(self.latency_history) * 1000,
+                    "max_ms": max(self.latency_history) * 1000,
+                    "min_ms": min(self.latency_history) * 1000
                 }
 
             # Frame statistics
             total_frames = self.last_received_sequence + 1 if self.last_received_sequence >= 0 else 0
             received_frames = total_frames - self.missed_frame_count
 
             stats["frames"] = {
-                "total",
-                "received",
-                "missed",
-                "interpolated",
-                "consistency_percent") if total_frames > 0 else 0  # SC-002
+                "total": total_frames,
+                "received": received_frames,
+                "missed": self.missed_frame_count,
+                "interpolated": self.interpolated_frame_count,
+                "consistency_percent": (received_frames / total_frames * 100) if total_frames > 0 else 0  # SC-002
             }
 
             # Clock offset
             if self.clock_offset:
                 stats["clock"] = {
-                    "offset_ms",
-                    "drift",
+                    "offset_ms": self.clock_offset.offset * 1000,
+                    "drift": self.clock_offset.drift,
                     "updated_at": self.clock_offset.updated_at
                 }
 
         else:
             # Master statistics
             stats["clients"] = {
-                "total"),
-                "active")
+                "total": len(self.connected_clients),
+                "active": len([c for c in self.connected_clients.values()
                               if time.time() - c["last_seen"] < 10])
             }
 
         return stats
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("Node Synchronizer Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("Node Synchronizer Self-Test")
+    print("=" * 60)
 
     # Test master node
-    logger.info("\n1. Testing master node initialization...")
+    print("\n1. Testing master node initialization...")
     master_config = NodeSyncConfig(
         role=NodeRole.MASTER,
         enable_logging=True
-
+    )
     master = NodeSynchronizer(master_config)
-    logger.info("   OK)
+    print("   OK: Master initialization")
 
     # Test client node
-    logger.info("\n2. Testing client node initialization...")
+    print("\n2. Testing client node initialization...")
     client_config = NodeSyncConfig(
         role=NodeRole.CLIENT,
-        master_url="ws://localhost,
+        master_url="ws://localhost:8000/ws/sync",
         enable_logging=True
-
+    )
     client = NodeSynchronizer(client_config)
-    logger.info("   OK)
+    print("   OK: Client initialization")
 
     # Test frame processing
-    logger.info("\n3. Testing frame processing...")
+    print("\n3. Testing frame processing...")
     master.process_local_state(
         phi_phase=0.5,
         phi_depth=0.8,
         criticality=1.0,
         coherence=0.7,
         ici=0.3
-
-    logger.info("   OK)
+    )
+    print("   OK: Frame processing")
 
     # Test client registration
-    logger.info("\n4. Testing client registration...")
-    master.register_client("client_1", {"name")
-    master.register_client("client_2", {"name")
-    logger.info("   OK, len(master.connected_clients))
+    print("\n4. Testing client registration...")
+    master.register_client("client_1", {"name": "Client 1"})
+    master.register_client("client_2", {"name": "Client 2"})
+    print(f"   OK: {len(master.connected_clients)} clients registered")
 
     # Test status
-    logger.info("\n5. Testing status...")
+    print("\n5. Testing status...")
     master_status = master.get_status()
     client_status = client.get_status()
-    logger.info("   OK)", master_status['connected_clients'])
-    logger.info("   OK)", client_status['connected'])
+    print(f"   OK: Master status (clients={master_status['connected_clients']})")
+    print(f"   OK: Client status (connected={client_status['connected']})")
 
     # Test statistics
-    logger.info("\n6. Testing statistics...")
+    print("\n6. Testing statistics...")
     master_stats = master.get_statistics()
     client_stats = client.get_statistics()
-    logger.info("   OK)", master_stats['clients']['total'])
-    logger.info("   OK)
+    print(f"   OK: Master stats (clients={master_stats['clients']['total']})")
+    print(f"   OK: Client stats")
 
     # Test frame interpolation
-    logger.info("\n7. Testing frame interpolation...")
+    print("\n7. Testing frame interpolation...")
     frame1 = SyncFrame(
         phi_phase=0.0,
         phi_depth=0.5,
         criticality=1.0,
         coherence=0.5,
         ici=0.2,
         timestamp=0.0,
         master_timestamp=0.0,
         sequence=0
-
+    )
     frame2 = SyncFrame(
         phi_phase=1.0,
         phi_depth=0.8,
         criticality=1.2,
         coherence=0.7,
         ici=0.4,
         timestamp=0.1,
         master_timestamp=0.1,
         sequence=5
-
+    )
     interpolated = client._interpolate_frames(frame1, frame2, 4)
-    logger.info("   OK, len(interpolated))
-
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
-    logger.info("Note)
+    print(f"   OK: Interpolated {len(interpolated)} frames")
 
-"""  # auto-closed missing docstring
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
+    print("Note: Full network testing requires running nodes")
diff --git a/server/phasenet_protocol.py b/server/phasenet_protocol.py
index 6bac7fc99779eb6769cd502f8bcfc5a6748fb54d..2b4d04e32b8970260f048160a6f2e4596f746595 100644
--- a/server/phasenet_protocol.py
+++ b/server/phasenet_protocol.py
@@ -1,570 +1,839 @@
 """
 PhaseNet Protocol - Feature 021
 Low-latency distributed mesh network for synchronizing Φ-phase, coherence, and criticality
 data between Soundlab nodes.
 
+Features:
+- Mesh network topology with peer discovery
+- Dynamic master election (Raft-lite)
 - UDP socket transport for low latency
 - AES-128 packet encryption
 - Per-node drift compensation
 - Topology healing and auto-reconnect
 - Network monitoring and metrics
 
 Requirements:
 - FR-001: PhaseNetNode class
-
-- FR-003, authentication, master election
-- FR-004, phi_phase, phi_depth, criticality, coherence}
+- FR-002: UDP socket transport (WebRTC optional)
+- FR-003: Peer discovery, authentication, master election
+- FR-004: Compact packet format {t, phi_phase, phi_depth, criticality, coherence}
 - FR-005: Per-node drift table with adaptive compensation
 - FR-006: AES-128 encryption
 - FR-007: Auto-reconnect and topology healing
 - FR-008: Network status API
 
 Success Criteria:
-- SC-001, jitter <2ms
+- SC-001: Network delay <5ms, jitter <2ms
 - SC-002: Phase coherence ≥0.99
 - SC-003: Master election <2s
+- SC-004: 100% uptime for 1h under churn
+"""
 
 import socket
 import json
 import time
 import threading
 import struct
 import hashlib
 from typing import Optional, Dict, List, Callable, Any, Tuple
 from dataclasses import dataclass, asdict
 from enum import Enum
 from collections import deque
 import statistics
 
 
 # Encryption support
-try, algorithms, modes
+try:
+    from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
     from cryptography.hazmat.backends import default_backend
     from cryptography.hazmat.primitives import padding
     ENCRYPTION_AVAILABLE = True
-except ImportError))"""
+except ImportError:
+    ENCRYPTION_AVAILABLE = False
+
+
+class NodeState(Enum):
+    """Node state in Raft-lite election (FR-003)"""
     FOLLOWER = "follower"
     CANDIDATE = "candidate"
     LEADER = "leader"
 
 
 @dataclass
 class PhaseNetConfig:
     """Configuration for PhaseNet node"""
     node_id: Optional[str] = None  # Auto-generated if None
     bind_address: str = "0.0.0.0"
     bind_port: int = 9000
     broadcast_address: str = "255.255.255.255"
     broadcast_port: int = 9001
-    network_key)
-    election_timeout)
+    network_key: Optional[str] = None  # Shared key for encryption (FR-006)
+    election_timeout: float = 2.0  # Leader election timeout (SC-003)
     heartbeat_interval: float = 0.5  # Leader heartbeat interval
     sync_rate: int = 30  # Phase sync rate in Hz
-    max_drift_samples)
-    enable_encryption)
+    max_drift_samples: int = 100  # Drift table size (FR-005)
+    enable_encryption: bool = True  # AES-128 encryption (FR-006)
     enable_logging: bool = True
 
 
 @dataclass
-class PhasePacket)"""
+class PhasePacket:
+    """Phase data packet (FR-004)"""
     t: float  # Timestamp
     phi_phase: float
     phi_depth: float
     criticality: float
     coherence: float
     ici: float
     node_id: str
     sequence: int
 
 
 @dataclass
 class PeerInfo:
     """Peer node information"""
     node_id: str
     address: str
     port: int
     last_seen: float
     is_leader: bool = False
-    drift_offset)
+    drift_offset: float = 0.0  # Clock offset (FR-005)
     latency: float = 0.0  # Network latency
     phase_diff: float = 0.0  # Phase coherence difference
 
 
-class PhaseNetNode)
+class PhaseNetNode:
+    """
+    PhaseNet Protocol Node (Feature 021)
 
     Distributed mesh network for phase-locked synchronization across multiple nodes.
     Supports dynamic master election, encryption, and topology healing.
     """
 
-    def __init__(self, config: Optional[PhaseNetConfig]) :
+    def __init__(self, config: Optional[PhaseNetConfig] = None):
         """
         Initialize PhaseNet node
 
         Args:
-            config)
+            config: PhaseNet configuration
+        """
+        self.config = config or PhaseNetConfig()
 
         # Generate node ID if not provided
-        if not self.config.node_id) * 1000)}"
+        if not self.config.node_id:
+            self.config.node_id = f"node_{int(time.time() * 1000)}"
 
         self.node_id = self.config.node_id
 
         # Network state
         self.socket = None
         self.is_running = False
 
         # Raft-lite state (FR-003)
         self.state = NodeState.FOLLOWER
         self.current_term = 0
         self.voted_for = None
         self.leader_id = None
         self.last_heartbeat = 0.0
         self.election_timer = None
         self.heartbeat_timer = None
 
         # Peers (mesh network)
-        self.peers, PeerInfo] = {}
+        self.peers: Dict[str, PeerInfo] = {}
         self.peer_lock = threading.Lock()
 
         # Phase synchronization
         self.local_phase = PhasePacket(
             t=time.time(),
             phi_phase=0.0,
             phi_depth=0.0,
             criticality=1.0,
             coherence=0.0,
             ici=0.0,
             node_id=self.node_id,
             sequence=0
-
+        )
         self.sequence_counter = 0
 
         # Drift compensation (FR-005)
-        self.drift_table, deque] = {}  # node_id : Optional[Callable] = None  # Called on phase update
+        self.drift_table: Dict[str, deque] = {}  # node_id -> drift samples
+        self.latency_table: Dict[str, deque] = {}  # node_id -> latency samples
+
+        # Encryption (FR-006)
+        self.cipher = None
+        if self.config.enable_encryption and self.config.network_key and ENCRYPTION_AVAILABLE:
+            self._init_encryption()
+
+        # Statistics (SC-001, SC-002)
+        self.packet_count = 0
+        self.packets_sent = 0
+        self.packets_received = 0
+        self.packets_dropped = 0
+        self.latency_history = deque(maxlen=100)
+        self.jitter_history = deque(maxlen=100)
+        self.coherence_history = deque(maxlen=100)
+
+        # Callbacks
+        self.phase_callback: Optional[Callable] = None  # Called on phase update
         self.leader_callback: Optional[Callable] = None  # Called on leader change
 
         # Background threads
         self.receiver_thread = None
         self.discovery_thread = None
         self.health_check_thread = None
 
-        if self.config.enable_logging, self.node_id)
+        if self.config.enable_logging:
+            print(f"[PhaseNet] Initialized node {self.node_id}")
 
-    def _init_encryption(self) :16]
+    def _init_encryption(self):
+        """Initialize AES-128 encryption (FR-006)"""
+        # Derive 128-bit key from network key
+        key = hashlib.sha256(self.config.network_key.encode()).digest()[:16]
         # Use first 16 bytes for AES-128
         self.encryption_key = key
 
-        if self.config.enable_logging)")
+        if self.config.enable_logging:
+            print("[PhaseNet] Encryption enabled (AES-128)")
+
+    def start(self):
+        """Start PhaseNet node (FR-001)"""
+        if self.is_running:
+            return
+
+        self.is_running = True
+
+        # Create UDP socket (FR-002)
+        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
+        self.socket.bind((self.config.bind_address, self.config.bind_port))
+        self.socket.settimeout(0.1)  # Non-blocking with timeout
+
+        # Start background threads
+        self.receiver_thread = threading.Thread(target=self._receiver_loop, daemon=True)
+        self.receiver_thread.start()
+
+        self.discovery_thread = threading.Thread(target=self._discovery_loop, daemon=True)
+        self.discovery_thread.start()
 
-    def start(self) :
-            logger.info("[PhaseNet] Started on %s, self.config.bind_address, self.config.bind_port)
+        self.health_check_thread = threading.Thread(target=self._health_check_loop, daemon=True)
+        self.health_check_thread.start()
 
-    def stop(self) :
+        # Start election timer (follower state)
+        self._reset_election_timer()
+
+        if self.config.enable_logging:
+            print(f"[PhaseNet] Started on {self.config.bind_address}:{self.config.bind_port}")
+
+    def stop(self):
         """Stop PhaseNet node"""
         if not self.is_running:
             return
 
         self.is_running = False
 
         # Cancel timers
-        if self.election_timer)
-        if self.heartbeat_timer)
+        if self.election_timer:
+            self.election_timer.cancel()
+        if self.heartbeat_timer:
+            self.heartbeat_timer.cancel()
 
         # Close socket
-        if self.socket)
+        if self.socket:
+            self.socket.close()
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[PhaseNet] Stopped")
 
-    def update_phase(self, phi_phase, phi_depth,
-                     criticality, coherence, ici))
+    def update_phase(self, phi_phase: float, phi_depth: float,
+                     criticality: float, coherence: float, ici: float):
+        """
+        Update local phase and broadcast if leader (FR-004)
 
         Args:
             phi_phase: Current phi phase
             phi_depth: Current phi depth
             criticality: Current criticality
             coherence: Current coherence
-            ici),
+            ici: Current ICI
+        """
+        # Update local phase
+        self.local_phase = PhasePacket(
+            t=time.time(),
             phi_phase=phi_phase,
             phi_depth=phi_depth,
             criticality=criticality,
             coherence=coherence,
             ici=ici,
             node_id=self.node_id,
             sequence=self.sequence_counter
-
+        )
         self.sequence_counter += 1
 
         # Broadcast to all peers
         self._broadcast_phase()
 
-    def _broadcast_phase(self) :
+    def _broadcast_phase(self):
+        """Broadcast phase packet to all peers (FR-004, SC-001)"""
+        if not self.is_running or not self.socket:
             return
 
         # Create packet
         packet_data = {
-            "type",
-            "t",
-            "phi_phase",
-            "phi_depth",
-            "criticality",
-            "coherence",
-            "ici",
-            "node_id",
-            "sequence")
+            "type": "phase",
+            "t": self.local_phase.t,
+            "phi_phase": self.local_phase.phi_phase,
+            "phi_depth": self.local_phase.phi_depth,
+            "criticality": self.local_phase.criticality,
+            "coherence": self.local_phase.coherence,
+            "ici": self.local_phase.ici,
+            "node_id": self.local_phase.node_id,
+            "sequence": self.local_phase.sequence
+        }
+
+        # Encrypt if enabled (FR-006)
         packet_bytes = self._encrypt_packet(packet_data)
 
         # Send to all known peers
-        with self.peer_lock):
-                try, (peer.address, peer.port))
+        with self.peer_lock:
+            for peer in self.peers.values():
+                try:
+                    self.socket.sendto(packet_bytes, (peer.address, peer.port))
                     self.packets_sent += 1
                 except Exception as e:
                     if self.config.enable_logging:
-                        logger.error("[PhaseNet] Send error to %s, peer.node_id, e)
+                        print(f"[PhaseNet] Send error to {peer.node_id}: {e}")
+
+    def _encrypt_packet(self, data: Dict) -> bytes:
+        """
+        Encrypt packet with AES-128 (FR-006)
 
-    def _encrypt_packet(self, data) :
+        Args:
             data: Packet data dictionary
 
-        Returns).encode('utf-8')
+        Returns:
+            Encrypted packet bytes
+        """
+        # Serialize to JSON
+        json_bytes = json.dumps(data).encode('utf-8')
 
         # If encryption enabled, encrypt
-        if self.cipher and self.config.enable_encryption and ENCRYPTION_AVAILABLE).padder()
+        if self.cipher and self.config.enable_encryption and ENCRYPTION_AVAILABLE:
+            # Add PKCS7 padding
+            padder = padding.PKCS7(128).padder()
             padded_data = padder.update(json_bytes) + padder.finalize()
 
             # Generate random IV
             import os
             iv = os.urandom(16)
 
             # Encrypt
             cipher = Cipher(
                 algorithms.AES(self.encryption_key),
                 modes.CBC(iv),
                 backend=default_backend()
-
+            )
             encryptor = cipher.encryptor()
             encrypted = encryptor.update(padded_data) + encryptor.finalize()
 
             # Prepend IV to encrypted data
             return iv + encrypted
-        else, return plain JSON
+        else:
+            # No encryption, return plain JSON
             return json_bytes
 
-    def _decrypt_packet(self, data) :
+    def _decrypt_packet(self, data: bytes) -> Optional[Dict]:
+        """
+        Decrypt packet with AES-128 (FR-006)
+
+        Args:
             data: Encrypted packet bytes
 
         Returns:
             Decrypted packet dictionary or None if failed
         """
-        try, decrypt
-            if self.cipher and self.config.enable_encryption and ENCRYPTION_AVAILABLE)
+        try:
+            # If encryption enabled, decrypt
+            if self.cipher and self.config.enable_encryption and ENCRYPTION_AVAILABLE:
+                # Extract IV (first 16 bytes)
                 iv = data[:16]
-                encrypted = data[16),
+                encrypted = data[16:]
+
+                # Decrypt
+                cipher = Cipher(
+                    algorithms.AES(self.encryption_key),
                     modes.CBC(iv),
                     backend=default_backend()
-
+                )
                 decryptor = cipher.decryptor()
                 padded_data = decryptor.update(encrypted) + decryptor.finalize()
 
                 # Remove PKCS7 padding
                 unpadder = padding.PKCS7(128).unpadder()
                 json_bytes = unpadder.update(padded_data) + unpadder.finalize()
-            else, parse plain JSON
+            else:
+                # No encryption, parse plain JSON
                 json_bytes = data
 
             # Parse JSON
             return json.loads(json_bytes.decode('utf-8'))
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[PhaseNet] Decrypt error, e)
+                print(f"[PhaseNet] Decrypt error: {e}")
             self.packets_dropped += 1
             return None
 
-    @lru_cache(maxsize=128)
-    def _receiver_loop(self) :
-            try, addr = self.socket.recvfrom(4096)
+    def _receiver_loop(self):
+        """Receive and process packets (FR-002)"""
+        while self.is_running:
+            try:
+                data, addr = self.socket.recvfrom(4096)
                 receive_time = time.time()
 
                 # Decrypt packet
                 packet = self._decrypt_packet(data)
-                if not packet)
+                if not packet:
+                    continue
+
+                self.packets_received += 1
+
+                # Handle packet by type
+                packet_type = packet.get("type")
 
-                if packet_type == "phase", addr, receive_time)
-                elif packet_type == "discovery", addr)
-                elif packet_type == "heartbeat", addr)
-                elif packet_type == "vote_request", addr)
-                elif packet_type == "vote_response", addr)
+                if packet_type == "phase":
+                    self._handle_phase_packet(packet, addr, receive_time)
+                elif packet_type == "discovery":
+                    self._handle_discovery_packet(packet, addr)
+                elif packet_type == "heartbeat":
+                    self._handle_heartbeat_packet(packet, addr)
+                elif packet_type == "vote_request":
+                    self._handle_vote_request(packet, addr)
+                elif packet_type == "vote_response":
+                    self._handle_vote_response(packet, addr)
 
             except socket.timeout:
                 continue
             except Exception as e:
                 if self.is_running and self.config.enable_logging:
-                    logger.error("[PhaseNet] Receiver error, e)
+                    print(f"[PhaseNet] Receiver error: {e}")
 
-    def _handle_phase_packet(self, packet: Dict, addr: Tuple, receive_time: float) :
+    def _handle_phase_packet(self, packet: Dict, addr: Tuple, receive_time: float):
+        """
+        Handle received phase packet (FR-004, FR-005)
+
+        Args:
             packet: Phase packet data
             addr: Sender address
-            receive_time)
+            receive_time: Packet receive time
+        """
+        node_id = packet.get("node_id")
         if node_id == self.node_id:
             return  # Ignore own packets
 
         # Update peer info
         with self.peer_lock:
-            if node_id not in self.peers,
+            if node_id not in self.peers:
+                self.peers[node_id] = PeerInfo(
+                    node_id=node_id,
                     address=addr[0],
                     port=addr[1],
                     last_seen=receive_time
+                )
+            else:
+                self.peers[node_id].last_seen = receive_time
+
+            peer = self.peers[node_id]
 
-            else)
+            # Calculate latency (FR-005)
             packet_time = packet.get("t", 0)
             latency = receive_time - packet_time
 
             # Update latency table
-            if node_id not in self.latency_table)
+            if node_id not in self.latency_table:
+                self.latency_table[node_id] = deque(maxlen=self.config.max_drift_samples)
             self.latency_table[node_id].append(latency)
 
             # Calculate drift offset (simple moving average)
             peer.latency = statistics.mean(self.latency_table[node_id]) if self.latency_table[node_id] else 0
 
             # Calculate phase coherence (SC-002)
             peer.phase_diff = abs(packet.get("phi_phase", 0) - self.local_phase.phi_phase)
             self.coherence_history.append(1.0 - min(peer.phase_diff, 1.0))
 
             # Track jitter
-            if len(self.latency_table[node_id]) > 1)
+            if len(self.latency_table[node_id]) > 1:
+                latencies = list(self.latency_table[node_id])
                 jitter = abs(latencies[-1] - latencies[-2])
                 self.jitter_history.append(jitter)
 
         # Call phase callback if registered
-        if self.phase_callback)
+        if self.phase_callback:
+            self.phase_callback(packet)
 
-    def _discovery_loop(self) :
+    def _discovery_loop(self):
+        """Peer discovery loop (FR-003, FR-007)"""
+        while self.is_running:
             try:
                 # Broadcast discovery packet
                 discovery_packet = {
-                    "type",
-                    "node_id",
-                    "port")
+                    "type": "discovery",
+                    "node_id": self.node_id,
+                    "port": self.config.bind_port
+                }
+
+                packet_bytes = self._encrypt_packet(discovery_packet)
 
                 self.socket.sendto(
                     packet_bytes,
                     (self.config.broadcast_address, self.config.broadcast_port)
+                )
 
                 # Wait before next discovery
                 time.sleep(5.0)
 
             except Exception as e:
                 if self.is_running and self.config.enable_logging:
-                    logger.error("[PhaseNet] Discovery error, e)
+                    print(f"[PhaseNet] Discovery error: {e}")
 
-    def _handle_discovery_packet(self, packet: Dict, addr: Tuple) :
+    def _handle_discovery_packet(self, packet: Dict, addr: Tuple):
+        """
+        Handle discovery packet (FR-003)
+
+        Args:
             packet: Discovery packet data
-            addr)
-        if node_id == self.node_id, self.config.bind_port)
+            addr: Sender address
+        """
+        node_id = packet.get("node_id")
+        if node_id == self.node_id:
+            return
+
+        port = packet.get("port", self.config.bind_port)
 
         # Add peer if not known
         with self.peer_lock:
-            if node_id not in self.peers,
+            if node_id not in self.peers:
+                self.peers[node_id] = PeerInfo(
+                    node_id=node_id,
                     address=addr[0],
                     port=port,
                     last_seen=time.time()
+                )
 
                 if self.config.enable_logging:
-                    logger.info("[PhaseNet] Discovered peer: %s at %s, node_id, addr[0], port)
+                    print(f"[PhaseNet] Discovered peer: {node_id} at {addr[0]}:{port}")
 
-    def _health_check_loop(self) :
-            try)
+    def _health_check_loop(self):
+        """Health check and topology healing loop (FR-007, SC-004)"""
+        while self.is_running:
+            try:
+                current_time = time.time()
 
                 # Remove stale peers (not seen for 10 seconds)
-                with self.peer_lock, peer in self.peers.items()
+                with self.peer_lock:
+                    stale_peers = [
+                        peer_id for peer_id, peer in self.peers.items()
                         if current_time - peer.last_seen > 10.0
                     ]
 
                     for peer_id in stale_peers:
                         if self.config.enable_logging:
-                            logger.info("[PhaseNet] Removing stale peer, peer_id)
+                            print(f"[PhaseNet] Removing stale peer: {peer_id}")
                         del self.peers[peer_id]
 
                 # Wait before next check
                 time.sleep(2.0)
 
             except Exception as e:
                 if self.is_running and self.config.enable_logging:
-                    logger.error("[PhaseNet] Health check error, e)
+                    print(f"[PhaseNet] Health check error: {e}")
+
+    def _reset_election_timer(self):
+        """Reset election timer (FR-003)"""
+        if self.election_timer:
+            self.election_timer.cancel()
+
+        # Random timeout to avoid split votes
+        import random
+        timeout = self.config.election_timeout + random.uniform(0, 1.0)
+
+        self.election_timer = threading.Timer(timeout, self._start_election)
+        self.election_timer.daemon = True
+        self.election_timer.start()
 
-    def _reset_election_timer(self) :
+    def _start_election(self):
+        """Start leader election (FR-003, SC-003)"""
+        if not self.is_running:
             return
 
         # Become candidate
         self.state = NodeState.CANDIDATE
         self.current_term += 1
         self.voted_for = self.node_id
         votes_received = 1  # Vote for self
 
-        if self.config.enable_logging)", self.current_term)
+        if self.config.enable_logging:
+            print(f"[PhaseNet] Starting election (term {self.current_term})")
 
         # Request votes from all peers
         vote_request = {
-            "type",
-            "node_id",
-            "term")
+            "type": "vote_request",
+            "node_id": self.node_id,
+            "term": self.current_term
+        }
+
+        packet_bytes = self._encrypt_packet(vote_request)
 
-        with self.peer_lock):
-                try, (peer.address, peer.port))
-                except)
+        with self.peer_lock:
+            for peer in self.peers.values():
+                try:
+                    self.socket.sendto(packet_bytes, (peer.address, peer.port))
+                except:
+                    pass
+
+        # Wait for votes (simplified - should collect votes asynchronously)
         time.sleep(0.5)
 
         # Check if won election (majority)
-        with self.peer_lock) + 1
-            if votes_received > total_nodes / 2)
-            else)
+        with self.peer_lock:
+            total_nodes = len(self.peers) + 1
+            if votes_received > total_nodes / 2:
+                self._become_leader()
+            else:
+                # Reset to follower and restart timer
+                self.state = NodeState.FOLLOWER
+                self._reset_election_timer()
+
+    def _become_leader(self):
+        """Become leader and start heartbeats (FR-003)"""
+        self.state = NodeState.LEADER
+        self.leader_id = self.node_id
+
+        if self.config.enable_logging:
+            print(f"[PhaseNet] Became leader (term {self.current_term})")
 
-    def _become_leader(self) :
+        # Call leader callback
+        if self.leader_callback:
+            self.leader_callback(True)
+
+        # Start heartbeat timer
+        self._send_heartbeat()
+
+    def _send_heartbeat(self):
+        """Send heartbeat to followers (FR-003)"""
+        if not self.is_running or self.state != NodeState.LEADER:
             return
 
         # Send heartbeat
         heartbeat = {
-            "type",
-            "node_id",
-            "term")
+            "type": "heartbeat",
+            "node_id": self.node_id,
+            "term": self.current_term
+        }
 
-        with self.peer_lock):
-                try, (peer.address, peer.port))
-                except,
-            self._send_heartbeat
+        packet_bytes = self._encrypt_packet(heartbeat)
 
+        with self.peer_lock:
+            for peer in self.peers.values():
+                try:
+                    self.socket.sendto(packet_bytes, (peer.address, peer.port))
+                except:
+                    pass
+
+        # Schedule next heartbeat
+        self.heartbeat_timer = threading.Timer(
+            self.config.heartbeat_interval,
+            self._send_heartbeat
+        )
         self.heartbeat_timer.daemon = True
         self.heartbeat_timer.start()
 
-    def _handle_heartbeat_packet(self, packet: Dict, addr: Tuple) :
+    def _handle_heartbeat_packet(self, packet: Dict, addr: Tuple):
+        """Handle heartbeat from leader (FR-003)"""
+        node_id = packet.get("node_id")
+        term = packet.get("term", 0)
+
+        # Update leader
+        if term >= self.current_term:
+            self.current_term = term
+            self.leader_id = node_id
+            self.last_heartbeat = time.time()
+
+            # Reset to follower if not already
+            if self.state != NodeState.FOLLOWER:
                 self.state = NodeState.FOLLOWER
 
                 # Call leader callback
-                if self.leader_callback)
+                if self.leader_callback:
+                    self.leader_callback(False)
 
             # Reset election timer
             self._reset_election_timer()
 
             # Mark peer as leader
             with self.peer_lock:
-                if node_id in self.peers, packet: Dict, addr: Tuple) :
+                if node_id in self.peers:
+                    self.peers[node_id].is_leader = True
+
+    def _handle_vote_request(self, packet: Dict, addr: Tuple):
+        """Handle vote request (FR-003)"""
+        node_id = packet.get("node_id")
+        term = packet.get("term", 0)
+
+        # Vote if haven't voted this term
+        if term > self.current_term and (self.voted_for is None or self.voted_for == node_id):
             self.current_term = term
             self.voted_for = node_id
 
             # Send vote response
             vote_response = {
-                "type",
-                "node_id",
-                "term",
-                "vote_granted")
+                "type": "vote_response",
+                "node_id": self.node_id,
+                "term": self.current_term,
+                "vote_granted": True
+            }
+
+            packet_bytes = self._encrypt_packet(vote_response)
             self.socket.sendto(packet_bytes, addr)
 
-    def _handle_vote_response(self, packet: Dict, addr: Tuple) :
+    def _handle_vote_response(self, packet: Dict, addr: Tuple):
+        """Handle vote response (FR-003)"""
+        # This would be collected during election
+        # Simplified implementation
+        pass
+
+    def get_status(self) -> Dict[str, Any]:
+        """
+        Get network status (FR-008)
+
+        Returns:
             Status dictionary
         """
         with self.peer_lock:
             return {
-                "node_id",
-                "state",
-                "is_leader",
-                "leader_id",
-                "current_term",
-                "peer_count"),
+                "node_id": self.node_id,
+                "state": self.state.value,
+                "is_leader": self.state == NodeState.LEADER,
+                "leader_id": self.leader_id,
+                "current_term": self.current_term,
+                "peer_count": len(self.peers),
                 "peers": [
                     {
-                        "node_id",
-                        "address",
-                        "is_leader",
-                        "latency_ms",
-                        "phase_diff",
-                        "last_seen")
+                        "node_id": peer.node_id,
+                        "address": peer.address,
+                        "is_leader": peer.is_leader,
+                        "latency_ms": peer.latency * 1000,
+                        "phase_diff": peer.phase_diff,
+                        "last_seen": peer.last_seen
+                    }
+                    for peer in self.peers.values()
                 ],
-                "packets_sent",
-                "packets_received",
-                "packets_dropped") :
+                "packets_sent": self.packets_sent,
+                "packets_received": self.packets_received,
+                "packets_dropped": self.packets_dropped
+            }
+
+    def get_statistics(self) -> Dict[str, Any]:
+        """
+        Get network statistics (SC-001, SC-002)
+
+        Returns:
             Statistics dictionary
         """
         stats = {
-            "node_id",
-            "state")
+            "node_id": self.node_id,
+            "state": self.state.value
+        }
+
+        # Latency statistics (SC-001)
         if self.latency_history:
             stats["latency"] = {
-                "mean_ms") * 1000,
-                "max_ms") * 1000,
-                "min_ms") * 1000,
-                "target_ms")
+                "mean_ms": statistics.mean(self.latency_history) * 1000,
+                "max_ms": max(self.latency_history) * 1000,
+                "min_ms": min(self.latency_history) * 1000,
+                "target_ms": 5.0  # SC-001
+            }
+
+        # Jitter statistics (SC-001)
         if self.jitter_history:
             stats["jitter"] = {
-                "mean_ms") * 1000,
-                "max_ms") * 1000,
-                "target_ms")
+                "mean_ms": statistics.mean(self.jitter_history) * 1000,
+                "max_ms": max(self.jitter_history) * 1000,
+                "target_ms": 2.0  # SC-001
+            }
+
+        # Phase coherence statistics (SC-002)
         if self.coherence_history:
             stats["coherence"] = {
-                "mean"),
-                "min"),
+                "mean": statistics.mean(self.coherence_history),
+                "min": min(self.coherence_history),
                 "target": 0.99  # SC-002
             }
 
         # Packet statistics
         total_packets = self.packets_sent + self.packets_received
         stats["packets"] = {
-            "sent",
-            "received",
-            "dropped",
+            "sent": self.packets_sent,
+            "received": self.packets_received,
+            "dropped": self.packets_dropped,
             "drop_rate": self.packets_dropped / total_packets if total_packets > 0 else 0
         }
 
         return stats
 
 
 # Self-test
-if __name__ == "__main__")
-    logger.info("PhaseNet Protocol Self-Test")
-    logger.info("=" * 60)
+if __name__ == "__main__":
+    print("=" * 60)
+    print("PhaseNet Protocol Self-Test")
+    print("=" * 60)
 
     # Test node initialization
-    logger.info("\n1. Testing node initialization...")
+    print("\n1. Testing node initialization...")
     config = PhaseNetConfig(
         node_id="test_node_1",
         bind_port=9000,
         enable_logging=True
-
+    )
     node = PhaseNetNode(config)
-    logger.info("   OK)
+    print("   OK: Node initialization")
 
     # Test phase update
-    logger.info("\n2. Testing phase update...")
+    print("\n2. Testing phase update...")
     node.update_phase(
         phi_phase=0.5,
         phi_depth=0.8,
         criticality=1.0,
         coherence=0.7,
         ici=0.3
-
-    logger.info("   OK)
+    )
+    print("   OK: Phase update")
 
     # Test status
-    logger.info("\n3. Testing status...")
+    print("\n3. Testing status...")
     status = node.get_status()
-    logger.info("   OK, peers=%s)", status['state'], status['peer_count'])
+    print(f"   OK: Status (state={status['state']}, peers={status['peer_count']})")
 
     # Test statistics
-    logger.info("\n4. Testing statistics...")
+    print("\n4. Testing statistics...")
     stats = node.get_statistics()
-    logger.info("   OK)", stats['packets'])
+    print(f"   OK: Statistics (packets={stats['packets']})")
 
     # Test encryption if available
-    if ENCRYPTION_AVAILABLE and config.network_key)
+    if ENCRYPTION_AVAILABLE and config.network_key:
+        print("\n5. Testing encryption...")
         config_enc = PhaseNetConfig(
             node_id="test_node_2",
             network_key="test_key_12345",
             enable_encryption=True,
             enable_logging=True
-
+        )
         node_enc = PhaseNetNode(config_enc)
-        logger.info("   OK)
-    else)")
-
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
-    logger.info("Note)
-
+        print("   OK: Encryption initialized")
+    else:
+        print("\n5. Skipping encryption test (cryptography not available)")
+
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
+    print("Note: Full network testing requires multiple running nodes")
diff --git a/server/phi_adaptive_controller.py b/server/phi_adaptive_controller.py
index d5ee92bdeeae729da42425406f5e6486b3a1b2a5..307d6017c7b70593d2143daa6a45e08da7e86a18 100644
--- a/server/phi_adaptive_controller.py
+++ b/server/phi_adaptive_controller.py
@@ -1,336 +1,508 @@
 """
 PhiAdaptiveController - Feature 012: Predictive Phi-Adaptation Engine
 
 Main controller for adaptive Phi modulation based on feedback learning.
 
 Features:
-- FR-001, coherence, criticality) in real time
+- FR-001: Monitor metrics (ICI, coherence, criticality) in real time
 - FR-002: Adjust Phi parameters based on feedback
 - SC-001: Maintain ICI ≈ 0.5 ± 0.05 for > 60s
 - SC-002: Average Phi-update latency <= 200 ms
 - Manual override support
 
 Requirements:
 - FR-001: System MUST monitor metrics in real time
 - FR-002: System MUST adjust Phi based on feedback
 - SC-001: Maintain ICI ≈ 0.5 ± 0.05 for > 60s
 - SC-002: Phi-update latency <= 200 ms
+- SC-004: Manual override responds < 50 ms
+"""
 
 import time
 import threading
 from typing import Optional, Callable, Dict
 from dataclasses import dataclass
 from enum import Enum
 import numpy as np
 
 from .session_memory import SessionMemory, MetricSnapshot
 from .phi_predictor import PhiPredictor, PredictionResult
 
 
 class AdaptiveMode(Enum):
     """Adaptive control modes"""
     OFF = 0           # Adaptive disabled
     REACTIVE = 1      # React to current metrics only
     PREDICTIVE = 2    # Use predictive model
     LEARNING = 3      # Learning + predictive
 
 
 @dataclass
 class AdaptiveConfig:
     """Configuration for adaptive controller"""
-    target_ici)
-    ici_tolerance)
+    target_ici: float = 0.5              # Target ICI value (SC-001)
+    ici_tolerance: float = 0.05          # ICI tolerance (SC-001)
     update_rate_hz: float = 10.0         # Control loop rate
     adaptation_gain: float = 0.3         # How aggressively to adapt
     metric_timeout_s: float = 1.0        # Max time to hold last state
     enable_logging: bool = False
 
 
 @dataclass
 class AdaptiveStatus:
     """Current status of adaptive controller"""
     is_enabled: bool
     mode: AdaptiveMode
     current_ici: float
     target_ici: float
     ici_error: float
     current_phi: float
     predicted_phi: float
     phi_adjustment: float
     update_count: int
     avg_update_latency_ms: float
-    ici_stable_time_s)
+    ici_stable_time_s: float             # How long ICI has been stable (SC-001)
     manual_override_active: bool
 
 
 class PhiAdaptiveController:
     """
     PhiAdaptiveController - Closed-loop Phi adaptation
 
-
-
-
-
+    Handles:
+    - Real-time metric monitoring (FR-001)
+    - Feedback-based Phi adjustment (FR-002)
+    - ICI stability maintenance (SC-001)
+    - Low-latency updates (SC-002)
+    - Manual override (SC-004)
     """
 
     PHI_MIN = 0.618033988749895
     PHI_MAX = 1.618033988749895
 
-    def __init__(self, config: Optional[AdaptiveConfig]) :
+    def __init__(self, config: Optional[AdaptiveConfig] = None):
         """
         Initialize PhiAdaptiveController
 
         Args:
-            config)
+            config: Controller configuration
+        """
+        self.config = config or AdaptiveConfig()
 
         # Components
         self.session_memory = SessionMemory(max_samples=10000)
         self.predictor = PhiPredictor(self.session_memory)
 
         # State
         self.is_enabled = False
         self.mode = AdaptiveMode.REACTIVE
         self.manual_override = False
 
         # Current metrics
         self.current_ici = 0.5
         self.current_coherence = 0.5
         self.current_criticality = 0.5
         self.current_phi = 1.0
         self.current_phi_phase = 0.0
         self.current_phi_depth = 0.5
         self.active_source = "internal"
         self.last_metric_time = time.time()
 
         # Control state
         self.predicted_phi = 1.0
         self.phi_adjustment = 0.0
-        self.ici_stable_start_time)
+        self.ici_stable_start_time: Optional[float] = None
+
+        # Performance tracking
+        self.update_count = 0
+        self.update_latencies = []
+        self.max_latency_samples = 100
+
+        # Threading
+        self.lock = threading.Lock()
         self.control_thread: Optional[threading.Thread] = None
         self.is_running = False
 
         # Callback for Phi updates
-        self.phi_update_callback, float], None]] = None
+        self.phi_update_callback: Optional[Callable[[float, float], None]] = None
 
-    def set_phi_update_callback(self, callback: Callable[[float, float], None]) :
+    def set_phi_update_callback(self, callback: Callable[[float, float], None]):
         """
         Set callback for Phi updates
 
         Args:
-            callback, phi_phase) to call on updates
+            callback: Function(phi_value, phi_phase) to call on updates
         """
         self.phi_update_callback = callback
 
-    def enable(self, mode: AdaptiveMode) :
+    def enable(self, mode: AdaptiveMode = AdaptiveMode.REACTIVE):
         """
         Enable adaptive control
 
         Args:
             mode: Adaptive mode to use
         """
         with self.lock:
-            if self.is_enabled)
+            if self.is_enabled:
+                return
+
+            self.is_enabled = True
+            self.mode = mode
+            self.manual_override = False
+
+            # Start session recording
+            self.session_memory.start_session()
 
             # Start control loop
             self.is_running = True
             self.control_thread = threading.Thread(target=self._control_loop, daemon=True)
             self.control_thread.start()
 
-            if self.config.enable_logging, mode.name)
+            if self.config.enable_logging:
+                print(f"[PhiAdaptive] Enabled in {mode.name} mode")
 
-    def disable(self) :
+    def disable(self):
         """Disable adaptive control"""
         with self.lock:
             if not self.is_enabled:
                 return
 
             self.is_enabled = False
             self.is_running = False
 
             # Stop control loop
-            if self.control_thread)
+            if self.control_thread:
+                self.control_thread.join(timeout=1.0)
 
             # Stop session recording
             self.session_memory.stop_session()
 
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[PhiAdaptive] Disabled")
+
+    def set_manual_override(self, enabled: bool):
+        """
+        Set manual override mode (SC-004)
 
-    def set_manual_override(self, enabled: bool) :
+        Args:
             enabled: True to enable manual override
         """
         with self.lock:
             self.manual_override = enabled
 
             if self.config.enable_logging:
-                logger.info("[PhiAdaptive] Manual override, 'ON' if enabled else 'OFF')
+                print(f"[PhiAdaptive] Manual override: {'ON' if enabled else 'OFF'}")
 
-    def update_metrics(self, ici, coherence, criticality,
-                       phi_value, phi_phase, phi_depth,
-                       active_source))
+    def update_metrics(self, ici: float, coherence: float, criticality: float,
+                       phi_value: float, phi_phase: float, phi_depth: float,
+                       active_source: str = "unknown"):
+        """
+        Update current metrics (FR-001)
 
         Args:
             ici: Current ICI value
             coherence: Current coherence
             criticality: Current criticality
             phi_value: Current Phi value
             phi_phase: Current Phi phase
             phi_depth: Current Phi depth
             active_source: Active Phi source
         """
-        with self.lock)
+        with self.lock:
+            self.current_ici = ici
+            self.current_coherence = coherence
+            self.current_criticality = criticality
+            self.current_phi = phi_value
+            self.current_phi_phase = phi_phase
+            self.current_phi_depth = phi_depth
+            self.active_source = active_source
+            self.last_metric_time = time.time()
 
             # Record snapshot
-            if self.is_enabled,
+            if self.is_enabled:
+                snapshot = MetricSnapshot(
+                    timestamp=self.last_metric_time,
                     ici=ici,
                     coherence=coherence,
                     criticality=criticality,
                     phi_value=phi_value,
                     phi_phase=phi_phase,
                     phi_depth=phi_depth,
                     active_source=active_source
-
+                )
                 self.session_memory.record_snapshot(snapshot)
 
-    def _control_loop(self) :
+    def _control_loop(self):
         """Main control loop for adaptive Phi adjustment"""
         loop_period = 1.0 / self.config.update_rate_hz
 
-        while self.is_running)
+        while self.is_running:
+            loop_start = time.time()
 
-            try)
-                if self.manual_override)
+            try:
+                # Skip if manual override active (SC-004)
+                if self.manual_override:
+                    time.sleep(loop_period)
                     continue
 
                 # Check metric timeout
-                if (time.time() - self.last_metric_time) > self.config.metric_timeout_s)
+                if (time.time() - self.last_metric_time) > self.config.metric_timeout_s:
+                    # Hold last state
+                    time.sleep(loop_period)
                     continue
 
-                with self.lock)
+                with self.lock:
+                    # Compute ICI error
+                    ici_error = self.current_ici - self.config.target_ici
+
+                    # Check ICI stability (SC-001)
                     if abs(ici_error) <= self.config.ici_tolerance:
-                        if self.ici_stable_start_time is None)
+                        if self.ici_stable_start_time is None:
+                            self.ici_stable_start_time = time.time()
                     else:
                         self.ici_stable_start_time = None
 
                     # Determine target Phi
-                    if self.mode == AdaptiveMode.PREDICTIVE or self.mode == AdaptiveMode.LEARNING,
+                    if self.mode == AdaptiveMode.PREDICTIVE or self.mode == AdaptiveMode.LEARNING:
+                        # Use predictor
+                        prediction = self.predictor.predict_phi(
+                            ici=self.current_ici,
                             coherence=self.current_coherence,
                             criticality=self.current_criticality
-
+                        )
                         self.predicted_phi = prediction.predicted_phi
                         target_phi = prediction.predicted_phi
-                    else, self.PHI_MIN, self.PHI_MAX)
+                    else:
+                        # Reactive mode - simple feedback control
+                        # ICI too high → decrease Phi
+                        # ICI too low → increase Phi
+                        phi_correction = -ici_error * self.config.adaptation_gain
+                        target_phi = self.current_phi + phi_correction
+                        self.predicted_phi = target_phi
+
+                    # Clamp target
+                    target_phi = np.clip(target_phi, self.PHI_MIN, self.PHI_MAX)
 
                     # Compute adjustment
                     self.phi_adjustment = target_phi - self.current_phi
 
                     # Apply adjustment via callback
-                    if self.phi_update_callback and abs(self.phi_adjustment) > 0.001, new_phase)
+                    if self.phi_update_callback and abs(self.phi_adjustment) > 0.001:
+                        # Update Phi
+                        new_phi = target_phi
+                        new_phase = self.current_phi_phase  # Keep phase for now
+
+                        self.phi_update_callback(new_phi, new_phase)
 
                         self.update_count += 1
 
                         # Track latency (SC-002)
                         latency_ms = (time.time() - loop_start) * 1000
                         self.update_latencies.append(latency_ms)
-                        if len(self.update_latencies) > self.max_latency_samples)
+                        if len(self.update_latencies) > self.max_latency_samples:
+                            self.update_latencies.pop(0)
 
                     # Learning mode - update predictor
-                    if self.mode == AdaptiveMode.LEARNING) >= 100:
+                    if self.mode == AdaptiveMode.LEARNING:
+                        if self.session_memory.get_sample_count() >= 100:
                             # Periodically re-learn
-                            if self.update_count % 100 == 0)
+                            if self.update_count % 100 == 0:
+                                self.predictor.learn_from_session()
 
                         # Check for divergence
-                        if self.predictor.check_divergence())
+                        if self.predictor.check_divergence():
+                            self.predictor.reset_model()
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[PhiAdaptive] Control loop error, e)
+                    print(f"[PhiAdaptive] Control loop error: {e}")
 
             # Sleep to maintain loop rate
             elapsed = time.time() - loop_start
             sleep_time = max(0, loop_period - elapsed)
             time.sleep(sleep_time)
 
-    @lru_cache(maxsize=128)
-    def get_status(self) :
+    def get_status(self) -> AdaptiveStatus:
         """
         Get current adaptive control status
 
         Returns:
             AdaptiveStatus with current state
         """
-        with self.lock)
-            if self.ici_stable_start_time) - self.ici_stable_start_time
-            else)
-            if len(self.update_latencies) > 0)
-            else,
+        with self.lock:
+            ici_error = self.current_ici - self.config.target_ici
+
+            # Compute stable time (SC-001)
+            if self.ici_stable_start_time:
+                stable_time = time.time() - self.ici_stable_start_time
+            else:
+                stable_time = 0.0
+
+            # Compute average latency (SC-002)
+            if len(self.update_latencies) > 0:
+                avg_latency = np.mean(self.update_latencies)
+            else:
+                avg_latency = 0.0
+
+            return AdaptiveStatus(
+                is_enabled=self.is_enabled,
                 mode=self.mode,
                 current_ici=self.current_ici,
                 target_ici=self.config.target_ici,
                 ici_error=ici_error,
                 current_phi=self.current_phi,
                 predicted_phi=self.predicted_phi,
                 phi_adjustment=self.phi_adjustment,
                 update_count=self.update_count,
                 avg_update_latency_ms=avg_latency,
                 ici_stable_time_s=stable_time,
                 manual_override_active=self.manual_override
+            )
 
-    @lru_cache(maxsize=128)
-    def learn_from_current_session(self) :
+    def learn_from_current_session(self) -> bool:
         """
         Trigger learning from current session
 
-    @lru_cache(maxsize=128)
-    def get_session_stats(self) : %s (target)", status.current_ici, status.target_ici)
-    logger.error("   ICI error, status.ici_error)
-    logger.info("   Current Phi, status.current_phi)
-    logger.info("   Predicted Phi, status.predicted_phi)
-    logger.info("   Update count, status.update_count)
-    logger.info("   Avg latency, status.avg_update_latency_ms)
+        Returns:
+            True if learning successful
+        """
+        return self.predictor.learn_from_session()
+
+    def get_session_stats(self) -> Optional[Dict]:
+        """Get current session statistics"""
+        stats = self.session_memory.compute_stats()
+        if stats:
+            from dataclasses import asdict
+            return asdict(stats)
+        return None
+
+    def save_session(self, filepath: str) -> bool:
+        """Save current session to file"""
+        return self.session_memory.save_session(filepath)
+
+    def load_session(self, filepath: str) -> bool:
+        """Load session from file"""
+        return self.session_memory.load_session(filepath)
+
+
+# Self-test function
+def _self_test():
+    """Run basic self-test of PhiAdaptiveController"""
+    print("=" * 60)
+    print("PhiAdaptiveController Self-Test")
+    print("=" * 60)
+    print()
+
+    # Create controller
+    print("1. Creating PhiAdaptiveController...")
+    config = AdaptiveConfig(
+        target_ici=0.5,
+        ici_tolerance=0.05,
+        update_rate_hz=20.0,
+        enable_logging=True
+    )
+    controller = PhiAdaptiveController(config)
+    print("   [OK] Controller created")
+    print()
+
+    # Track Phi updates
+    phi_updates = []
+
+    def phi_callback(phi, phase):
+        phi_updates.append((time.time(), phi, phase))
+
+    controller.set_phi_update_callback(phi_callback)
+
+    # Enable adaptive control
+    print("2. Enabling adaptive control (REACTIVE mode)...")
+    controller.enable(AdaptiveMode.REACTIVE)
+    time.sleep(0.2)
+    status = controller.get_status()
+    print(f"   [OK] Enabled: {status.is_enabled}, Mode: {status.mode.name}")
+    print()
+
+    # Simulate metric updates
+    print("3. Simulating metric updates for 2 seconds...")
+    start_time = time.time()
+    update_count = 0
+
+    while (time.time() - start_time) < 2.0:
+        # Simulate ICI oscillating around target
+        t = time.time() - start_time
+        ici = 0.5 + 0.08 * np.sin(t * 2.0)
+        coherence = 0.6
+        criticality = 0.4
+        phi = 1.0  # Will be adjusted by controller
+
+        controller.update_metrics(
+            ici=ici,
+            coherence=coherence,
+            criticality=criticality,
+            phi_value=phi,
+            phi_phase=0.0,
+            phi_depth=0.5,
+            active_source="test"
+        )
+
+        update_count += 1
+        time.sleep(0.05)  # 20 Hz
+
+    print(f"   [OK] Sent {update_count} metric updates")
+    print(f"   [OK] Received {len(phi_updates)} Phi updates")
+    print()
+
+    # Check status
+    print("4. Checking adaptive control status...")
+    status = controller.get_status()
+    print(f"   ICI: {status.current_ici:.3f} (target: {status.target_ici:.3f})")
+    print(f"   ICI error: {status.ici_error:.3f}")
+    print(f"   Current Phi: {status.current_phi:.3f}")
+    print(f"   Predicted Phi: {status.predicted_phi:.3f}")
+    print(f"   Update count: {status.update_count}")
+    print(f"   Avg latency: {status.avg_update_latency_ms:.2f} ms")
 
     latency_ok = status.avg_update_latency_ms <= 200
-    logger.error("   [%s] Latency %s SC-002 (<= 200 ms)", 'OK' if latency_ok else 'FAIL', 'meets' if latency_ok else 'exceeds')
-    logger.info(str())
+    print(f"   [{'OK' if latency_ok else 'FAIL'}] Latency {'meets' if latency_ok else 'exceeds'} SC-002 (<= 200 ms)")
+    print()
 
     # Test manual override
-    logger.info("5. Testing manual override (SC-004)...")
+    print("5. Testing manual override (SC-004)...")
     override_start = time.time()
     controller.set_manual_override(True)
     override_latency = (time.time() - override_start) * 1000
 
     status = controller.get_status()
     override_ok = status.manual_override_active and override_latency < 50
-    logger.info("   Override latency, override_latency)
-    logger.error("   [%s] Manual override %s < 50 ms", 'OK' if override_ok else 'FAIL', 'responds' if override_ok else 'exceeds')
+    print(f"   Override latency: {override_latency:.2f} ms")
+    print(f"   [{'OK' if override_ok else 'FAIL'}] Manual override {'responds' if override_ok else 'exceeds'} < 50 ms")
 
     controller.set_manual_override(False)
-    logger.info(str())
+    print()
 
     # Disable controller
-    logger.info("6. Disabling adaptive control...")
+    print("6. Disabling adaptive control...")
     controller.disable()
     status = controller.get_status()
-    logger.info("   [OK] Disabled, not status.is_enabled)
-    logger.info(str())
+    print(f"   [OK] Disabled: {not status.is_enabled}")
+    print()
 
     # Session stats
-    logger.info("7. Checking session statistics...")
+    print("7. Checking session statistics...")
     stats = controller.get_session_stats()
     if stats:
-        logger.info("   Duration, stats['duration'])
-        logger.info("   Sample count, stats['sample_count'])
-        logger.info("   Avg ICI, stats['avg_ici'])
-        logger.info("   ICI stability, stats['ici_stability_score'])
-        logger.info("   [OK] Session stats available")
-    logger.info(str())
+        print(f"   Duration: {stats['duration']:.2f} s")
+        print(f"   Sample count: {stats['sample_count']}")
+        print(f"   Avg ICI: {stats['avg_ici']:.3f}")
+        print(f"   ICI stability: {stats['ici_stability_score']:.3f}")
+        print("   [OK] Session stats available")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_modulator.py b/server/phi_modulator.py
index 3138ae6c3d27e90ae50440c9df04a0af33e97e67..6bb94c97380b86ac36f9b7db5b04170e05c406ff 100644
--- a/server/phi_modulator.py
+++ b/server/phi_modulator.py
@@ -1,279 +1,325 @@
 """
 PhiModulator - Golden Ratio (Φ) Modulation Generator
 
 Generates modulation signals based on the golden ratio (Φ = 1.618...) for
 real-time audio processing with chromatic field characteristics.
 
-Mathematical basis) / 2 ≈ 1.618033988749895
+Mathematical basis:
+- Φ = (1 + √5) / 2 ≈ 1.618033988749895
 - Φ^-1 = Φ - 1 ≈ 0.618033988749895
+- Modulation frequency: f_mod = sample_rate * Φ^-1
+"""
 
 import numpy as np
 from typing import Optional
 
 
 class PhiModulator:
     """
     Golden ratio modulation generator for D-ASE audio engine
 
-    Generates Φ-modulated control signals with, 2π]
+    Generates Φ-modulated control signals with:
+    - Phase control [0, 2π]
     - Depth control [0, 1]
     - Multi-channel support with Φ-rotated phases
     """
 
     # Mathematical constants
     PHI = 1.618033988749895  # Golden ratio
     PHI_INV = 0.618033988749895  # 1/Φ = Φ - 1
     PHI_SQ = 2.618033988749895  # Φ^2 = Φ + 1
     TWO_PI = 2.0 * np.pi
 
-    def __init__(self, sample_rate: int) :
+    def __init__(self, sample_rate: int = 48000):
         """
         Initialize Φ-modulator
 
         Args:
-            sample_rate)
+            sample_rate: Audio sample rate in Hz (48000 for spec compliance)
         """
         self.sample_rate = sample_rate
         self.phase_accumulator = 0.0
 
         # Calculate fundamental Φ-modulation frequency
-        # For 48kHz,665 Hz
+        # For 48kHz: f_phi ≈ 29,665 Hz
         self.phi_frequency = sample_rate * self.PHI_INV
 
-        logger.info("[PhiModulator] Initialized @ %sHz", sample_rate)
-        logger.info("[PhiModulator] Φ-frequency, self.phi_frequency)
+        print(f"[PhiModulator] Initialized @ {sample_rate}Hz")
+        print(f"[PhiModulator] Φ-frequency: {self.phi_frequency:.2f} Hz")
 
-    @lru_cache(maxsize=128)
     def generateModulation(self,
-                          phi_phase,
-                          phi_depth,
-                          num_samples) :
+                          phi_phase: float,
+                          phi_depth: float,
+                          num_samples: int) -> np.ndarray:
         """
         Generate Φ-modulated control signal for one audio block
 
         Args:
-            phi_phase, 2π]
-            phi_depth, 1] (0=no modulation, 1=full)
+            phi_phase: Phase offset in radians [0, 2π]
+            phi_depth: Modulation depth [0, 1] (0=no modulation, 1=full)
             num_samples: Number of samples to generate
 
         Returns:
             float32[num_samples] modulation envelope
-                Range, 1+depth] when depth ≤ 1
+                Range: [1-depth, 1+depth] when depth ≤ 1
 
-        Algorithm)
+        Algorithm:
+            modulation[n] = 1.0 + depth * sin(2π * Φ^-1 * n/SR + phase)
 
-        This produces a sinusoid at the golden ratio frequency with)
+        This produces a sinusoid at the golden ratio frequency with:
+        - DC offset of 1.0 (for multiplicative modulation)
         - Peak amplitude controlled by depth
         - Phase controlled by phi_phase
         """
         # Validate inputs
         phi_depth = np.clip(phi_depth, 0.0, 1.0)
         phi_phase = phi_phase % self.TWO_PI
 
         # Time vector for this block
         t = np.arange(num_samples, dtype=np.float32) / self.sample_rate
 
         # Generate Φ-modulated sinusoid
         # Frequency is based on golden ratio relationship to sample rate
         angular_freq = self.TWO_PI * self.phi_frequency
         modulation = 1.0 + phi_depth * np.sin(angular_freq * t + phi_phase)
 
         return modulation.astype(np.float32)
 
-    @lru_cache(maxsize=128)
     def generateMultiChannelModulation(self,
-                                      phi_phase,
-                                      phi_depth,
-                                      num_samples,
-                                      num_channels) :
+                                      phi_phase: float,
+                                      phi_depth: float,
+                                      num_samples: int,
+                                      num_channels: int = 8) -> np.ndarray:
         """
         Generate Φ-modulated signals for multiple channels with Φ-rotated phases
 
         Each channel receives a phase-shifted version based on golden ratio rotation.
         This creates a natural spiraling phase relationship across channels.
 
         Args:
-            phi_phase, 2π]
-            phi_depth, 1]
+            phi_phase: Base phase offset [0, 2π]
+            phi_depth: Modulation depth [0, 1]
             num_samples: Number of samples per channel
-            num_channels: Number of channels (default)
+            num_channels: Number of channels (default: 8)
 
-        Returns, num_samples] multi-channel modulation
+        Returns:
+            float32[num_channels, num_samples] multi-channel modulation
 
-        Phase rotation formula, num_samples), dtype=np.float32)
+        Phase rotation formula:
+            channel[i]_phase = phi_phase + i * Φ^-1 * 2π
+        """
+        # Initialize output array
+        output = np.zeros((num_channels, num_samples), dtype=np.float32)
 
         # Generate modulation for each channel with Φ-rotated phase
-        for ch_idx in range(num_channels)) % self.TWO_PI
+        for ch_idx in range(num_channels):
+            # Calculate phase offset for this channel based on Φ-rotation
+            channel_phase_offset = ch_idx * self.PHI_INV * self.TWO_PI
+            total_phase = (phi_phase + channel_phase_offset) % self.TWO_PI
 
             # Generate modulation for this channel
             output[ch_idx] = self.generateModulation(total_phase, phi_depth, num_samples)
 
         return output
 
-    @lru_cache(maxsize=128)
     def applyModulation(self,
-                       signal,
-                       modulation,
-                       mode) :
+                       signal: np.ndarray,
+                       modulation: np.ndarray,
+                       mode: str = 'amplitude') -> np.ndarray:
         """
         Apply modulation to audio signal
 
         Args:
-            signal)
-            modulation)
+            signal: Input signal (any shape)
+            modulation: Modulation envelope (must broadcast with signal)
             mode: Modulation mode
+                - 'amplitude': Multiply signal by modulation (AM)
+                - 'ring': Ring modulation (bipolar, centered at 0)
+                - 'phase': Apply as phase modulation (experimental)
 
-
-
+        Returns:
+            Modulated signal (same shape as input)
         """
         if mode == 'amplitude':
             # Standard amplitude modulation
             return signal * modulation
 
         elif mode == 'ring':
-            # Ring modulation)
+            # Ring modulation: remove DC offset from modulation
+            ring_mod = (modulation - 1.0)
             return signal * ring_mod
 
-        elif mode == 'phase')
+        elif mode == 'phase':
+            # Phase modulation (experimental)
             # Interpret modulation as phase shift in radians
             phase_shift = (modulation - 1.0) * np.pi
             # Apply via Hilbert transform phase rotation
             from scipy import signal as sp_signal
             analytic = sp_signal.hilbert(signal, axis=-1)
             magnitude = np.abs(analytic)
             phase = np.angle(analytic) + phase_shift
             return magnitude * np.cos(phase)
 
         else:
-            raise ValueError(f"Unknown modulation mode)
+            raise ValueError(f"Unknown modulation mode: {mode}")
 
-    def getPhiRelationships(self) :
+    def getPhiRelationships(self) -> dict:
         """
         Get golden ratio mathematical relationships
 
         Returns:
             Dictionary of Φ-related constants and relationships
         """
         return {
-            'phi',
-            'phi_inverse',
-            'phi_squared',
-            'phi_frequency_hz',
-            'phi_period_samples',
-            'phi_period_ms',
-            'sample_rate',
+            'phi': self.PHI,
+            'phi_inverse': self.PHI_INV,
+            'phi_squared': self.PHI_SQ,
+            'phi_frequency_hz': self.phi_frequency,
+            'phi_period_samples': self.sample_rate / self.phi_frequency,
+            'phi_period_ms': 1000.0 / self.phi_frequency,
+            'sample_rate': self.sample_rate,
             # Interesting relationships
-            'phi_minus_1',  # = PHI_INV
-            'phi_times_phi_inv',  # = 1
-            'phi_squared_minus_phi',  # = 1
+            'phi_minus_1': self.PHI - 1.0,  # = PHI_INV
+            'phi_times_phi_inv': self.PHI * self.PHI_INV,  # = 1
+            'phi_squared_minus_phi': self.PHI_SQ - self.PHI,  # = 1
         }
 
-    @lru_cache(maxsize=128)
-    def generateFibonacciPhases(self, num_phases) :
+    def generateFibonacciPhases(self, num_phases: int = 8) -> np.ndarray:
         """
         Generate phase offsets based on Fibonacci sequence relationship to Φ
 
         The ratio of consecutive Fibonacci numbers approaches Φ as n→∞.
         This creates a natural spiral phase distribution.
 
         Args:
             num_phases: Number of phases to generate
 
-        Returns, 2π)
+        Returns:
+            float32[num_phases] phase offsets in radians [0, 2π)
         """
         # Generate Fibonacci-like phase distribution
         # Each phase rotates by Φ^-1 * 2π (golden angle ≈ 137.5°)
         golden_angle = self.PHI_INV * self.TWO_PI
 
         phases = np.array([
             (i * golden_angle) % self.TWO_PI
             for i in range(num_phases)
         ], dtype=np.float32)
 
         return phases
 
     def visualizeModulation(self,
-                           phi_phase,
-                           phi_depth,
-                           duration_ms,
-                           num_channels):
+                           phi_phase: float = 0.0,
+                           phi_depth: float = 0.5,
+                           duration_ms: float = 10.0,
+                           num_channels: int = 3):
         """
         Generate visualization data for Φ-modulation
 
         Args:
-            phi_phase, 2π]
-            phi_depth, 1]
+            phi_phase: Phase offset [0, 2π]
+            phi_depth: Modulation depth [0, 1]
             duration_ms: Duration to visualize in milliseconds
             num_channels: Number of channels to show
 
+        Returns:
+            Dictionary with visualization data
+        """
+        num_samples = int(duration_ms * self.sample_rate / 1000)
+
         # Generate multi-channel modulation
         mod = self.generateMultiChannelModulation(
             phi_phase, phi_depth, num_samples, num_channels
+        )
 
         # Time axis
         t_ms = np.arange(num_samples) / self.sample_rate * 1000
 
         return {
-            'time_ms',
-            'modulation',
-            'num_channels',
-            'phi_frequency',
-            'duration_ms') :
-            logger.info("   %s, key, value)
+            'time_ms': t_ms,
+            'modulation': mod,
+            'num_channels': num_channels,
+            'phi_frequency': self.phi_frequency,
+            'duration_ms': duration_ms
+        }
+
+    def reset(self):
+        """Reset internal phase accumulator"""
+        self.phase_accumulator = 0.0
+        print("[PhiModulator] Phase accumulator reset")
+
+
+# Self-test function
+def _self_test():
+    """Run basic self-test of PhiModulator"""
+    print("=" * 60)
+    print("PhiModulator Self-Test")
+    print("=" * 60)
+
+    try:
+        # Create modulator
+        print("\n1. Initializing modulator...")
+        modulator = PhiModulator(sample_rate=48000)
+        print("   ✓ Modulator initialized")
+
+        # Display Φ relationships
+        print("\n2. Golden ratio relationships:")
+        relationships = modulator.getPhiRelationships()
+        for key, value in relationships.items():
+            print(f"   {key}: {value}")
 
         # Generate single-channel modulation
-        logger.info("\n3. Generating single-channel modulation...")
+        print("\n3. Generating single-channel modulation...")
         mod_single = modulator.generateModulation(
             phi_phase=0.0,
             phi_depth=0.5,
             num_samples=512
-
-        logger.info("   ✓ Generated %s samples", len(mod_single))
-        logger.info("   Range, %s]", np.min(mod_single), np.max(mod_single))
-        logger.info("   Mean, np.mean(mod_single))
+        )
+        print(f"   ✓ Generated {len(mod_single)} samples")
+        print(f"   Range: [{np.min(mod_single):.3f}, {np.max(mod_single):.3f}]")
+        print(f"   Mean: {np.mean(mod_single):.3f}")
 
         # Generate multi-channel modulation
-        logger.info("\n4. Generating multi-channel modulation...")
+        print("\n4. Generating multi-channel modulation...")
         mod_multi = modulator.generateMultiChannelModulation(
             phi_phase=0.0,
             phi_depth=0.75,
             num_samples=512,
             num_channels=8
-
-        logger.info("   ✓ Generated %s channels × %s samples", mod_multi.shape[0], mod_multi.shape[1])
+        )
+        print(f"   ✓ Generated {mod_multi.shape[0]} channels × {mod_multi.shape[1]} samples")
 
         # Test different modes
-        logger.info("\n5. Testing modulation modes...")
+        print("\n5. Testing modulation modes...")
         test_signal = np.sin(2 * np.pi * 1000 * np.arange(512) / 48000)
         test_mod = mod_single
 
         am_output = modulator.applyModulation(test_signal, test_mod, mode='amplitude')
-        logger.info("   Amplitude modulation, am_output.shape)
+        print(f"   Amplitude modulation: {am_output.shape}")
 
         ring_output = modulator.applyModulation(test_signal, test_mod, mode='ring')
-        logger.info("   Ring modulation, ring_output.shape)
+        print(f"   Ring modulation: {ring_output.shape}")
 
-        logger.info("   ✓ All modulation modes working")
+        print("   ✓ All modulation modes working")
 
         # Test Fibonacci phases
-        logger.info("\n6. Fibonacci phase distribution...")
+        print("\n6. Fibonacci phase distribution...")
         fib_phases = modulator.generateFibonacciPhases(num_phases=8)
-        logger.info("   Phases (radians), fib_phases)
-        logger.info("   Phases (degrees), np.degrees(fib_phases))
-        logger.info("   ✓ Fibonacci phases generated")
+        print(f"   Phases (radians): {fib_phases}")
+        print(f"   Phases (degrees): {np.degrees(fib_phases)}")
+        print("   ✓ Fibonacci phases generated")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_modulator_controller.py b/server/phi_modulator_controller.py
index d897ec37af8c852d110aefdd67378606362f70eb..24cd61a2024a2015fed34817e935b9934af6958d 100644
--- a/server/phi_modulator_controller.py
+++ b/server/phi_modulator_controller.py
@@ -1,234 +1,410 @@
 """
 Enhanced PhiModulator Controller - Multi-Source Φ Modulation Manager
 
 Manages multiple Φ-modulation sources with smooth crossfading and mode switching.
 Integrates Manual, Audio, MIDI, Sensor, and Internal sources.
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
-from typing import Optional
 
 import numpy as np
 from typing import Optional, Dict, Literal
 import time
 import os
 from datetime import datetime
 
 from .phi_sources import (
     PhiSource, PhiState,
     ManualSource, AudioEnvelopeSource, InternalOscillatorSource,
     MIDISource, SensorSource
+)
+
 
 PhiMode = Literal["manual", "audio", "midi", "sensor", "internal"]
 
 
 class PhiModulatorController:
     """
     Enhanced Φ-Modulation Controller
 
-
+    Features:
+    - Multi-source management (5 modes)
+    - Smooth crossfading between sources (<100ms)
     - Automatic fallback to internal oscillator
     - Logging and state monitoring
     - WebSocket-ready state export
     """
 
     CROSSFADE_TIME = 0.1  # 100ms crossfade
 
     def __init__(self,
-                 sample_rate,
-                 block_size,
-                 log_dir):
+                 sample_rate: int = 48000,
+                 block_size: int = 512,
+                 log_dir: Optional[str] = None):
         """
         Initialize PhiModulator controller
 
         Args:
-            sample_rate)
-            block_size)
-            log_dir)
+            sample_rate: Audio sample rate (48000 Hz)
+            block_size: Audio block size (512 samples)
+            log_dir: Directory for logging (None = logs/phi_modulator/)
         """
         self.sample_rate = sample_rate
         self.block_size = block_size
 
         # Initialize all sources
-        self.sources, PhiSource] = {
-            "manual"),
-            "audio", attack_ms=20, release_ms=100),
-            "midi"),
-            "sensor"),
-            "internal", frequency=0.1)
+        self.sources: Dict[PhiMode, PhiSource] = {
+            "manual": ManualSource(sample_rate),
+            "audio": AudioEnvelopeSource(sample_rate, attack_ms=20, release_ms=100),
+            "midi": MIDISource(sample_rate),
+            "sensor": SensorSource(sample_rate),
+            "internal": InternalOscillatorSource(sample_rate, frequency=0.1)
         }
 
         # Active mode and crossfading
         self.current_mode: PhiMode = "internal"
-        self.previous_mode, 1.0 = current
+        self.previous_mode: Optional[PhiMode] = None
+        self.crossfade_progress = 1.0  # 0.0 = prev, 1.0 = current
         self.crossfade_start_time = 0.0
 
         # Activate internal source by default
         self.sources["internal"].activate()
 
         # State tracking
         self.current_state = PhiState(source="internal")
         self.state_history = []
         self.max_history = 1000
 
         # Logging
-        if log_dir is None), "..", "logs", "phi_modulator")
+        if log_dir is None:
+            log_dir = os.path.join(os.path.dirname(__file__), "..", "logs", "phi_modulator")
         self.log_dir = log_dir
         os.makedirs(self.log_dir, exist_ok=True)
 
         self.log_file = None
         self._init_logging()
 
-        logger.info("[PhiModulatorController] Initialized @ %sHz, block=%s", sample_rate, block_size)
-        logger.info("[PhiModulatorController] Default mode, self.current_mode)
-
-    def _init_logging(self) :
-            logger.warning("[PhiModulatorController] Warning: Could not init logging, e)
+        print(f"[PhiModulatorController] Initialized @ {sample_rate}Hz, block={block_size}")
+        print(f"[PhiModulatorController] Default mode: {self.current_mode}")
+
+    def _init_logging(self):
+        """Initialize logging to file"""
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        log_path = os.path.join(self.log_dir, f"phi_session_{timestamp}.log")
+
+        try:
+            self.log_file = open(log_path, 'w')
+            self.log_file.write(f"# Φ-Modulator Session Log\n")
+            self.log_file.write(f"# Started: {datetime.now().isoformat()}\n")
+            self.log_file.write(f"# Sample Rate: {self.sample_rate} Hz\n")
+            self.log_file.write(f"# Block Size: {self.block_size} samples\n")
+            self.log_file.write("timestamp,mode,phase,depth,frequency\n")
+            self.log_file.flush()
+            print(f"[PhiModulatorController] Logging to {log_path}")
+        except Exception as e:
+            print(f"[PhiModulatorController] Warning: Could not init logging: {e}")
             self.log_file = None
 
-    def _log_state(self, state) :
+    def _log_state(self, state: PhiState):
         """Log current state to file"""
         if self.log_file and not self.log_file.closed:
             try:
                 self.log_file.write(
-                    f"{state.timestamp,{state.source},"
-                    f"{state.phase,{state.depth,{state.frequency)
+                    f"{state.timestamp:.3f},{state.source},"
+                    f"{state.phase:.4f},{state.depth:.4f},{state.frequency:.4f}\n"
+                )
             except Exception as e:
-                logger.error("[PhiModulatorController] Warning: Logging failed, e)
-
-    def set_mode(self, mode) :
-
+                print(f"[PhiModulatorController] Warning: Logging failed: {e}")
 
+    def set_mode(self, mode: PhiMode):
         """
         Switch to a new Φ-modulation mode
 
         Initiates crossfade from current to new mode
 
         Args:
-            mode, 'audio', 'midi', 'sensor', 'internal')
+            mode: New mode ('manual', 'audio', 'midi', 'sensor', 'internal')
         """
         if mode not in self.sources:
-            raise ValueError(f"Invalid mode))}")
+            raise ValueError(f"Invalid mode: {mode}. Must be one of {list(self.sources.keys())}")
 
         if mode == self.current_mode:
             return  # Already in this mode
 
-        logger.info("[PhiModulatorController] Switching mode, self.current_mode, mode)
+        print(f"[PhiModulatorController] Switching mode: {self.current_mode} → {mode}")
 
         # Start crossfade
         self.previous_mode = self.current_mode
         self.current_mode = mode
         self.crossfade_progress = 0.0
         self.crossfade_start_time = time.time()
 
         # Deactivate previous, activate new
-        if self.previous_mode)
+        if self.previous_mode:
+            self.sources[self.previous_mode].deactivate()
         self.sources[self.current_mode].activate()
 
         # Log mode change
         self._log_state(PhiState(source=f"MODE_CHANGE_{mode}"))
 
-    @lru_cache(maxsize=128)
-    def update(self, audio_block, **kwargs) :
-            audio_block)
+    def update(self, audio_block: Optional[np.ndarray] = None, **kwargs) -> PhiState:
+        """
+        Update Φ-modulation state
+
+        This is called at audio block rate (typically every 512 samples @ 48kHz)
+
+        Args:
+            audio_block: Audio input (for audio-driven mode)
             **kwargs: Additional parameters for specific sources
 
+        Returns:
+            Current PhiState (potentially crossfaded between sources)
         """
         current_time = time.time()
 
         # Update crossfade progress
-        if self.crossfade_progress < 1.0, elapsed / self.CROSSFADE_TIME)
+        if self.crossfade_progress < 1.0:
+            elapsed = current_time - self.crossfade_start_time
+            self.crossfade_progress = min(1.0, elapsed / self.CROSSFADE_TIME)
 
         # Update current source
         current_source = self.sources[self.current_mode]
 
         # Pass appropriate data to source
-        if self.current_mode == "audio" and audio_block is not None, **kwargs)
-        else)
+        if self.current_mode == "audio" and audio_block is not None:
+            current_state = current_source.update(audio_block=audio_block, **kwargs)
+        else:
+            current_state = current_source.update(**kwargs)
 
         # If crossfading, blend with previous source
-        if self.crossfade_progress < 1.0 and self.previous_mode)
+        if self.crossfade_progress < 1.0 and self.previous_mode:
+            prev_source = self.sources[self.previous_mode]
+            prev_state = prev_source.get_state()
 
             # Blend phase and depth
             alpha = self.crossfade_progress  # 0 = prev, 1 = current
 
             # Use smooth crossfade (equal power)
             alpha_smooth = 0.5 * (1.0 - np.cos(alpha * np.pi))
 
             blended_phase = (
                 (1.0 - alpha_smooth) * prev_state.phase +
                 alpha_smooth * current_state.phase
-
+            )
             blended_depth = (
                 (1.0 - alpha_smooth) * prev_state.depth +
                 alpha_smooth * current_state.depth
+            )
 
             self.current_state = PhiState(
                 phase=blended_phase,
                 depth=blended_depth,
                 source=f"{self.previous_mode}→{self.current_mode}",
                 frequency=current_state.frequency
+            )
+        else:
+            self.current_state = current_state
 
-        else)
-        if len(self.state_history) > self.max_history)
+        # Store in history
+        self.state_history.append(self.current_state)
+        if len(self.state_history) > self.max_history:
+            self.state_history.pop(0)
 
         # Log periodically (every 100 updates to avoid overhead)
-        if len(self.state_history) % 100 == 0)
+        if len(self.state_history) % 100 == 0:
+            self._log_state(self.current_state)
+
+        return self.current_state
 
+    def get_state(self) -> PhiState:
+        """Get current Φ state without updating"""
         return self.current_state
 
-    def get_state(self) :
+    def get_state_dict(self) -> Dict:
+        """Get state as dictionary for JSON serialization"""
+        state_dict = self.current_state.to_dict()
+        state_dict['crossfade_progress'] = float(self.crossfade_progress)
+        state_dict['available_modes'] = list(self.sources.keys())
+        return state_dict
+
+    def get_mode(self) -> PhiMode:
+        """Get current active mode"""
+        return self.current_mode
+
+    # Convenience methods for mode-specific controls
+
+    def set_manual_params(self, phase: Optional[float] = None, depth: Optional[float] = None):
         """Set manual mode parameters"""
         manual = self.sources["manual"]
-        if phase is not None)
-        if depth is not None)
+        if phase is not None:
+            manual.set_phase(phase)
+        if depth is not None:
+            manual.set_depth(depth)
 
-    def set_audio_envelope_params(self, attack_ms: Optional[float], release_ms: Optional[float]) :
+    def set_audio_envelope_params(self, attack_ms: Optional[float] = None, release_ms: Optional[float] = None):
         """Configure audio envelope follower"""
         audio = self.sources["audio"]
-        if attack_ms is not None)
-        if release_ms is not None)
-
-    def set_internal_frequency(self, frequency: float) : int) : Optional[str]) :
+        if attack_ms is not None:
+            audio.set_attack(attack_ms)
+        if release_ms is not None:
+            audio.set_release(release_ms)
+
+    def set_internal_frequency(self, frequency: float):
+        """Set internal oscillator frequency [0.01, 10 Hz]"""
+        internal = self.sources["internal"]
+        internal.set_frequency(frequency)
+
+    def set_midi_cc1(self, value: int):
+        """Set MIDI CC1 value [0, 127]"""
+        midi = self.sources["midi"]
+        midi.set_cc1(value)
+
+    def set_sensor_data(self,
+                       heart_rate: Optional[float] = None,
+                       gsr: Optional[float] = None,
+                       accel: Optional[tuple] = None):
+        """Update sensor data"""
+        sensor = self.sources["sensor"]
+        sensor.update(heart_rate=heart_rate, gsr=gsr, accel=accel)
+
+    def detect_midi_ports(self) -> list:
+        """Detect available MIDI input ports"""
+        midi = self.sources["midi"]
+        return midi.detect_ports()
+
+    def connect_midi(self, port_name: Optional[str] = None):
+        """Connect to MIDI input port"""
+        midi = self.sources["midi"]
+        midi.connect(port_name)
+
+    def get_metrics(self) -> Dict:
         """
         Get comprehensive metrics for monitoring
 
         Returns:
             Dictionary with:
                 - current_state: Current Φ state
                 - mode: Active mode
-                - crossfade_progress, 1]
+                - crossfade_progress: Crossfade progress [0, 1]
                 - history_size: Number of states in history
                 - uptime_seconds: Time since initialization
         """
         return {
-            'current_state'),
-            'mode',
-            'crossfade_progress'),
-            'history_size'),
+            'current_state': self.current_state.to_dict(),
+            'mode': self.current_mode,
+            'crossfade_progress': float(self.crossfade_progress),
+            'history_size': len(self.state_history),
             'source_states': {
-                mode).to_dict()
+                mode: source.get_state().to_dict()
                 for mode, source in self.sources.items()
             }
         }
 
-    def reset(self) :
-            self.log_file.write(f"# Session ended).isoformat()}\n")
+    def reset(self):
+        """Reset all sources and state"""
+        print("[PhiModulatorController] Resetting all sources")
+
+        for source in self.sources.values():
+            source.reset()
+
+        self.current_mode = "internal"
+        self.previous_mode = None
+        self.crossfade_progress = 1.0
+        self.current_state = PhiState(source="internal")
+        self.state_history.clear()
+
+        self.sources["internal"].activate()
+
+    def close(self):
+        """Cleanup and close resources"""
+        print("[PhiModulatorController] Shutting down")
+
+        if self.log_file and not self.log_file.closed:
+            self.log_file.write(f"# Session ended: {datetime.now().isoformat()}\n")
             self.log_file.close()
 
-    def __del__(self))
+    def __del__(self):
+        """Ensure cleanup on destruction"""
+        self.close()
 
 
 # Self-test function
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test PhiModulatorController"""
+    print("=" * 60)
+    print("PhiModulatorController Self-Test")
+    print("=" * 60)
+
+    try:
+        # Initialize controller
+        print("\n1. Initializing controller...")
+        controller = PhiModulatorController(sample_rate=48000, block_size=512)
+        print(f"   ✓ Controller initialized in mode: {controller.get_mode()}")
+
+        # Test internal oscillator (default)
+        print("\n2. Testing internal oscillator...")
+        for i in range(5):
+            state = controller.update()
+            print(f"   Update {i+1}: {state}")
+            time.sleep(0.05)
+        print("   ✓ Internal oscillator working")
+
+        # Switch to manual mode
+        print("\n3. Switching to manual mode...")
+        controller.set_mode("manual")
+        controller.set_manual_params(phase=np.pi/4, depth=0.8)
+        time.sleep(0.15)  # Wait for crossfade
+        state = controller.update()
+        print(f"   Manual state: {state}")
+        assert abs(state.depth - 0.8) < 0.1, f"Expected depth ≈0.8, got {state.depth}"
+        print("   ✓ Manual mode working")
+
+        # Switch to audio mode
+        print("\n4. Switching to audio mode...")
+        controller.set_mode("audio")
+        test_audio = np.random.randn(512).astype(np.float32) * 0.5
+        time.sleep(0.15)  # Wait for crossfade
+        for i in range(3):
+            state = controller.update(audio_block=test_audio)
+            print(f"   Audio update {i+1}: depth={state.depth:.3f}")
+        print("   ✓ Audio mode working")
+
+        # Test MIDI port detection
+        print("\n5. Testing MIDI detection...")
+        ports = controller.detect_midi_ports()
+        print(f"   Available MIDI ports: {ports if ports else 'None'}")
+        print("   ✓ MIDI detection OK (even if no ports)")
+
+        # Test metrics
+        print("\n6. Testing metrics...")
+        metrics = controller.get_metrics()
+        print(f"   Mode: {metrics['mode']}")
+        print(f"   Crossfade: {metrics['crossfade_progress']:.2f}")
+        print(f"   History size: {metrics['history_size']}")
+        print("   ✓ Metrics working")
+
+        # Test state export
+        print("\n7. Testing state export...")
+        state_dict = controller.get_state_dict()
+        print(f"   State dict keys: {list(state_dict.keys())}")
+        assert 'phase' in state_dict
+        assert 'depth' in state_dict
+        print("   ✓ State export working")
+
+        # Cleanup
+        print("\n8. Cleanup...")
+        controller.close()
+        print("   ✓ Cleanup complete")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_predictor.py b/server/phi_predictor.py
index f3ed305f3373922d882e687c102a690cdec18d51..b5eae4e64f24c7a8d08c21be78b2573884e885d9 100644
--- a/server/phi_predictor.py
+++ b/server/phi_predictor.py
@@ -1,340 +1,419 @@
 """
 PhiPredictor - Feature 012: Predictive Phi-Adaptation Engine
 
 Learns from historical sessions to predict optimal Phi settings.
 
 Features:
 - FR-004: Replay and fit session patterns
 - Pattern recognition from metric history
 - Predictive model for next Phi value
 - Auto-reset on divergence > 20%
 
 Requirements:
 - FR-004: Predictive engine MUST replay and fit session patterns
+- SC-003: Predictive accuracy >= 90% against previous sessions
+"""
 
 import time
 import threading
 from typing import Optional, List, Tuple, Dict
 from dataclasses import dataclass
 import numpy as np
 from scipy.interpolate import interp1d
 from scipy.signal import savgol_filter
 
 from .session_memory import SessionMemory, MetricSnapshot
 
 
 @dataclass
 class PredictionResult:
     """Result of a prediction"""
     predicted_phi: float          # Predicted Phi value
-    confidence)
+    confidence: float             # Confidence (0-1)
     prediction_time: float        # When prediction was made
-    actual_phi)
+    actual_phi: Optional[float]   # Actual Phi (for validation)
     error: Optional[float]        # Prediction error
 
 
 class PhiPredictor:
     """
     PhiPredictor - Learns patterns and predicts optimal Phi values
 
+    Handles:
+    - Pattern learning from session history (FR-004)
     - Predictive modeling based on metrics
-
+    - Accuracy tracking (SC-003)
     - Auto-reset on divergence
     """
 
     PHI_MIN = 0.618033988749895
     PHI_MAX = 1.618033988749895
 
-    def __init__(self, session_memory: SessionMemory) :
+    def __init__(self, session_memory: SessionMemory):
         """
         Initialize PhiPredictor
 
         Args:
             session_memory: SessionMemory instance for historical data
         """
         self.session_memory = session_memory
 
         # Learned model parameters
         self.ici_to_phi_model: Optional[interp1d] = None
         self.coherence_weight = 0.3
         self.criticality_weight = 0.2
 
         # Prediction tracking
-        self.predictions)
+        self.predictions: List[PredictionResult] = []
+        self.max_predictions = 1000
+        self.lock = threading.Lock()
 
         # Auto-reset threshold
         self.divergence_threshold = 0.20  # 20%
         self.accuracy_window = 50         # Last N predictions for accuracy
 
-    @lru_cache(maxsize=128)
-    def learn_from_session(self) :
+    def learn_from_session(self) -> bool:
+        """
+        Learn patterns from current session data (FR-004)
+
+        Returns:
             True if learning successful
         """
-        try)
+        try:
+            samples = self.session_memory.get_all_samples()
+
+            if len(samples) < 10:
+                return False
 
-            if len(samples) < 10)
+            # Extract metric arrays
+            icis = np.array([s.ici for s in samples])
             coherences = np.array([s.coherence for s in samples])
             criticalities = np.array([s.criticality for s in samples])
             phis = np.array([s.phi_value for s in samples])
 
             # Learn ICI → Phi mapping
             # Sort by ICI for interpolation
             sort_idx = np.argsort(icis)
             sorted_ici = icis[sort_idx]
             sorted_phi = phis[sort_idx]
 
             # Remove duplicates (needed for interpolation)
             unique_ici, unique_indices = np.unique(sorted_ici, return_index=True)
             unique_phi = sorted_phi[unique_indices]
 
-            if len(unique_ici) < 2,
+            if len(unique_ici) < 2:
+                return False
+
+            # Create interpolation model
+            self.ici_to_phi_model = interp1d(
+                unique_ici,
                 unique_phi,
                 kind='linear',
                 bounds_error=False,
                 fill_value=(unique_phi[0], unique_phi[-1])
+            )
 
             # Learn influence weights
             # Higher coherence → prefer higher Phi
             # Higher criticality → prefer lower Phi (more stable)
             phi_coherence_corr = np.corrcoef(phis, coherences)[0, 1]
             phi_criticality_corr = np.corrcoef(phis, criticalities)[0, 1]
 
             self.coherence_weight = max(0.0, min(0.5, phi_coherence_corr * 0.5))
             self.criticality_weight = max(0.0, min(0.5, -phi_criticality_corr * 0.5))
 
             return True
 
         except Exception as e:
-            logger.error("[PhiPredictor] Error learning from session, e)
+            print(f"[PhiPredictor] Error learning from session: {e}")
             return False
 
-    def predict_phi(self, ici, coherence, criticality) :
+    def predict_phi(self, ici: float, coherence: float, criticality: float) -> PredictionResult:
         """
         Predict optimal Phi value based on current metrics
 
         Args:
             ici: Current ICI value
             coherence: Current coherence
             criticality: Current criticality
 
         Returns:
             PredictionResult with prediction
         """
-        with self.lock)
+        with self.lock:
+            current_time = time.time()
 
             # Base prediction from ICI
             if self.ici_to_phi_model is not None:
-                try))
+                try:
+                    base_phi = float(self.ici_to_phi_model(ici))
                 except:
                     base_phi = 1.0  # Fallback to golden ratio
-            else, map deviations to Phi adjustments
+            else:
+                # No model yet - use simple heuristic
+                # ICI target is 0.5, map deviations to Phi adjustments
                 ici_error = ici - 0.5
                 base_phi = 1.0 - ici_error * 0.5  # Inverse relationship
 
             # Adjust based on coherence
             coherence_adjustment = (coherence - 0.5) * self.coherence_weight
 
             # Adjust based on criticality
             criticality_adjustment = -(criticality - 0.5) * self.criticality_weight
 
             # Combine
             predicted_phi = base_phi + coherence_adjustment + criticality_adjustment
 
             # Clamp to valid range
             predicted_phi = np.clip(predicted_phi, self.PHI_MIN, self.PHI_MAX)
 
             # Confidence based on model availability and metric quality
             if self.ici_to_phi_model is not None:
                 confidence = 0.9  # High confidence with learned model
             else:
                 confidence = 0.5  # Medium confidence with heuristic
 
             # Reduce confidence if metrics are extreme
             if ici < 0.2 or ici > 0.8:
                 confidence *= 0.7
-            if coherence < 0.2,
+            if coherence < 0.2:
+                confidence *= 0.8
+
+            result = PredictionResult(
+                predicted_phi=predicted_phi,
                 confidence=confidence,
                 prediction_time=current_time,
                 actual_phi=None,
                 error=None
+            )
 
             # Store prediction
             self.predictions.append(result)
-            if len(self.predictions) > self.max_predictions)
+            if len(self.predictions) > self.max_predictions:
+                self.predictions.pop(0)
 
             return result
 
-    def update_prediction_accuracy(self, prediction_idx: int, actual_phi: float) :
+    def update_prediction_accuracy(self, prediction_idx: int, actual_phi: float):
         """
         Update prediction with actual value for accuracy tracking
 
         Args:
             prediction_idx: Index of prediction to update
             actual_phi: Actual Phi value that was used
         """
-        with self.lock))
+        with self.lock:
+            if 0 <= prediction_idx < len(self.predictions):
+                pred = self.predictions[prediction_idx]
+                pred.actual_phi = actual_phi
+                pred.error = abs(pred.predicted_phi - actual_phi)
 
-    def get_recent_accuracy(self, window) :
+    def get_recent_accuracy(self, window: Optional[int] = None) -> float:
+        """
+        Calculate recent prediction accuracy (SC-003)
+
+        Args:
             window: Number of recent predictions to consider
 
-        Returns), where 1.0 is perfect
+        Returns:
+            Accuracy (0-1), where 1.0 is perfect
         """
         with self.lock:
             window = window or self.accuracy_window
 
             # Get recent predictions with actual values
-            recent = [p for p in self.predictions[-window) == 0) for p in recent if p.error is not None]
+            recent = [p for p in self.predictions[-window:] if p.actual_phi is not None]
 
-            if len(errors) == 0)
+            if len(recent) == 0:
+                return 0.0
+
+            # Calculate mean absolute percentage error
+            errors = [p.error / (p.actual_phi + 1e-9) for p in recent if p.error is not None]
+
+            if len(errors) == 0:
+                return 0.0
+
+            mape = np.mean(errors)
 
             # Convert to accuracy (1.0 - error)
             accuracy = max(0.0, 1.0 - mape)
 
             return accuracy
 
-    def check_divergence(self) :
+    def check_divergence(self) -> bool:
         """
         Check if predictions are diverging > 20% from actual
 
+        Returns:
+            True if divergence detected
+        """
+        accuracy = self.get_recent_accuracy(window=20)
+
         # Divergence if accuracy < 80% (error > 20%)
         return accuracy < 0.80
 
-    def reset_model(self) :
+    def reset_model(self):
+        """Reset predictive model (auto-reset on divergence)"""
+        with self.lock:
+            self.ici_to_phi_model = None
+            self.coherence_weight = 0.3
+            self.criticality_weight = 0.2
+            print("[PhiPredictor] Model reset due to divergence")
+
+    def get_prediction_stats(self) -> Dict:
         """
         Get prediction statistics
 
         Returns:
             Dictionary with prediction stats
         """
-        with self.lock)
+        with self.lock:
+            total_predictions = len(self.predictions)
             validated_predictions = len([p for p in self.predictions if p.actual_phi is not None])
             recent_accuracy = self.get_recent_accuracy()
 
-            if validated_predictions > 0)
+            if validated_predictions > 0:
+                avg_confidence = np.mean([p.confidence for p in self.predictions if p.actual_phi is not None])
                 avg_error = np.mean([p.error for p in self.predictions if p.error is not None])
             else:
                 avg_confidence = 0.0
                 avg_error = 0.0
 
             return {
-                "total_predictions",
-                "validated_predictions",
-                "recent_accuracy",
-                "avg_confidence",
-                "avg_error",
-                "has_learned_model", target_session) :
+                "total_predictions": total_predictions,
+                "validated_predictions": validated_predictions,
+                "recent_accuracy": recent_accuracy,
+                "avg_confidence": avg_confidence,
+                "avg_error": avg_error,
+                "has_learned_model": self.ici_to_phi_model is not None
+            }
+
+    def replay_session_pattern(self, target_session: SessionMemory) -> Tuple[float, List[float]]:
+        """
+        Replay session pattern and measure accuracy (FR-004, SC-003)
+
+        Args:
             target_session: Session to replay
 
-        Returns, predicted_phis) tuple
+        Returns:
+            (accuracy, predicted_phis) tuple
         """
         samples = target_session.get_all_samples()
 
-        if len(samples) < 10, []
+        if len(samples) < 10:
+            return 0.0, []
 
         predicted_phis = []
         actual_phis = []
 
-        for sample in samples,
+        for sample in samples:
+            # Predict Phi based on metrics
+            prediction = self.predict_phi(
+                ici=sample.ici,
                 coherence=sample.coherence,
                 criticality=sample.criticality
+            )
 
             predicted_phis.append(prediction.predicted_phi)
             actual_phis.append(sample.phi_value)
 
         # Calculate accuracy
         predicted = np.array(predicted_phis)
         actual = np.array(actual_phis)
 
         # Mean absolute percentage error
         mape = np.mean(np.abs((actual - predicted) / (actual + 1e-9)))
         accuracy = max(0.0, 1.0 - mape)
 
         return accuracy, predicted_phis
 
 
 # Self-test function
-@lru_cache(maxsize=128)
-def _self_test() -> None)
-    logger.info("PhiPredictor Self-Test")
-    logger.info("=" * 60)
-    logger.info(str())
+def _self_test():
+    """Run basic self-test of PhiPredictor"""
+    print("=" * 60)
+    print("PhiPredictor Self-Test")
+    print("=" * 60)
+    print()
 
     from session_memory import SessionMemory, MetricSnapshot
 
     # Create session with synthetic data
-    logger.info("1. Creating synthetic session...")
+    print("1. Creating synthetic session...")
     memory = SessionMemory(max_samples=200)
     memory.start_session("test_session")
 
     base_time = time.time()
-    for i in range(200))
+    for i in range(200):
+        # Simulate ICI oscillation
+        ici = 0.5 + 0.15 * np.sin(i * 0.05)
 
         # Phi should adapt inversely to ICI
         phi = 1.0 - (ici - 0.5) * 0.3
 
         snapshot = MetricSnapshot(
             timestamp=base_time + i * 0.1,
             ici=ici,
             coherence=0.6 + 0.1 * np.sin(i * 0.03),
             criticality=0.4 + 0.05 * np.cos(i * 0.04),
             phi_value=phi,
             phi_phase=i * 0.1,
             phi_depth=0.5,
             active_source="test"
-
+        )
         memory.record_snapshot(snapshot)
 
-    logger.info("   [OK] Created session with %s samples", memory.get_sample_count())
-    logger.info(str())
+    print(f"   [OK] Created session with {memory.get_sample_count()} samples")
+    print()
 
     # Create predictor
-    logger.info("2. Creating PhiPredictor...")
+    print("2. Creating PhiPredictor...")
     predictor = PhiPredictor(memory)
-    logger.info("   [OK] PhiPredictor created")
-    logger.info(str())
+    print("   [OK] PhiPredictor created")
+    print()
 
     # Learn from session
-    logger.info("3. Learning from session...")
+    print("3. Learning from session...")
     success = predictor.learn_from_session()
-    logger.error("   [%s] Learning %s", 'OK' if success else 'FAIL', 'successful' if success else 'failed')
-    logger.info(str())
+    print(f"   [{'OK' if success else 'FAIL'}] Learning {'successful' if success else 'failed'}")
+    print()
 
     # Test predictions
-    logger.info("4. Testing predictions...")
+    print("4. Testing predictions...")
     test_cases = [
         (0.4, 0.6, 0.4),  # Low ICI
         (0.5, 0.6, 0.4),  # Target ICI
         (0.6, 0.6, 0.4),  # High ICI
     ]
 
-    for ici, coherence, criticality in test_cases, coherence, criticality)
-        logger.info("   ICI=%s → Phi=%s (confidence=%s)", ici, result.predicted_phi, result.confidence)
+    for ici, coherence, criticality in test_cases:
+        result = predictor.predict_phi(ici, coherence, criticality)
+        print(f"   ICI={ici:.2f} → Phi={result.predicted_phi:.3f} (confidence={result.confidence:.2f})")
 
-    logger.info("   [OK] Predictions generated")
-    logger.info(str())
+    print("   [OK] Predictions generated")
+    print()
 
     # Test replay
-    logger.info("5. Testing session replay...")
+    print("5. Testing session replay...")
     accuracy, predicted_phis = predictor.replay_session_pattern(memory)
-    logger.info("   Replay accuracy, accuracy)
-    logger.warning("   [%s] Accuracy %s SC-003 (>= 90%)", 'OK' if accuracy >= 0.9 else 'WARN', 'meets' if accuracy >= 0.9 else 'below')
-    logger.info(str())
+    print(f"   Replay accuracy: {accuracy:.1%}")
+    print(f"   [{'OK' if accuracy >= 0.9 else 'WARN'}] Accuracy {'meets' if accuracy >= 0.9 else 'below'} SC-003 (>= 90%)")
+    print()
 
     # Test stats
-    logger.info("6. Testing prediction statistics...")
+    print("6. Testing prediction statistics...")
     stats = predictor.get_prediction_stats()
-    logger.info("   Total predictions, stats['total_predictions'])
-    logger.info("   Has learned model, stats['has_learned_model'])
-    logger.info("   [OK] Statistics computed")
-    logger.info(str())
+    print(f"   Total predictions: {stats['total_predictions']}")
+    print(f"   Has learned model: {stats['has_learned_model']}")
+    print("   [OK] Statistics computed")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_router.py b/server/phi_router.py
index 25458bdf82513390643a707ee8f11fb9e902d545..eba3925dee644b5a1e0d1749750b102912c09d3e 100644
--- a/server/phi_router.py
+++ b/server/phi_router.py
@@ -1,347 +1,462 @@
 """
-PhiRouter - Feature 011, handles selection, normalization, and fallback.
-
-
+PhiRouter - Feature 011: Real-time Phi Sensor Binding
 
+Manages multiple Φ sources, handles selection, normalization, and fallback.
 
+Features:
+- Multi-source routing with priority (FR-001)
+- Automatic source switching (SC-002)
+- Input normalization to [0.618–1.618] (FR-002)
+- Fallback mode if input stops > 2s (FR-005)
+- Real-time telemetry (FR-004)
 
 Requirements:
 - FR-001: Accept Φ input from multiple sources
 - FR-002: Normalize input to [0.618–1.618]
 - FR-005: Fallback mode if input stops for > 2s
 
 Success Criteria:
 - SC-002: Automatic source switching without restart
+- SC-004: Fallback mode prevents audio instability
+"""
 
 import time
 import threading
 from typing import Optional, Callable, Dict, List
 from dataclasses import dataclass, asdict
 from enum import Enum
 import numpy as np
 
 from .phi_sensor_bridge import SensorData, SensorType
 
 
-class PhiSourcePriority(Enum))
+class PhiSourcePriority(Enum):
+    """Priority levels for Φ sources"""
+    MANUAL = 0          # Lowest priority (user override)
     INTERNAL = 1        # Internal auto-modulation
     MICROPHONE = 2      # Microphone envelope
     AUDIO_BEAT = 3      # Audio beat detection
     MIDI = 4            # MIDI controller
     SERIAL = 5          # Serial sensor
     BIOMETRIC = 6       # Biometric sensor
     WEBSOCKET = 7       # WebSocket stream (highest priority)
 
 
 @dataclass
 class PhiRouterConfig:
     """Configuration for PhiRouter"""
-    fallback_timeout_s)
+    fallback_timeout_s: float = 2.0      # Fallback if no input for this long (FR-005)
     fallback_phi: float = 1.0            # Φ value to use in fallback mode
-    enable_auto_switch)
+    enable_auto_switch: bool = True      # Enable automatic source switching (SC-002)
     enable_logging: bool = True
 
 
 @dataclass
 class PhiRouterStatus:
     """Current status of PhiRouter"""
     timestamp: float
     active_source: str                   # Currently active source
     phi_value: float                     # Current Φ value
     phi_phase: float                     # Current Φ phase
-    is_fallback_mode)
+    is_fallback_mode: bool               # In fallback mode (FR-005)
     last_update_time: float              # Time of last update
     source_count: int                    # Number of active sources
     update_rate_hz: float                # Current update rate
 
 
 class PhiRouter:
     """
     PhiRouter - Manages multiple Φ sources with automatic selection
 
-
-
+    Handles:
+    - Multiple simultaneous Φ sources
+    - Priority-based source selection
+    - Automatic switching (SC-002)
+    - Fallback mode (FR-005, SC-004)
+    - Input normalization (FR-002)
     """
 
     PHI_MIN = 0.618033988749895  # 1/Φ
     PHI_MAX = 1.618033988749895  # Φ
     PHI = 1.618033988749895      # Golden ratio
 
-    def __init__(self, config: Optional[PhiRouterConfig]) :
+    def __init__(self, config: Optional[PhiRouterConfig] = None):
         """
         Initialize PhiRouter
 
         Args:
-            config)
+            config: Router configuration
+        """
+        self.config = config or PhiRouterConfig()
+
+        # Active sources (source_id -> (priority, last_data, last_update_time))
+        self.sources: Dict[str, tuple[PhiSourcePriority, SensorData, float]] = {}
+        self.lock = threading.Lock()
+
+        # Current state
+        self.current_phi = 1.0  # Start at golden ratio
+        self.current_phase = 0.0
+        self.active_source_id = "internal"
+        self.is_fallback_mode = False
+
+        # Telemetry
+        self.update_count = 0
+        self.last_telemetry_time = time.time()
+        self.update_rate_hz = 0.0
+
+        # Callbacks for updates
+        self.phi_update_callbacks: List[Callable[[float, float], None]] = []
+        self.status_update_callbacks: List[Callable[[PhiRouterStatus], None]] = []
+
+        # Watchdog thread for fallback
+        self.watchdog_thread = None
+        self.is_running = False
+
+    def start(self):
+        """Start router watchdog for fallback detection"""
+        self.is_running = True
+        self.watchdog_thread = threading.Thread(target=self._watchdog_loop, daemon=True)
+        self.watchdog_thread.start()
+
+        if self.config.enable_logging:
+            print("[PhiRouter] Started")
 
-        # Active sources (source_id :
+    def stop(self):
         """Stop router"""
         self.is_running = False
 
-        if self.watchdog_thread)
+        if self.watchdog_thread:
+            self.watchdog_thread.join(timeout=1.0)
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[PhiRouter] Stopped")
 
-    def register_source(self, source_id: str, priority: PhiSourcePriority) :
+    def register_source(self, source_id: str, priority: PhiSourcePriority):
         """
         Register a Φ source
 
         Args:
             source_id: Unique identifier for source
             priority: Source priority level
         """
         with self.lock:
-            if source_id not in self.sources,
+            if source_id not in self.sources:
+                # Initialize with placeholder data
+                placeholder = SensorData(
+                    sensor_type=SensorType.MIDI_CC,
                     timestamp=time.time(),
                     raw_value=0.0,
                     normalized_value=1.0,
                     source_id=source_id
-
+                )
                 self.sources[source_id] = (priority, placeholder, 0.0)
 
                 if self.config.enable_logging:
-                    logger.info("[PhiRouter] Registered source: %s (priority)", source_id, priority.value)
+                    print(f"[PhiRouter] Registered source: {source_id} (priority: {priority.value})")
 
-    def unregister_source(self, source_id: str) :
+    def unregister_source(self, source_id: str):
         """
         Unregister a Φ source
 
         Args:
             source_id: Source identifier to remove
         """
         with self.lock:
             if source_id in self.sources:
                 del self.sources[source_id]
 
                 if self.config.enable_logging:
-                    logger.info("[PhiRouter] Unregistered source, source_id)
+                    print(f"[PhiRouter] Unregistered source: {source_id}")
 
-    def update_source(self, source_id: str, data: SensorData) :
+    def update_source(self, source_id: str, data: SensorData):
+        """
+        Update Φ value from a source (FR-001, FR-003)
+
+        Args:
             source_id: Source identifier
-            data)
+            data: New sensor data
+        """
+        current_time = time.time()
 
         with self.lock:
             if source_id not in self.sources:
                 if self.config.enable_logging:
-                    logger.warning("[PhiRouter] Warning, source_id)
+                    print(f"[PhiRouter] Warning: Unknown source '{source_id}'")
                 return
 
             # Update source data
             priority, _, _ = self.sources[source_id]
             self.sources[source_id] = (priority, data, current_time)
 
             # Update telemetry
             self.update_count += 1
 
-            # Select active source (SC-002)
-            if self.config.enable_auto_switch)
+            # Select active source (SC-002: automatic switching)
+            if self.config.enable_auto_switch:
+                self._select_active_source()
 
-            # If this is the active source, update Φ (FR-003)
-            if source_id == self.active_source_id, 1.618] (FR-002)
+            # If this is the active source, update Φ (FR-003: < 100 ms)
+            if source_id == self.active_source_id:
+                # Normalize value to [0.618, 1.618] (FR-002)
                 normalized_phi = np.clip(data.normalized_value, self.PHI_MIN, self.PHI_MAX)
 
                 self.current_phi = normalized_phi
 
                 # Update phase based on value change
                 # Phase advances proportionally to Φ value
                 self.current_phase += (normalized_phi - 1.0) * 0.1
                 self.current_phase = self.current_phase % (2 * np.pi)
 
                 # Clear fallback mode
                 self.is_fallback_mode = False
 
                 # Call callbacks
                 self._notify_phi_update()
 
-    @lru_cache(maxsize=128)
-    def set_manual_phi(self, phi: float, phase: float) :
-            phi, 1.618])
+    def set_manual_phi(self, phi: float, phase: float):
+        """
+        Manually set Φ value (overrides active source temporarily)
+
+        Args:
+            phi: Φ value (will be clipped to [0.618, 1.618])
             phase: Φ phase in radians
         """
-        with self.lock)
+        with self.lock:
+            # Normalize (FR-002)
             self.current_phi = np.clip(phi, self.PHI_MIN, self.PHI_MAX)
             self.current_phase = phase % (2 * np.pi)
 
             # Set active source to manual
             self.active_source_id = "manual"
             self.is_fallback_mode = False
 
             # Call callbacks
             self._notify_phi_update()
 
-    def get_current_phi(self) :
+    def get_current_phi(self) -> tuple[float, float]:
         """
         Get current Φ value and phase
 
-        Returns, phi_phase) tuple
+        Returns:
+            (phi_value, phi_phase) tuple
+        """
+        with self.lock:
+            return (self.current_phi, self.current_phase)
+
+    def get_status(self) -> PhiRouterStatus:
         """
-        with self.lock, self.current_phase)
+        Get current router status (FR-004)
 
-    def get_status(self) :
+        Returns:
+            PhiRouterStatus with current state
+        """
+        current_time = time.time()
+
+        with self.lock:
             # Calculate update rate
             time_delta = current_time - self.last_telemetry_time
             if time_delta > 0:
                 self.update_rate_hz = self.update_count / time_delta
                 self.update_count = 0
                 self.last_telemetry_time = current_time
 
             # Get last update time for active source
             last_update_time = 0.0
-            if self.active_source_id in self.sources, _, last_update_time = self.sources[self.active_source_id]
+            if self.active_source_id in self.sources:
+                _, _, last_update_time = self.sources[self.active_source_id]
 
             return PhiRouterStatus(
                 timestamp=current_time,
                 active_source=self.active_source_id,
                 phi_value=self.current_phi,
                 phi_phase=self.current_phase,
                 is_fallback_mode=self.is_fallback_mode,
                 last_update_time=last_update_time,
                 source_count=len(self.sources),
                 update_rate_hz=self.update_rate_hz
+            )
 
-    def register_phi_callback(self, callback: Callable[[float, float], None]) :
+    def register_phi_callback(self, callback: Callable[[float, float], None]):
         """
         Register callback for Φ updates
 
         Args:
-            callback, phase) to call on updates
+            callback: Function(phi, phase) to call on updates
         """
         self.phi_update_callbacks.append(callback)
 
-    def register_status_callback(self, callback: Callable[[PhiRouterStatus], None]) :
+    def register_status_callback(self, callback: Callable[[PhiRouterStatus], None]):
         """
         Register callback for status updates
 
         Args:
-            callback) to call on status changes
+            callback: Function(status) to call on status changes
         """
         self.status_update_callbacks.append(callback)
 
-    def _select_active_source(self) :
+    def _select_active_source(self):
+        """
+        Select active source based on priority (SC-002)
+
+        Chooses highest priority source with recent data
+        """
+        current_time = time.time()
+
+        # Find highest priority source with recent data
+        best_source = None
+        best_priority = -1
+
+        for source_id, (priority, data, last_update) in self.sources.items():
+            # Check if source is recent (within timeout)
+            if (current_time - last_update) < self.config.fallback_timeout_s:
                 if priority.value > best_priority:
                     best_priority = priority.value
                     best_source = source_id
 
         # Switch source if needed
         if best_source and best_source != self.active_source_id:
             if self.config.enable_logging:
-                logger.info("[PhiRouter] Switching source, self.active_source_id, best_source)
+                print(f"[PhiRouter] Switching source: {self.active_source_id} → {best_source}")
 
             self.active_source_id = best_source
 
             # Notify status callbacks
             self._notify_status_update()
 
-    def _watchdog_loop(self) :
+    def _watchdog_loop(self):
+        """
+        Watchdog thread for fallback detection (FR-005, SC-004)
+
+        Monitors for source timeouts and engages fallback mode
+        """
+        while self.is_running:
+            time.sleep(0.5)  # Check every 500ms
+
+            current_time = time.time()
+
+            with self.lock:
                 # Check if active source has timed out
-                if self.active_source_id in self.sources, _, last_update = self.sources[self.active_source_id]
+                if self.active_source_id in self.sources:
+                    _, _, last_update = self.sources[self.active_source_id]
 
                     if (current_time - last_update) > self.config.fallback_timeout_s:
                         # Source timeout - enter fallback mode
                         if not self.is_fallback_mode:
-                            if self.config.enable_logging, self.active_source_id)
+                            if self.config.enable_logging:
+                                print(f"[PhiRouter] Source '{self.active_source_id}' timeout - entering fallback mode")
 
                             self.is_fallback_mode = True
                             self.current_phi = self.config.fallback_phi
 
                             # Notify callbacks
                             self._notify_phi_update()
                             self._notify_status_update()
 
-    def _notify_phi_update(self) :
+    def _notify_phi_update(self):
         """Notify callbacks of Φ update"""
         for callback in self.phi_update_callbacks:
-            try, self.current_phase)
+            try:
+                callback(self.current_phi, self.current_phase)
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[PhiRouter] Callback error, e)
+                    print(f"[PhiRouter] Callback error: {e}")
+
+    def _notify_status_update(self):
+        """Notify callbacks of status update"""
+        status = self.get_status()
 
-    def _notify_status_update(self) :
-            try)
+        for callback in self.status_update_callbacks:
+            try:
+                callback(status)
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[PhiRouter] Status callback error, e)
+                    print(f"[PhiRouter] Status callback error: {e}")
 
 
 # Self-test function
-def _self_test() -> None)
-    logger.info("PhiRouter Self-Test")
-    logger.info("=" * 60)
+def _self_test():
+    """Run basic self-test of PhiRouter"""
+    print("=" * 60)
+    print("PhiRouter Self-Test")
+    print("=" * 60)
 
     # Create router
-    logger.info("\n1. Creating PhiRouter...")
+    print("\n1. Creating PhiRouter...")
     config = PhiRouterConfig(
         fallback_timeout_s=1.0,
         enable_logging=True
-
+    )
     router = PhiRouter(config)
     router.start()
-    logger.info("   ✓ Router started")
+    print("   ✓ Router started")
 
     # Register sources
-    logger.info("\n2. Registering sources...")
+    print("\n2. Registering sources...")
     router.register_source("manual", PhiSourcePriority.MANUAL)
     router.register_source("midi", PhiSourcePriority.MIDI)
     router.register_source("sensor", PhiSourcePriority.SERIAL)
-    logger.info("   ✓ 3 sources registered")
+    print("   ✓ 3 sources registered")
 
     # Track updates
     phi_updates = []
 
-    def phi_callback(phi, phase) -> None, phase))
-        logger.info("   Φ update, phase, phi, phase)
+    def phi_callback(phi, phase):
+        phi_updates.append((phi, phase))
+        print(f"   Φ update: {phi:.3f}, phase: {phase:.2f}")
 
     router.register_phi_callback(phi_callback)
 
     # Test manual update
-    logger.info("\n3. Testing manual Φ update...")
+    print("\n3. Testing manual Φ update...")
     router.set_manual_phi(1.2, 1.57)
     time.sleep(0.1)
     assert len(phi_updates) > 0, "No Φ updates received"
-    logger.info("   ✓ Manual update working")
+    print("   ✓ Manual update working")
 
     # Test source update (simulated MIDI)
-    logger.info("\n4. Testing source update (simulated MIDI)...")
+    print("\n4. Testing source update (simulated MIDI)...")
     from phi_sensor_bridge import SensorData, SensorType
 
     midi_data = SensorData(
         sensor_type=SensorType.MIDI_CC,
         timestamp=time.time(),
         raw_value=64,
         normalized_value=1.0,
         source_id="midi"
+    )
 
     router.update_source("midi", midi_data)
     time.sleep(0.1)
 
     # Check automatic source switching
     status = router.get_status()
-    logger.info("   Active source, status.active_source)
-    logger.info("   Φ value, status.phi_value)
-    logger.info("   ✓ Source update working")
+    print(f"   Active source: {status.active_source}")
+    print(f"   Φ value: {status.phi_value:.3f}")
+    print("   ✓ Source update working")
 
     # Test fallback mode
-    logger.info("\n5. Testing fallback mode (wait 1.5s for timeout)...")
+    print("\n5. Testing fallback mode (wait 1.5s for timeout)...")
     initial_fallback = status.is_fallback_mode
     time.sleep(1.5)
 
     status = router.get_status()
-    logger.info("   Fallback mode, status.is_fallback_mode)
-    logger.info("   Φ value, status.phi_value)
+    print(f"   Fallback mode: {status.is_fallback_mode}")
+    print(f"   Φ value: {status.phi_value:.3f}")
 
-    if status.is_fallback_mode)
-    else)
+    if status.is_fallback_mode:
+        print("   ✓ Fallback mode engaged")
+    else:
+        print("   ⚠ Fallback mode not engaged")
 
     # Stop router
     router.stop()
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED ✓")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED ✓")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_sensor_bridge.py b/server/phi_sensor_bridge.py
index 1882737a8b31c9c458e64e1c130fdbc581515465..79b2600a77b24094a714cb54ddd5dd1ff726bd6c 100644
--- a/server/phi_sensor_bridge.py
+++ b/server/phi_sensor_bridge.py
@@ -1,414 +1,534 @@
 """
-PhiSensorBridge - Feature 011, sensors, audio beat, biometrics) as Φ-modulation inputs.
-
-
-
+PhiSensorBridge - Feature 011: Real-time Phi Sensor Binding
 
+Handles external data sources (MIDI, sensors, audio beat, biometrics) as Φ-modulation inputs.
 
+Features:
+- MIDI CC input support (FR-001)
+- Generic sensor input via serial/websocket (FR-001)
+- Audio beat detection (FR-001)
+- Input normalization to [0.618–1.618] range (FR-002)
+- Real-time parameter updates < 100 ms (FR-003)
+- Fallback mode if input stops > 2s (FR-005)
 
 Requirements:
 - FR-001: Accept Φ input from external sensor/MIDI streams
 - FR-002: Normalize input to [0.618–1.618]
-
+- FR-003: Update engine parameters in real time (< 100 ms)
 - FR-005: Fallback mode if input stops for > 2s
 
 Success Criteria:
 - SC-001: Live Φ updates visible/audible < 100 ms
+- SC-005: CPU overhead ≤ 5% from sensor loop
+"""
 
 import time
 import threading
 import queue
 from typing import Optional, Callable, Dict, List, Tuple
 from dataclasses import dataclass, asdict
 from enum import Enum
 import numpy as np
 
 # Optional MIDI support
 try:
     import mido
     MIDI_AVAILABLE = True
 except ImportError:
     MIDI_AVAILABLE = False
-    logger.warning("[PhiSensorBridge] Warning)
+    print("[PhiSensorBridge] Warning: mido not available. MIDI support disabled.")
 
 # Optional serial support for sensors
 try:
     import serial
     import serial.tools.list_ports
     SERIAL_AVAILABLE = True
 except ImportError:
     SERIAL_AVAILABLE = False
-    logger.warning("[PhiSensorBridge] Warning)
+    print("[PhiSensorBridge] Warning: pyserial not available. Serial sensor support disabled.")
 
 
-class SensorType(Enum), etc.)
+class SensorType(Enum):
+    """Sensor input types"""
+    MIDI_CC = "midi_cc"              # MIDI Control Change
+    SERIAL_ANALOG = "serial_analog"  # Serial analog sensor (Arduino, etc.)
     WEBSOCKET = "websocket"          # WebSocket data stream
     AUDIO_BEAT = "audio_beat"        # Audio beat detection
     BIOMETRIC = "biometric"          # Biometric sensor (heart rate, etc.)
     OSC = "osc"                      # OSC (Open Sound Control)
 
 
 @dataclass
 class SensorData:
     """Raw sensor data packet"""
     sensor_type: SensorType
     timestamp: float
     raw_value: float          # Raw value from sensor
-    normalized_value, 1.618]
+    normalized_value: float   # Normalized to [0.618, 1.618]
     source_id: str           # Device identifier
 
 
 @dataclass
 class SensorConfig:
     """Configuration for sensor input"""
     sensor_type: SensorType
     device_id: Optional[str] = None      # Device ID/port
-    midi_channel)
-    midi_cc_number)
+    midi_channel: int = 0                 # MIDI channel (0-15)
+    midi_cc_number: int = 1               # MIDI CC number (0-127)
     serial_baudrate: int = 9600           # Serial baudrate
     websocket_url: Optional[str] = None   # WebSocket URL
-    input_range, float] = (0.0, 1.0)  # Expected input range
+    input_range: Tuple[float, float] = (0.0, 1.0)  # Expected input range
     smoothing_alpha: float = 0.1          # Smoothing factor
     enable_logging: bool = False
 
 
 class MIDIInput:
+    """
+    MIDI input handler
+
+    Listens to MIDI Control Change messages and extracts Φ values
+    """
+
+    def __init__(self, config: SensorConfig, callback: Callable[[SensorData], None]):
         """
         Initialize MIDI input
 
         Args:
             config: Sensor configuration
             callback: Function to call when data received
         """
         if not MIDI_AVAILABLE:
-            raise RuntimeError("MIDI support not available. Install mido)
+            raise RuntimeError("MIDI support not available. Install mido: pip install mido python-rtmidi")
 
         self.config = config
         self.callback = callback
         self.port = None
         self.is_running = False
         self.thread = None
 
         # Smoothing
         self.smoothed_value = 0.5
 
-    def connect(self) :
+    def connect(self) -> bool:
         """
         Connect to MIDI device
 
         Returns:
             True if connected successfully
         """
-        try)
+        try:
+            # List available MIDI ports
+            available_ports = mido.get_input_names()
 
-            if not available_ports)
+            if not available_ports:
+                print("[MIDIInput] No MIDI input devices found")
                 return False
 
             # Use specified device or first available
             if self.config.device_id:
                 if self.config.device_id in available_ports:
                     port_name = self.config.device_id
-                else, self.config.device_id)
+                else:
+                    print(f"[MIDIInput] Device '{self.config.device_id}' not found")
                     return False
-            else)
+            else:
+                port_name = available_ports[0]
+
+            self.port = mido.open_input(port_name)
 
             if self.config.enable_logging:
-                logger.info("[MIDIInput] Connected to, port_name)
-                logger.info("[MIDIInput] Listening for CC%s on channel %s", self.config.midi_cc_number, self.config.midi_channel)
+                print(f"[MIDIInput] Connected to: {port_name}")
+                print(f"[MIDIInput] Listening for CC{self.config.midi_cc_number} on channel {self.config.midi_channel}")
 
             return True
 
         except Exception as e:
-            logger.error("[MIDIInput] Failed to connect, e)
+            print(f"[MIDIInput] Failed to connect: {e}")
             return False
 
-    def start(self) :
+    def start(self) -> bool:
         """
         Start MIDI input thread
 
         Returns:
             True if started successfully
         """
-        if not self.port, daemon=True)
+        if not self.port:
+            return False
+
+        self.is_running = True
+        self.thread = threading.Thread(target=self._midi_loop, daemon=True)
         self.thread.start()
 
         return True
 
-    @lru_cache(maxsize=128)
-    def stop(self) :
+    def stop(self):
         """Stop MIDI input thread"""
         self.is_running = False
 
-        if self.thread)
+        if self.thread:
+            self.thread.join(timeout=1.0)
 
-        if self.port)
+        if self.port:
+            self.port.close()
             self.port = None
 
-    @lru_cache(maxsize=128)
-    def _midi_loop(self) :
+    def _midi_loop(self):
         """MIDI message processing loop"""
         while self.is_running:
-            try, timeout=0.1)
+            try:
+                # Non-blocking receive with timeout
+                msg = self.port.receive(block=True, timeout=0.1)
 
                 if msg is None:
                     continue
 
                 # Filter for Control Change messages
-                if msg.type == 'control_change'), 1]
+                if msg.type == 'control_change':
+                    # Check channel and CC number
+                    if (msg.channel == self.config.midi_channel and
+                        msg.control == self.config.midi_cc_number):
+
+                        # MIDI CC values are 0-127
+                        raw_value = msg.value / 127.0  # Normalize to [0, 1]
 
                         # Apply smoothing
                         self.smoothed_value = (
                             self.config.smoothing_alpha * raw_value +
                             (1 - self.config.smoothing_alpha) * self.smoothed_value
+                        )
 
                         # Map to Φ range [0.618, 1.618]
                         PHI_MIN = 0.618033988749895
                         PHI_MAX = 1.618033988749895
                         normalized_value = PHI_MIN + self.smoothed_value * (PHI_MAX - PHI_MIN)
 
                         # Create sensor data packet
                         sensor_data = SensorData(
                             sensor_type=SensorType.MIDI_CC,
                             timestamp=time.time(),
                             raw_value=msg.value,
                             normalized_value=normalized_value,
                             source_id=f"MIDI_CC{self.config.midi_cc_number}"
+                        )
 
-                        # Call callback (SC-001)
+                        # Call callback (SC-001: < 100 ms latency)
                         self.callback(sensor_data)
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[MIDIInput] Error in MIDI loop, e)
+                    print(f"[MIDIInput] Error in MIDI loop: {e}")
 
     @staticmethod
-    def list_devices() :
+    def list_devices() -> List[str]:
         """
         List available MIDI input devices
 
         Returns:
             List of device names
         """
         if not MIDI_AVAILABLE:
             return []
 
-        try)
+        try:
+            return mido.get_input_names()
         except:
             return []
 
 
 class SerialSensorInput:
-    Expected format, one per line
     """
+    Serial sensor input handler
 
-    def __init__(self, config: SensorConfig, callback: Callable[[SensorData], None]) :
+    Reads analog sensor values from serial port (Arduino, etc.)
+    Expected format: ASCII decimal values, one per line
+    """
+
+    def __init__(self, config: SensorConfig, callback: Callable[[SensorData], None]):
         """
         Initialize serial sensor input
 
         Args:
             config: Sensor configuration
             callback: Function to call when data received
         """
         if not SERIAL_AVAILABLE:
-            raise RuntimeError("Serial support not available. Install pyserial)
+            raise RuntimeError("Serial support not available. Install pyserial: pip install pyserial")
 
         self.config = config
         self.callback = callback
         self.serial_port = None
         self.is_running = False
         self.thread = None
 
         # Smoothing
         self.smoothed_value = 0.5
 
-    def connect(self) :
+    def connect(self) -> bool:
         """
         Connect to serial device
 
         Returns:
             True if connected successfully
         """
         try:
             # Find device
             if self.config.device_id:
                 port = self.config.device_id
-            else))
-                if not ports)
+            else:
+                # Use first available serial port
+                ports = list(serial.tools.list_ports.comports())
+                if not ports:
+                    print("[SerialSensor] No serial ports found")
                     return False
                 port = ports[0].device
 
             # Open serial port
             self.serial_port = serial.Serial(
                 port=port,
                 baudrate=self.config.serial_baudrate,
                 timeout=0.1
+            )
 
             if self.config.enable_logging:
-                logger.info("[SerialSensor] Connected to, port, self.config.serial_baudrate)
+                print(f"[SerialSensor] Connected to: {port} @ {self.config.serial_baudrate} baud")
 
             return True
 
         except Exception as e:
-            logger.error("[SerialSensor] Failed to connect, e)
+            print(f"[SerialSensor] Failed to connect: {e}")
             return False
 
-    def start(self) :
+    def start(self) -> bool:
         """
         Start serial input thread
 
         Returns:
             True if started successfully
         """
-        if not self.serial_port, daemon=True)
+        if not self.serial_port:
+            return False
+
+        self.is_running = True
+        self.thread = threading.Thread(target=self._serial_loop, daemon=True)
         self.thread.start()
 
         return True
 
-    @lru_cache(maxsize=128)
-    def stop(self) :
+    def stop(self):
         """Stop serial input thread"""
         self.is_running = False
 
-        if self.thread)
+        if self.thread:
+            self.thread.join(timeout=1.0)
 
-        if self.serial_port)
+        if self.serial_port:
+            self.serial_port.close()
             self.serial_port = None
 
-    @lru_cache(maxsize=128)
-    def _serial_loop(self) :
+    def _serial_loop(self):
         """Serial data processing loop"""
         while self.is_running:
             try:
                 # Read line from serial
-                if self.serial_port.in_waiting > 0).decode('utf-8').strip()
+                if self.serial_port.in_waiting > 0:
+                    line = self.serial_port.readline().decode('utf-8').strip()
 
                     if line:
-                        try)
+                        try:
+                            # Parse value
+                            raw_value = float(line)
 
                             # Normalize from input range to [0, 1]
                             input_min, input_max = self.config.input_range
                             normalized_01 = (raw_value - input_min) / (input_max - input_min)
                             normalized_01 = np.clip(normalized_01, 0.0, 1.0)
 
                             # Apply smoothing
                             self.smoothed_value = (
                                 self.config.smoothing_alpha * normalized_01 +
                                 (1 - self.config.smoothing_alpha) * self.smoothed_value
+                            )
 
                             # Map to Φ range [0.618, 1.618] (FR-002)
                             PHI_MIN = 0.618033988749895
                             PHI_MAX = 1.618033988749895
                             normalized_phi = PHI_MIN + self.smoothed_value * (PHI_MAX - PHI_MIN)
 
                             # Create sensor data packet
                             sensor_data = SensorData(
                                 sensor_type=SensorType.SERIAL_ANALOG,
                                 timestamp=time.time(),
                                 raw_value=raw_value,
                                 normalized_value=normalized_phi,
                                 source_id=f"Serial_{self.config.device_id}"
+                            )
 
-                            # Call callback (SC-001)
+                            # Call callback (SC-001: < 100 ms latency)
                             self.callback(sensor_data)
 
                         except ValueError:
                             # Skip invalid values
                             pass
 
-                else)
+                else:
+                    # Brief sleep to avoid busy-waiting
+                    time.sleep(0.01)
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[SerialSensor] Error in serial loop, e)
+                    print(f"[SerialSensor] Error in serial loop: {e}")
 
     @staticmethod
-    def list_devices() :
+    def list_devices() -> List[str]:
         """
         List available serial ports
 
         Returns:
             List of port names
         """
         if not SERIAL_AVAILABLE:
             return []
 
-        try)
+        try:
+            ports = serial.tools.list_ports.comports()
             return [port.device for port in ports]
         except:
             return []
 
 
 class AudioBeatDetector:
+    """
+    Audio beat detection for Φ modulation
+
+    Analyzes audio input to detect beats and modulate Φ accordingly
+    """
+
+    def __init__(self, config: SensorConfig, callback: Callable[[SensorData], None]):
         """
         Initialize beat detector
 
         Args:
             config: Sensor configuration
-            callback)
-    def process_audio(self, audio_block: np.ndarray, sample_rate: int) :
+            callback: Function to call when beat detected
+        """
+        self.config = config
+        self.callback = callback
+
+        # Beat detection state
+        self.energy_history = []
+        self.last_beat_time = 0.0
+        self.beat_strength = 0.0
+
+        # Φ modulation
+        self.phi_value = 1.0  # Start at golden ratio
+
+    def process_audio(self, audio_block: np.ndarray, sample_rate: int = 48000):
         """
         Process audio block for beat detection
 
         Args:
-            audio_block, float32)
-            sample_rate)
+            audio_block: Audio samples (mono, float32)
+            sample_rate: Sample rate in Hz
+        """
+        # Calculate instantaneous energy
+        energy = np.sum(audio_block ** 2)
 
         # Track energy history
         self.energy_history.append(energy)
-        if len(self.energy_history) > 43)
+        if len(self.energy_history) > 43:  # ~1 second at 512 samples/block
+            self.energy_history.pop(0)
 
         # Detect beat (energy spike)
         if len(self.energy_history) >= 10:
-            avg_energy = np.mean(self.energy_history[-10)
+            avg_energy = np.mean(self.energy_history[-10:])
             threshold = avg_energy * 1.5
 
             current_time = time.time()
 
-            if energy > threshold and (current_time - self.last_beat_time) > 0.3), 2.0)
+            if energy > threshold and (current_time - self.last_beat_time) > 0.3:
+                # Beat detected!
+                self.last_beat_time = current_time
+                self.beat_strength = min(energy / (avg_energy + 1e-9), 2.0)
 
                 # Modulate Φ based on beat strength
                 # Stronger beats → higher Φ
                 PHI_MIN = 0.618033988749895
                 PHI_MAX = 1.618033988749895
                 self.phi_value = PHI_MIN + (self.beat_strength / 2.0) * (PHI_MAX - PHI_MIN)
 
                 # Create sensor data packet
                 sensor_data = SensorData(
                     sensor_type=SensorType.AUDIO_BEAT,
                     timestamp=current_time,
                     raw_value=self.beat_strength,
                     normalized_value=self.phi_value,
                     source_id="AudioBeat"
+                )
 
                 # Call callback
                 self.callback(sensor_data)
 
         # Decay Φ value towards center
         decay_rate = 0.95
         center = 1.0
         self.phi_value = self.phi_value * decay_rate + center * (1 - decay_rate)
 
 
 # Self-test function
-def _self_test() : SensorData) -> bool)
-        logger.info("   Beat detected! Strength, Φ, data.raw_value, data.normalized_value)
+def _self_test():
+    """Run basic self-test of PhiSensorBridge"""
+    print("=" * 60)
+    print("PhiSensorBridge Self-Test")
+    print("=" * 60)
+
+    # Test MIDI device listing
+    print("\n1. Testing MIDI device listing...")
+    if MIDI_AVAILABLE:
+        devices = MIDIInput.list_devices()
+        print(f"   Found {len(devices)} MIDI devices:")
+        for dev in devices:
+            print(f"   - {dev}")
+    else:
+        print("   MIDI not available (install mido)")
+
+    # Test Serial device listing
+    print("\n2. Testing Serial device listing...")
+    if SERIAL_AVAILABLE:
+        devices = SerialSensorInput.list_devices()
+        print(f"   Found {len(devices)} serial devices:")
+        for dev in devices:
+            print(f"   - {dev}")
+    else:
+        print("   Serial not available (install pyserial)")
+
+    # Test audio beat detector
+    print("\n3. Testing Audio Beat Detector...")
+    beat_detected = []
+
+    def beat_callback(data: SensorData):
+        beat_detected.append(data)
+        print(f"   Beat detected! Strength: {data.raw_value:.2f}, Φ: {data.normalized_value:.3f}")
 
     config = SensorConfig(sensor_type=SensorType.AUDIO_BEAT)
     detector = AudioBeatDetector(config, beat_callback)
 
     # Simulate beats
-    logger.info("   Simulating 3 beats...")
-    for i in range(3)).astype(np.float32) * (2.0 if i % 2 == 0 else 0.5)
+    print("   Simulating 3 beats...")
+    for i in range(3):
+        # Create beat-like signal
+        beat_signal = np.random.randn(512).astype(np.float32) * (2.0 if i % 2 == 0 else 0.5)
         detector.process_audio(beat_signal)
         time.sleep(0.5)
 
-    logger.info("   Detected %s beats", len(beat_detected))
+    print(f"   Detected {len(beat_detected)} beats")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED ✓")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED ✓")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/phi_sources.py b/server/phi_sources.py
index df4b6adb6000bcb2362ee57024161e1830a922df..dc091e60d156c9589a86c1ea72e4a312b946f9ef 100644
--- a/server/phi_sources.py
+++ b/server/phi_sources.py
@@ -1,277 +1,592 @@
 """
 Φ-Modulation Sources - Abstract base and concrete implementations
 
 Supports multiple Φ modulation sources:
 - Manual: Direct user control via sliders
 - Audio: Envelope follower from audio input
-
+- MIDI: MIDI CC control (typically CC1 - Mod Wheel)
 - Sensor: Biometric/accelerometer data
-
+- Internal: Breathing oscillator (~0.1 Hz)
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import numpy as np
 from abc import ABC, abstractmethod
 from typing import Optional, Dict
 import time
 
 
 class PhiState:
     """
     Data container for Φ modulation state
 
     Attributes:
-        phase, 2π]
-        depth, 1.618]
+        phase: Φ-phase in radians [0, 2π]
+        depth: Φ-depth [0, 1.618]
         source: Active source name
-        frequency)
-        timestamp,
-                 phase,
-                 depth,
-                 source,
-                 frequency))
-
-    def to_dict(self) :
+        frequency: Modulation frequency in Hz (if applicable)
+        timestamp: Unix timestamp of last update
+    """
+
+    def __init__(self,
+                 phase: float = 0.0,
+                 depth: float = 0.618,
+                 source: str = "internal",
+                 frequency: float = 0.1):
+        self.phase = phase
+        self.depth = depth
+        self.source = source
+        self.frequency = frequency
+        self.timestamp = time.time()
+
+    def to_dict(self) -> Dict:
         """Convert to dictionary for JSON serialization"""
         return {
-            'phase'),
-            'depth'),
-            'source',
-            'frequency'),
-            'timestamp') :
-        return f"PhiState(φ={self.phase, Φd={self.depth, source={self.source})"
+            'phase': float(self.phase),
+            'depth': float(self.depth),
+            'source': self.source,
+            'frequency': float(self.frequency),
+            'timestamp': self.timestamp
+        }
+
+    def __repr__(self) -> str:
+        return f"PhiState(φ={self.phase:.3f}, Φd={self.depth:.3f}, source={self.source})"
 
 
-class PhiSource(ABC)) and get_state() methods
+class PhiSource(ABC):
+    """
+    Abstract base class for Φ modulation sources
+
+    All sources must implement update() and get_state() methods
     """
 
     PHI = 1.618033988749895  # Golden ratio
     PHI_INV = 0.618033988749895  # 1/Φ
 
-    def __init__(self, sample_rate: int) :
+    def __init__(self, sample_rate: int = 48000):
+        self.sample_rate = sample_rate
+        self.active = False
+        self.last_state = PhiState()
+
+    @abstractmethod
+    def update(self, **kwargs) -> PhiState:
         """
         Update and return current Φ state
 
-        Returns) :
+        Returns:
+            PhiState object with current phase and depth
+        """
+        pass
+
+    @abstractmethod
+    def get_state(self) -> PhiState:
         """
         Get current Φ state without updating
 
-        Returns) : int) : float) : float) :
+        Returns:
+            Last known PhiState
+        """
+        pass
+
+    def activate(self):
+        """Activate this source"""
+        self.active = True
+
+    def deactivate(self):
+        """Deactivate this source"""
+        self.active = False
+
+    def reset(self):
+        """Reset source state"""
+        self.last_state = PhiState()
+
+
+class ManualSource(PhiSource):
+    """
+    Manual Φ control via direct parameter setting
+
+    User directly sets phase and depth via UI sliders
+    """
+
+    def __init__(self, sample_rate: int = 48000):
+        super().__init__(sample_rate)
+        self._phase = 0.0
+        self._depth = 0.618  # Default to Φ^-1
+        self.last_state.source = "manual"
+
+    def set_phase(self, phase: float):
+        """Set phase directly [0, 2π]"""
+        self._phase = phase % (2 * np.pi)
+        self.last_state.phase = self._phase
+        self.last_state.timestamp = time.time()
+
+    def set_depth(self, depth: float):
+        """Set depth directly [0, 1.618]"""
+        self._depth = np.clip(depth, 0.0, self.PHI)
+        self.last_state.depth = self._depth
+        self.last_state.timestamp = time.time()
+
+    def update(self, phase: Optional[float] = None, depth: Optional[float] = None) -> PhiState:
         """
         Update manual parameters
 
         Args:
-            phase)
-            depth)
+            phase: New phase value (optional)
+            depth: New depth value (optional)
 
         Returns:
             Updated PhiState
         """
-        if phase is not None)
-        if depth is not None)
+        if phase is not None:
+            self.set_phase(phase)
+        if depth is not None:
+            self.set_depth(depth)
+
+        return self.last_state
 
+    def get_state(self) -> PhiState:
         return self.last_state
 
-    def get_state(self) : float) : float) :
+
+class AudioEnvelopeSource(PhiSource):
+    """
+    Audio envelope follower for Φ modulation
+
+    Maps RMS amplitude to Φ-depth with configurable attack/release
+    """
+
+    def __init__(self,
+                 sample_rate: int = 48000,
+                 attack_ms: float = 10.0,
+                 release_ms: float = 100.0,
+                 baseline_depth: float = 0.618):
+        super().__init__(sample_rate)
+        self.last_state.source = "audio"
+
+        # Envelope follower parameters
+        self.attack_coef = self._ms_to_coef(attack_ms)
+        self.release_coef = self._ms_to_coef(release_ms)
+        self.baseline_depth = baseline_depth
+
+        # State tracking
+        self.envelope = 0.0
+        self.silence_duration = 0.0
+        self.silence_threshold = 2.0  # seconds
+        self.last_update_time = time.time()
+
+        # Phase accumulator (slowly rotates)
+        self.phase_accumulator = 0.0
+        self.phase_rotation_freq = 0.05  # Hz (slow rotation)
+
+    def _ms_to_coef(self, time_ms: float) -> float:
+        """Convert time constant in ms to exponential coefficient"""
+        # exp(-1 / (time_constant * sample_rate))
+        time_samples = (time_ms / 1000.0) * self.sample_rate
+        return np.exp(-1.0 / time_samples) if time_samples > 0 else 0.0
+
+    def set_attack(self, attack_ms: float):
+        """Set attack time in milliseconds [10-500]"""
+        attack_ms = np.clip(attack_ms, 10.0, 500.0)
+        self.attack_coef = self._ms_to_coef(attack_ms)
+
+    def set_release(self, release_ms: float):
+        """Set release time in milliseconds [10-500]"""
+        release_ms = np.clip(release_ms, 10.0, 500.0)
+        self.release_coef = self._ms_to_coef(release_ms)
+
+    def update(self, audio_block: np.ndarray, **kwargs) -> PhiState:
         """
         Update from audio input
 
         Args:
-            audio_block, will compute RMS)
+            audio_block: Audio samples (any shape, will compute RMS)
 
+        Returns:
+            PhiState with depth following envelope
+        """
+        current_time = time.time()
         dt = current_time - self.last_update_time
         self.last_update_time = current_time
 
         # Calculate RMS of input
         rms = np.sqrt(np.mean(audio_block ** 2))
 
         # Envelope follower with attack/release
-        if rms > self.envelope) * (1.0 - self.attack_coef)
-        else) * (1.0 - self.release_coef)
+        if rms > self.envelope:
+            # Attack
+            self.envelope += (rms - self.envelope) * (1.0 - self.attack_coef)
+        else:
+            # Release
+            self.envelope += (rms - self.envelope) * (1.0 - self.release_coef)
 
         # Track silence
         if rms < 0.001:  # Silence threshold
             self.silence_duration += dt
         else:
             self.silence_duration = 0.0
 
         # Decay to baseline after silence
-        if self.silence_duration > self.silence_threshold) * decay_rate * dt
-        else, 1.618]
+        if self.silence_duration > self.silence_threshold:
+            decay_rate = 0.1  # Smooth decay
+            target_depth = self.baseline_depth
+            self.last_state.depth += (target_depth - self.last_state.depth) * decay_rate * dt
+        else:
+            # Map envelope to depth [0, 1.618]
             # RMS of 0.5 (half-scale) maps to PHI
             self.last_state.depth = np.clip(self.envelope * 2.0 * self.PHI, 0.0, self.PHI)
 
         # Slowly rotate phase based on envelope energy
         phase_delta = 2 * np.pi * self.phase_rotation_freq * dt
         self.phase_accumulator += phase_delta * (1.0 + self.envelope)
         self.last_state.phase = self.phase_accumulator % (2 * np.pi)
 
         self.last_state.timestamp = current_time
         return self.last_state
 
-    def get_state(self) : float) :
+    def get_state(self) -> PhiState:
+        return self.last_state
+
+
+class InternalOscillatorSource(PhiSource):
+    """
+    Internal breathing oscillator (~0.1 Hz)
+
+    Provides baseline Φ modulation when no external source is active
+    Simulates natural breathing rhythm (6 cycles/minute)
+    """
+
+    DEFAULT_FREQUENCY = 0.1  # Hz (6 breaths per minute)
+
+    def __init__(self,
+                 sample_rate: int = 48000,
+                 frequency: float = DEFAULT_FREQUENCY,
+                 depth_range: tuple = (0.3, 0.9)):
+        super().__init__(sample_rate)
+        self.last_state.source = "internal"
+
+        self.frequency = np.clip(frequency, 0.01, 10.0)  # FR-008: limit range
+        self.depth_min, self.depth_max = depth_range
+
+        # Phase accumulator
+        self.phase_accumulator = 0.0
+        self.last_update_time = time.time()
+
+    def set_frequency(self, frequency: float):
+        """Set breathing frequency [0.01, 10 Hz]"""
+        self.frequency = np.clip(frequency, 0.01, 10.0)
+        self.last_state.frequency = self.frequency
+
+    def update(self, **kwargs) -> PhiState:
         """
         Update internal oscillator
 
+        Returns:
+            PhiState with sinusoidal breathing pattern
+        """
+        current_time = time.time()
         dt = current_time - self.last_update_time
         self.last_update_time = current_time
 
         # Advance phase
         phase_delta = 2 * np.pi * self.frequency * dt
         self.phase_accumulator += phase_delta
         self.phase_accumulator %= (2 * np.pi)
 
         # Sinusoidal depth modulation (breathing)
         # Oscillates between depth_min and depth_max
         depth_center = (self.depth_min + self.depth_max) / 2.0
         depth_amplitude = (self.depth_max - self.depth_min) / 2.0
         self.last_state.depth = depth_center + depth_amplitude * np.sin(self.phase_accumulator)
 
         # Phase follows same oscillator
         self.last_state.phase = self.phase_accumulator
         self.last_state.frequency = self.frequency
         self.last_state.timestamp = current_time
 
         return self.last_state
 
-    def get_state(self) : int) : int) :
-            value, 127]
+    def get_state(self) -> PhiState:
+        return self.last_state
+
+
+class MIDISource(PhiSource):
+    """
+    MIDI CC control for Φ modulation
+
+    Typically uses CC1 (Mod Wheel) for depth control
+    Can also use pitch bend for phase modulation
+    """
+
+    def __init__(self, sample_rate: int = 48000):
+        super().__init__(sample_rate)
+        self.last_state.source = "midi"
+
+        # MIDI state
+        self.cc1_value = 64  # 0-127
+        self.pitch_bend = 8192  # 0-16383, center = 8192
+
+        # MIDI input (will be initialized when device connected)
+        self.midi_input = None
+        self.available_ports = []
+
+    def set_cc1(self, value: int):
+        """
+        Set CC1 value (typically Mod Wheel)
+
+        Args:
+            value: MIDI CC value [0, 127]
         """
         self.cc1_value = np.clip(value, 0, 127)
 
         # Map CC1 to depth [0, 1.618]
         # CC1 = 64 → depth = 0.618 (Φ^-1)
         # CC1 = 127 → depth = 1.618 (Φ)
         self.last_state.depth = (self.cc1_value / 127.0) * self.PHI
         self.last_state.timestamp = time.time()
 
-    @lru_cache(maxsize=128)
-    def set_pitch_bend(self, value: int) :
+    def set_pitch_bend(self, value: int):
         """
         Set pitch bend value
 
         Args:
-            value, 16383], center = 8192
+            value: MIDI pitch bend [0, 16383], center = 8192
         """
         self.pitch_bend = np.clip(value, 0, 16383)
 
         # Map pitch bend to phase offset [-π, +π]
         normalized = (self.pitch_bend - 8192) / 8192.0  # [-1, +1]
         self.last_state.phase = normalized * np.pi
         self.last_state.timestamp = time.time()
 
-    def update(self, cc1, pitch_bend, **kwargs) :
+    def update(self, cc1: Optional[int] = None, pitch_bend: Optional[int] = None, **kwargs) -> PhiState:
         """
         Update from MIDI input
 
         Args:
-            cc1)
-            pitch_bend)
+            cc1: CC1 value (optional)
+            pitch_bend: Pitch bend value (optional)
 
         Returns:
             Updated PhiState
         """
-        if cc1 is not None)
-        if pitch_bend is not None)
+        if cc1 is not None:
+            self.set_cc1(cc1)
+        if pitch_bend is not None:
+            self.set_pitch_bend(pitch_bend)
 
         return self.last_state
 
-    def detect_ports(self) :
+    def detect_ports(self) -> list:
         """
         Detect available MIDI input ports
 
         Returns:
             List of available port names
         """
-        try)
+        try:
+            import mido
+            self.available_ports = mido.get_input_names()
             return self.available_ports
         except ImportError:
-            logger.warning("[MIDISource] Warning)
+            print("[MIDISource] Warning: mido not installed. MIDI support disabled.")
             return []
         except Exception as e:
-            logger.error("[MIDISource] Error detecting MIDI ports, e)
+            print(f"[MIDISource] Error detecting MIDI ports: {e}")
             return []
 
-    def connect(self, port_name: Optional[str]) :
+    def connect(self, port_name: Optional[str] = None):
         """
         Connect to MIDI input port
 
         Args:
-            port_name)
+            port_name: Port name (None = first available)
         """
         try:
             import mido
 
-            if port_name is None)
-                if not ports)
+            if port_name is None:
+                ports = self.detect_ports()
+                if not ports:
+                    raise RuntimeError("No MIDI ports available")
                 port_name = ports[0]
 
             self.midi_input = mido.open_input(port_name)
-            logger.info("[MIDISource] Connected to %s", port_name)
+            print(f"[MIDISource] Connected to {port_name}")
 
         except ImportError:
-            raise RuntimeError("mido library not installed. Install with)
+            raise RuntimeError("mido library not installed. Install with: pip install mido")
         except Exception as e:
-            raise RuntimeError(f"Failed to connect to MIDI port)
+            raise RuntimeError(f"Failed to connect to MIDI port: {e}")
+
+    def get_state(self) -> PhiState:
+        return self.last_state
+
 
-    def get_state(self) :
+class SensorSource(PhiSource):
     """
     Biometric/sensor input for Φ modulation
 
-    Supports) → depth modulation
+    Supports:
+    - Heart rate → phase frequency modulation
+    - GSR (galvanic skin response) → depth modulation
     - Accelerometer → phase offset
     """
 
-    def __init__(self, sample_rate: int) : float) :
-            bpm, 200]
+    def __init__(self, sample_rate: int = 48000):
+        super().__init__(sample_rate)
+        self.last_state.source = "sensor"
+
+        # Sensor values
+        self.heart_rate = 60.0  # BPM
+        self.gsr = 0.5  # Normalized [0, 1]
+        self.accel_x = 0.0  # Normalized [-1, 1]
+
+        # Thresholds
+        self.hr_threshold = 90.0  # BPM
+
+    def set_heart_rate(self, bpm: float):
+        """
+        Set heart rate in beats per minute
+
+        High heart rate (>90 BPM) increases phase rotation frequency
+
+        Args:
+            bpm: Heart rate in BPM [40, 200]
         """
         self.heart_rate = np.clip(bpm, 40.0, 200.0)
 
         # Map HR to phase frequency acceleration
-        if self.heart_rate > self.hr_threshold) / 10.0)
+        if self.heart_rate > self.hr_threshold:
+            # +10% frequency per 10 BPM above threshold
+            accel_factor = 1.0 + 0.1 * ((self.heart_rate - self.hr_threshold) / 10.0)
             self.last_state.frequency = 0.1 * accel_factor
-        else)
+        else:
+            self.last_state.frequency = 0.1
+
+        self.last_state.timestamp = time.time()
+
+    def set_gsr(self, gsr: float):
+        """
+        Set galvanic skin response (normalized)
 
-    @lru_cache(maxsize=128)
-    def set_gsr(self, gsr: float) :
-            gsr, 1]
+        Args:
+            gsr: GSR value [0, 1]
         """
         self.gsr = np.clip(gsr, 0.0, 1.0)
 
         # Map GSR to depth
         self.last_state.depth = self.gsr * self.PHI
         self.last_state.timestamp = time.time()
 
-    @lru_cache(maxsize=128)
-    def set_accelerometer(self, x: float, y: float, z: float) :
+    def set_accelerometer(self, x: float, y: float = 0.0, z: float = 0.0):
+        """
+        Set accelerometer values (normalized)
+
+        Args:
+            x, y, z: Acceleration [-1, 1]
+        """
+        self.accel_x = np.clip(x, -1.0, 1.0)
+
+        # Map X-axis to phase offset
+        self.last_state.phase = self.accel_x * np.pi
+        self.last_state.timestamp = time.time()
+
+    def update(self,
+               heart_rate: Optional[float] = None,
+               gsr: Optional[float] = None,
+               accel: Optional[tuple] = None,
+               **kwargs) -> PhiState:
         """
         Update from sensor data
 
         Args:
-            heart_rate)
-            gsr)
-            accel, y, z) accelerometer tuple (optional)
+            heart_rate: Heart rate in BPM (optional)
+            gsr: GSR value (optional)
+            accel: (x, y, z) accelerometer tuple (optional)
 
         Returns:
             Updated PhiState
         """
-        if heart_rate is not None)
-        if gsr is not None)
-        if accel is not None)
+        if heart_rate is not None:
+            self.set_heart_rate(heart_rate)
+        if gsr is not None:
+            self.set_gsr(gsr)
+        if accel is not None:
+            self.set_accelerometer(*accel)
+
+        return self.last_state
 
+    def get_state(self) -> PhiState:
         return self.last_state
 
-    @lru_cache(maxsize=128)
-    def get_state(self) :
-        logger.error("\n✗ Self-Test FAILED, e)
+
+# Self-test function
+def _self_test():
+    """Test all Φ sources"""
+    print("=" * 60)
+    print("PhiSource Self-Test")
+    print("=" * 60)
+
+    try:
+        # Test Manual Source
+        print("\n1. Testing ManualSource...")
+        manual = ManualSource()
+        manual.set_phase(np.pi / 2)
+        manual.set_depth(0.8)
+        state = manual.get_state()
+        print(f"   {state}")
+        assert abs(state.phase - np.pi/2) < 0.01
+        print("   ✓ ManualSource OK")
+
+        # Test Audio Envelope Source
+        print("\n2. Testing AudioEnvelopeSource...")
+        audio = AudioEnvelopeSource(attack_ms=20, release_ms=100)
+        test_audio = np.random.randn(512) * 0.5
+        state = audio.update(test_audio)
+        print(f"   {state}")
+        print(f"   Envelope: {audio.envelope:.3f}")
+        print("   ✓ AudioEnvelopeSource OK")
+
+        # Test Internal Oscillator
+        print("\n3. Testing InternalOscillatorSource...")
+        internal = InternalOscillatorSource(frequency=0.1)
+        states = []
+        for _ in range(5):
+            state = internal.update()
+            states.append(state.depth)
+            time.sleep(0.1)
+        print(f"   Depth range: [{min(states):.3f}, {max(states):.3f}]")
+        print("   ✓ InternalOscillatorSource OK")
+
+        # Test MIDI Source
+        print("\n4. Testing MIDISource...")
+        midi = MIDISource()
+        midi.set_cc1(64)
+        state = midi.get_state()
+        print(f"   {state}")
+        print(f"   CC1=64 → depth={state.depth:.3f} (expected ≈0.8)")
+        print("   ✓ MIDISource OK")
+
+        # Test Sensor Source
+        print("\n5. Testing SensorSource...")
+        sensor = SensorSource()
+        sensor.set_heart_rate(100)
+        sensor.set_gsr(0.7)
+        state = sensor.get_state()
+        print(f"   {state}")
+        print(f"   HR=100 → freq={state.frequency:.3f}")
+        print("   ✓ SensorSource OK")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/predictive_model.py b/server/predictive_model.py
index accf2897c4756aa878072cca8aafaf3b8bce3ccf..3fcef0a2eae3e21462f774559aefa6f202ec1aa8 100644
--- a/server/predictive_model.py
+++ b/server/predictive_model.py
@@ -1,484 +1,690 @@
 """
 Predictive Model - Feature 016
 Forecasts next consciousness state and key metrics using State Memory history
 
 Implements:
 - FR-001: PredictiveModel class
-
-- FR-003, Δcoherence, Δcriticality
+- FR-002: Input buffer from State Memory (N=128 frames)
+- FR-003: Linear regression for ΔICI, Δcoherence, Δcriticality
 - FR-004: Next probable state prediction
 - FR-005: Forecast JSON emission
 - FR-006: Hooks for preemptive adjustments
 - FR-007: Accuracy tracking and confidence
 
 Success Criteria:
 - SC-001: Prediction accuracy ≥80%
 - SC-002: Forecast latency <50ms
 - SC-003: Overshoot reduction ≥30%
+- SC-004: Average confidence >0.7
+"""
 
 import time
 import numpy as np
 from typing import Optional, Dict, List, Tuple, Callable
 from dataclasses import dataclass
 from collections import deque
 from enum import Enum
 
 
 # Import ConsciousnessState from state_classifier
 try:
     from state_classifier import ConsciousnessState
-except ImportError):
+except ImportError:
+    # Fallback if not available
+    class ConsciousnessState(Enum):
         COMA = "COMA"
         SLEEP = "SLEEP"
         DROWSY = "DROWSY"
         AWAKE = "AWAKE"
         ALERT = "ALERT"
         HYPERSYNC = "HYPERSYNC"
 
 
 @dataclass
-class PredictiveModelConfig)
+class PredictiveModelConfig:
+    """Configuration for Predictive Model"""
+
+    # Buffer size for prediction (FR-002)
     buffer_size: int = 128
 
     # Prediction horizon in seconds
     prediction_horizon: float = 1.5  # Predict 1-2 seconds ahead
 
     # Minimum buffer size for prediction
     min_buffer_size: int = 50
 
     # Confidence threshold for emitting predictions
     confidence_threshold: float = 0.5
 
     # Accuracy tracking window
     accuracy_window: int = 100
 
     # Enable logging
     enable_logging: bool = True
 
     # Log interval in seconds
     log_interval: float = 10.0
 
 
 @dataclass
 class ForecastFrame:
     """Single forecast prediction"""
 
     # Predicted state
     predicted_state: ConsciousnessState
 
     # Predicted metric changes
     delta_ici: float
     delta_coherence: float
     delta_criticality: float
 
     # Predicted absolute values
     predicted_ici: float
     predicted_coherence: float
-    predicted_criticality, 1]
-    confidence)
+    predicted_criticality: float
+
+    # Confidence score [0, 1]
+    confidence: float
+
+    # Prediction horizon (seconds)
     t_pred: float
 
     # Timestamp
     timestamp: float
 
 
 class PredictiveModel:
     """
     Predictive Model for consciousness state and metrics forecasting
 
-    Uses linear regression on recent State Memory history to predict, coherence, and criticality
+    Uses linear regression on recent State Memory history to predict:
+    - Next consciousness state
+    - Changes in ICI, coherence, and criticality
     - Confidence scores based on historical accuracy
 
     Provides hooks for preemptive system adjustments.
     """
 
-    def __init__(self, config: Optional[PredictiveModelConfig]) :
+    def __init__(self, config: Optional[PredictiveModelConfig] = None):
         """
         Initialize Predictive Model
 
         Args:
-            config)
+            config: PredictiveModelConfig (uses defaults if None)
         """
         self.config = config or PredictiveModelConfig()
 
         # Input buffer (filled by State Memory)
-        self.input_buffer)
+        self.input_buffer: deque = deque(maxlen=self.config.buffer_size)
 
         # Last forecast
         self.last_forecast: Optional[ForecastFrame] = None
 
         # Accuracy tracking
-        self.prediction_history)
-        self.actual_outcomes)
+        self.prediction_history: deque = deque(maxlen=self.config.accuracy_window)
+        self.actual_outcomes: deque = deque(maxlen=self.config.accuracy_window)
 
         # Performance tracking
         self.forecast_times: List[float] = []
-        self.total_forecasts, FR-006)
-        self.forecast_callback, None]] = None
+        self.total_forecasts: int = 0
+
+        # Forecast callback (FR-005, FR-006)
+        self.forecast_callback: Optional[Callable[[Dict], None]] = None
 
         # Logging
-        self.last_log_time)
-        logger.info("[PredictiveModel]   buffer_size=%s", self.config.buffer_size)
-        logger.info("[PredictiveModel]   prediction_horizon=%ss", self.config.prediction_horizon)
-        logger.info("[PredictiveModel]   min_buffer_size=%s", self.config.min_buffer_size)
+        self.last_log_time: float = 0.0
+
+        print("[PredictiveModel] Initialized")
+        print(f"[PredictiveModel]   buffer_size={self.config.buffer_size}")
+        print(f"[PredictiveModel]   prediction_horizon={self.config.prediction_horizon}s")
+        print(f"[PredictiveModel]   min_buffer_size={self.config.min_buffer_size}")
 
-    @lru_cache(maxsize=128)
     def add_frame(self,
-                  ici,
-                  coherence,
-                  criticality,
-                  current_state,
-                  timestamp) :
+                  ici: float,
+                  coherence: float,
+                  criticality: float,
+                  current_state: str,
+                  timestamp: float) -> Optional[ForecastFrame]:
+        """
+        Add metrics frame and compute forecast (FR-002, FR-003)
+
+        Args:
             ici: Current ICI value
             coherence: Current phase coherence
             criticality: Current criticality
             current_state: Current consciousness state
             timestamp: Frame timestamp
 
-        Returns, None otherwise
+        Returns:
+            ForecastFrame if prediction made, None otherwise
         """
         # Add to buffer
         self.input_buffer.append({
-            'ici',
-            'coherence',
-            'criticality',
-            'state',
-            'timestamp')
+            'ici': ici,
+            'coherence': coherence,
+            'criticality': criticality,
+            'state': current_state,
+            'timestamp': timestamp
+        })
 
         # Check if we have enough data (edge case handling)
-        if len(self.input_buffer) < self.config.min_buffer_size)
+        if len(self.input_buffer) < self.config.min_buffer_size:
+            return None
+
+        # Compute forecast
+        start_time = time.perf_counter()
         forecast = self._compute_forecast()
         elapsed = (time.perf_counter() - start_time) * 1000  # Convert to ms
 
         self.forecast_times.append(elapsed)
-        if len(self.forecast_times) > 100)
+        if len(self.forecast_times) > 100:
+            self.forecast_times.pop(0)
 
         self.total_forecasts += 1
         self.last_forecast = forecast
 
         # Emit forecast via callback (FR-005)
-        if forecast and self.forecast_callback)
+        if forecast and self.forecast_callback:
+            forecast_json = self._forecast_to_json(forecast)
             self.forecast_callback(forecast_json)
 
         # Log if enabled
-        if self.config.enable_logging)
-            if current_time - self.last_log_time >= self.config.log_interval)
+        if self.config.enable_logging:
+            current_time = time.time()
+            if current_time - self.last_log_time >= self.config.log_interval:
+                self._log_stats()
                 self.last_log_time = current_time
 
         return forecast
 
-    @lru_cache(maxsize=128)
-    def _compute_forecast(self) :
+    def _compute_forecast(self) -> Optional[ForecastFrame]:
+        """
+        Compute forecast using linear regression (FR-003, FR-004)
+
+        Returns:
+            ForecastFrame with predictions
+        """
+        if len(self.input_buffer) < self.config.min_buffer_size:
+            return None
+
+        # Extract time series
+        buffer_list = list(self.input_buffer)
+
+        ici_series = np.array([f['ici'] for f in buffer_list])
+        coherence_series = np.array([f['coherence'] for f in buffer_list])
+        criticality_series = np.array([f['criticality'] for f in buffer_list])
+        timestamps = np.array([f['timestamp'] for f in buffer_list])
+
+        # Compute time deltas (for regression)
+        dt_series = np.diff(timestamps)
+        avg_dt = np.mean(dt_series) if len(dt_series) > 0 else 0.033  # ~30Hz fallback
+
+        # Linear regression for each metric (FR-003)
+        delta_ici = self._predict_delta(ici_series, avg_dt)
+        delta_coherence = self._predict_delta(coherence_series, avg_dt)
+        delta_criticality = self._predict_delta(criticality_series, avg_dt)
+
+        # Current values (from most recent frame)
+        current_ici = ici_series[-1]
+        current_coherence = coherence_series[-1]
+        current_criticality = criticality_series[-1]
+
+        # Predicted values at prediction horizon
+        steps_ahead = int(self.config.prediction_horizon / avg_dt)
+        predicted_ici = current_ici + delta_ici * steps_ahead
+        predicted_coherence = current_coherence + delta_coherence * steps_ahead
+        predicted_criticality = current_criticality + delta_criticality * steps_ahead
+
+        # Clamp predictions to reasonable ranges
+        predicted_ici = np.clip(predicted_ici, 0.0, 1.0)
+        predicted_coherence = np.clip(predicted_coherence, 0.0, 1.0)
+        predicted_criticality = np.clip(predicted_criticality, 0.5, 1.5)
+
+        # Predict next state using Feature 015 thresholds (FR-004)
+        predicted_state = self._predict_state(
+            predicted_ici,
+            predicted_coherence,
+            predicted_criticality
+        )
+
+        # Calculate confidence (FR-007)
+        confidence = self._calculate_confidence(
+            ici_series,
+            coherence_series,
+            criticality_series
+        )
+
+        # Create forecast frame
+        forecast = ForecastFrame(
+            predicted_state=predicted_state,
+            delta_ici=delta_ici,
+            delta_coherence=delta_coherence,
+            delta_criticality=delta_criticality,
+            predicted_ici=predicted_ici,
+            predicted_coherence=predicted_coherence,
+            predicted_criticality=predicted_criticality,
+            confidence=confidence,
+            t_pred=self.config.prediction_horizon,
+            timestamp=time.time()
+        )
+
+        return forecast
+
+    def _predict_delta(self, series: np.ndarray, dt: float) -> float:
         """
         Predict rate of change using linear regression
 
         Args:
             series: Time series of metric values
             dt: Average time step
 
-        Returns) < 2)
+        Returns:
+            Predicted delta per time step
+        """
+        if len(series) < 2:
+            return 0.0
+
+        # Simple linear regression
+        n = len(series)
         x = np.arange(n)
 
         # Check for valid data
-        if np.any(np.isnan(series)) or np.any(np.isinf(series):
+        if np.any(np.isnan(series)) or np.any(np.isinf(series)):
             return 0.0
 
         # Compute slope
-        try, series, 1)
+        try:
+            coeffs = np.polyfit(x, series, 1)
             slope = coeffs[0]
-        except)
+        except:
+            slope = 0.0
+
+        return float(slope)
 
     def _predict_state(self,
-                       ici,
-                       coherence,
-                       criticality) :
+                       ici: float,
+                       coherence: float,
+                       criticality: float) -> ConsciousnessState:
+        """
+        Predict consciousness state using Feature 015 thresholds (FR-004)
+
+        Args:
             ici: Predicted ICI
             coherence: Predicted coherence
-            criticality)
+            criticality: Predicted criticality (not used in classification but available)
 
         Returns:
             Predicted ConsciousnessState
         """
         # Use same thresholds as state_classifier.py
 
         # HYPERSYNC: Very high ICI and coherence
         if ici > 0.9 and coherence > 0.9:
             return ConsciousnessState.HYPERSYNC
 
         # ALERT: High ICI and coherence
         if ici > 0.7 and coherence > 0.7:
             return ConsciousnessState.ALERT
 
         # COMA: Very low activity
         if ici < 0.1 and coherence < 0.2:
             return ConsciousnessState.COMA
 
         # SLEEP: Low activity with some coherence
         if coherence < 0.4:
             # Note: In state_classifier we also check spectral_centroid < 10
             # Here we approximate with coherence < 0.4 and ici < 0.3
             if ici < 0.3:
                 return ConsciousnessState.SLEEP
 
         # AWAKE: Moderate activity
         if 0.3 <= ici <= 0.7 and coherence >= 0.4:
             return ConsciousnessState.AWAKE
 
         # DROWSY: Low ICI
-        if ici < 0.3)
+        if ici < 0.3:
+            return ConsciousnessState.DROWSY
+
+        # Default to AWAKE
+        return ConsciousnessState.AWAKE
+
     def _calculate_confidence(self,
-                             ici_series,
-                             coherence_series,
-                             criticality_series) :
+                             ici_series: np.ndarray,
+                             coherence_series: np.ndarray,
+                             criticality_series: np.ndarray) -> float:
+        """
+        Calculate prediction confidence based on trend stability (FR-007)
+
+        High confidence when:
+        - Trends are consistent (low variance in derivatives)
+        - Recent history is stable
+        - Historical accuracy is high
+
+        Args:
             ici_series: Recent ICI history
             coherence_series: Recent coherence history
             criticality_series: Recent criticality history
 
-        Returns, 1]
+        Returns:
+            Confidence score [0, 1]
         """
-        # Component 1)
-        @lru_cache(maxsize=128)
-        def compute_r_squared(series) :
+        # Component 1: Trend consistency (R² of linear fit)
+        def compute_r_squared(series):
+            if len(series) < 3:
+                return 0.0
+
+            x = np.arange(len(series))
+            try:
+                coeffs = np.polyfit(x, series, 1)
+                fit = np.polyval(coeffs, x)
+                residuals = series - fit
+                ss_res = np.sum(residuals ** 2)
+                ss_tot = np.sum((series - np.mean(series)) ** 2)
+
+                if ss_tot < 1e-10:
+                    return 1.0  # Constant series = perfect fit
+
+                r_squared = 1 - (ss_res / ss_tot)
+                return np.clip(r_squared, 0.0, 1.0)
+            except:
                 return 0.0
 
-        r2_ici = compute_r_squared(ici_series[-30)  # Last 30 frames
-        r2_coherence = compute_r_squared(coherence_series[-30)
-        r2_criticality = compute_r_squared(criticality_series[-30)
+        r2_ici = compute_r_squared(ici_series[-30:])  # Last 30 frames
+        r2_coherence = compute_r_squared(coherence_series[-30:])
+        r2_criticality = compute_r_squared(criticality_series[-30:])
 
         trend_confidence = (r2_ici + r2_coherence + r2_criticality) / 3.0
 
-        # Component 2) > 10)[-20)
+        # Component 2: Historical accuracy
+        if len(self.prediction_history) > 10:
+            recent_accuracy = np.mean([p['correct'] for p in list(self.prediction_history)[-20:]])
         else:
             recent_accuracy = 0.5  # Neutral when no history
 
-        # Component 3)
+        # Component 3: Data stability (low recent variance = high confidence)
         recent_std = (
-            np.std(ici_series[-10) +
-            np.std(coherence_series[-10) +
-            np.std(criticality_series[-10)
+            np.std(ici_series[-10:]) +
+            np.std(coherence_series[-10:]) +
+            np.std(criticality_series[-10:])
         ) / 3.0
         stability_confidence = max(0.0, 1.0 - recent_std * 2.0)
 
         # Weighted combination
         confidence = (
             0.4 * trend_confidence +
             0.3 * recent_accuracy +
             0.3 * stability_confidence
+        )
 
         return float(np.clip(confidence, 0.0, 1.0))
 
-    def record_outcome(self, actual_state: str, actual_metrics: Dict[str, float]) :
+    def record_outcome(self, actual_state: str, actual_metrics: Dict[str, float]):
+        """
+        Record actual outcome for accuracy tracking (FR-007)
+
+        Args:
             actual_state: Actual consciousness state that occurred
-            actual_metrics, coherence, criticality)
+            actual_metrics: Actual metric values (ici, coherence, criticality)
         """
-        if not self.last_forecast)
+        if not self.last_forecast:
+            return
+
+        # Check state prediction accuracy
+        predicted_state_str = self.last_forecast.predicted_state.value
+        state_correct = (predicted_state_str == actual_state)
 
         # Check metric prediction accuracy (within tolerance)
         ici_error = abs(self.last_forecast.predicted_ici - actual_metrics.get('ici', 0.0))
         coherence_error = abs(self.last_forecast.predicted_coherence - actual_metrics.get('coherence', 0.0))
         criticality_error = abs(self.last_forecast.predicted_criticality - actual_metrics.get('criticality', 1.0))
 
         # Record for accuracy tracking
         self.prediction_history.append({
-            'correct',
-            'ici_error',
-            'coherence_error',
-            'criticality_error',
-            'timestamp')
+            'correct': state_correct,
+            'ici_error': ici_error,
+            'coherence_error': coherence_error,
+            'criticality_error': criticality_error,
+            'timestamp': time.time()
         })
 
         self.actual_outcomes.append({
-            'state',
-            'metrics',
-            'timestamp')
+            'state': actual_state,
+            'metrics': actual_metrics,
+            'timestamp': time.time()
         })
 
-    @lru_cache(maxsize=128)
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
+        """
+        Get prediction statistics (FR-007, SC-001, SC-002, SC-004)
+
+        Returns:
+            Dictionary with performance metrics
+        """
+        if len(self.prediction_history) == 0:
             return {
-                'total_forecasts',
-                'prediction_accuracy',
-                'avg_forecast_time_ms',
-                'avg_confidence',
-                'buffer_size')
+                'total_forecasts': self.total_forecasts,
+                'prediction_accuracy': 0.0,
+                'avg_forecast_time_ms': 0.0,
+                'avg_confidence': 0.0,
+                'buffer_size': len(self.input_buffer)
             }
 
         # State prediction accuracy (SC-001)
         state_accuracy = np.mean([p['correct'] for p in self.prediction_history])
 
         # Average errors
         avg_ici_error = np.mean([p['ici_error'] for p in self.prediction_history])
         avg_coherence_error = np.mean([p['coherence_error'] for p in self.prediction_history])
         avg_criticality_error = np.mean([p['criticality_error'] for p in self.prediction_history])
 
         # Forecast time (SC-002)
         avg_forecast_time = np.mean(self.forecast_times) if self.forecast_times else 0.0
         max_forecast_time = np.max(self.forecast_times) if self.forecast_times else 0.0
 
         # Average confidence (SC-004)
         if self.last_forecast:
             avg_confidence = self.last_forecast.confidence
         else:
             avg_confidence = 0.0
 
         return {
-            'total_forecasts',
-            'prediction_accuracy'),
-            'avg_ici_error'),
-            'avg_coherence_error'),
-            'avg_criticality_error'),
-            'avg_forecast_time_ms'),
-            'max_forecast_time_ms'),
-            'avg_confidence'),
-            'buffer_size'),
-            'prediction_history_size')
+            'total_forecasts': self.total_forecasts,
+            'prediction_accuracy': float(state_accuracy),
+            'avg_ici_error': float(avg_ici_error),
+            'avg_coherence_error': float(avg_coherence_error),
+            'avg_criticality_error': float(avg_criticality_error),
+            'avg_forecast_time_ms': float(avg_forecast_time),
+            'max_forecast_time_ms': float(max_forecast_time),
+            'avg_confidence': float(avg_confidence),
+            'buffer_size': len(self.input_buffer),
+            'prediction_history_size': len(self.prediction_history)
         }
 
-    def get_last_forecast(self) :
+    def get_last_forecast(self) -> Optional[Dict]:
+        """
+        Get last forecast as JSON (FR-005)
+
+        Returns:
             Forecast JSON or None
         """
-        if not self.last_forecast)
+        if not self.last_forecast:
+            return None
+
+        return self._forecast_to_json(self.last_forecast)
+
+    def _forecast_to_json(self, forecast: ForecastFrame) -> Dict:
+        """
+        Convert ForecastFrame to JSON (FR-005)
 
-    def _forecast_to_json(self, forecast) :
+        Args:
             forecast: ForecastFrame to convert
 
         Returns:
             JSON dictionary
         """
         return {
-            'type',
-            'state',
-            'confidence',
-            't_pred',
+            'type': 'forecast',
+            'state': forecast.predicted_state.value,
+            'confidence': forecast.confidence,
+            't_pred': forecast.t_pred,
             'predicted_metrics': {
-                'ici',
-                'coherence',
-                'criticality',
+                'ici': forecast.predicted_ici,
+                'coherence': forecast.predicted_coherence,
+                'criticality': forecast.predicted_criticality
+            },
             'deltas': {
-                'ici',
-                'coherence',
-                'criticality',
-            'timestamp') : "
-              f"accuracy={stats['prediction_accuracy'], "
+                'ici': forecast.delta_ici,
+                'coherence': forecast.delta_coherence,
+                'criticality': forecast.delta_criticality
+            },
+            'timestamp': forecast.timestamp
+        }
+
+    def _log_stats(self):
+        """Log performance statistics"""
+        stats = self.get_statistics()
+
+        print(f"[PredictiveModel] Stats: "
+              f"accuracy={stats['prediction_accuracy']:.2%}, "
               f"forecasts={stats['total_forecasts']}, "
-              f"avg_time={stats['avg_forecast_time_ms'], "
-              f"confidence={stats['avg_confidence'])
+              f"avg_time={stats['avg_forecast_time_ms']:.2f}ms, "
+              f"confidence={stats['avg_confidence']:.2f}")
 
-    def reset(self) -> None)
+    def reset(self):
+        """Reset model state"""
+        self.input_buffer.clear()
         self.last_forecast = None
         self.prediction_history.clear()
         self.actual_outcomes.clear()
         self.forecast_times.clear()
         self.total_forecasts = 0
 
-        logger.info("[PredictiveModel] State reset")
+        print("[PredictiveModel] State reset")
 
 
 # Self-test function
-def _self_test() -> None)
-    logger.info("Predictive Model Self-Test")
-    logger.info("=" * 60)
-
-    # Test 1)
+def _self_test():
+    """Test Predictive Model"""
+    print("=" * 60)
+    print("Predictive Model Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = PredictiveModelConfig(
         buffer_size=128,
         prediction_horizon=1.5,
         min_buffer_size=50
-
+    )
     model = PredictiveModel(config)
 
     assert len(model.input_buffer) == 0
     assert model.last_forecast is None
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Test 2)
+    # Test 2: Buffer filling
+    print("\n2. Testing buffer filling...")
 
     # Fill buffer with synthetic data (not enough for prediction)
-    for i in range(30),
+    for i in range(30):
+        forecast = model.add_frame(
+            ici=0.5,
             coherence=0.6,
             criticality=1.0,
             current_state="AWAKE",
             timestamp=time.time() + i * 0.033
-
+        )
         assert forecast is None, "Should not predict with insufficient data"
 
     assert len(model.input_buffer) == 30
-    logger.info("   OK)")
+    print("   OK: Buffer filling (no prediction yet)")
 
-    # Test 3)
+    # Test 3: First prediction
+    print("\n3. Testing first prediction...")
 
     # Add more frames to reach min_buffer_size
-    for i in range(30, 60),  # Increasing trend
+    for i in range(30, 60):
+        forecast = model.add_frame(
+            ici=0.5 + i * 0.005,  # Increasing trend
             coherence=0.6 + i * 0.003,
             criticality=1.0,
             current_state="AWAKE",
             timestamp=time.time() + i * 0.033
+        )
 
     assert forecast is not None, "Should have prediction now"
     assert forecast.confidence >= 0.0 and forecast.confidence <= 1.0
-    logger.info("   Predicted state, forecast.predicted_state.value)
-    logger.info("   Confidence, forecast.confidence)
-    logger.info("   Predicted ICI, forecast.predicted_ici)
-    logger.info("   OK)
+    print(f"   Predicted state: {forecast.predicted_state.value}")
+    print(f"   Confidence: {forecast.confidence:.2f}")
+    print(f"   Predicted ICI: {forecast.predicted_ici:.3f}")
+    print("   OK: First prediction")
 
-    # Test 4)
+    # Test 4: State transition prediction
+    print("\n4. Testing state transition...")
 
     # Simulate transition to ALERT (high ICI and coherence)
-    for i in range(60, 100)) * 0.01  # Rapid increase
+    for i in range(60, 100):
+        ici = 0.5 + (i - 60) * 0.01  # Rapid increase
         coherence = 0.6 + (i - 60) * 0.008
 
         forecast = model.add_frame(
             ici=ici,
             coherence=coherence,
             criticality=1.0,
             current_state="AWAKE",
             timestamp=time.time() + i * 0.033
+        )
 
     assert forecast is not None
-    logger.info("   Final predicted state, forecast.predicted_state.value)
-    logger.info("   Expected)")
-    logger.info("   OK)
+    print(f"   Final predicted state: {forecast.predicted_state.value}")
+    print(f"   Expected: ALERT or HYPERSYNC (high ICI/coherence)")
+    print("   OK: State transition prediction")
 
-    # Test 5)
+    # Test 5: Accuracy tracking
+    print("\n5. Testing accuracy tracking...")
 
     # Record some outcomes
     model.record_outcome(
         actual_state=forecast.predicted_state.value,
-        actual_metrics={'ici', 'coherence', 'criticality')
+        actual_metrics={'ici': forecast.predicted_ici, 'coherence': forecast.predicted_coherence, 'criticality': 1.0}
+    )
 
     stats = model.get_statistics()
-    logger.info("   Total forecasts, stats['total_forecasts'])
-    logger.info("   Prediction accuracy, stats['prediction_accuracy'])
-    logger.info("   Avg forecast time, stats['avg_forecast_time_ms'])
+    print(f"   Total forecasts: {stats['total_forecasts']}")
+    print(f"   Prediction accuracy: {stats['prediction_accuracy']:.2%}")
+    print(f"   Avg forecast time: {stats['avg_forecast_time_ms']:.2f}ms")
 
-    assert stats['avg_forecast_time_ms'] < 50.0, f"Forecast time {stats['avg_forecast_time_ms'])"
-    logger.info("   OK)
+    assert stats['avg_forecast_time_ms'] < 50.0, f"Forecast time {stats['avg_forecast_time_ms']:.2f}ms exceeds 50ms target (SC-002)"
+    print("   OK: Accuracy tracking and latency")
 
-    # Test 6)
+    # Test 6: JSON emission
+    print("\n6. Testing JSON emission...")
 
     forecast_json = model.get_last_forecast()
     assert forecast_json is not None
     assert forecast_json['type'] == 'forecast'
     assert 'state' in forecast_json
     assert 'confidence' in forecast_json
     assert 't_pred' in forecast_json
 
-    logger.info("   Forecast JSON, forecast_json['state'], forecast_json['confidence'])
-    logger.info("   OK)
+    print(f"   Forecast JSON: {forecast_json['state']} @ {forecast_json['confidence']:.2f} confidence")
+    print("   OK: JSON emission")
 
-    # Test 7)
+    # Test 7: Reset
+    print("\n7. Testing reset...")
 
     model.reset()
     assert len(model.input_buffer) == 0
     assert model.last_forecast is None
     assert model.total_forecasts == 0
-    logger.info("   OK)
+    print("   OK: Reset")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/preset_api.py b/server/preset_api.py
index 64c39ba1236079f165a06036bc892922daf83226..4daeddbd9a7a69915d67657e4b031f2a43d79491 100644
--- a/server/preset_api.py
+++ b/server/preset_api.py
@@ -1,293 +1,364 @@
 """
 Preset REST API - FastAPI Endpoints
 
-Implements FR-002, FR-003, FR-010)
-
+Implements FR-002, FR-003, FR-010: Complete REST API for preset management
+"""
 
 from fastapi import FastAPI, HTTPException, UploadFile, File, Query
 from fastapi.responses import JSONResponse, StreamingResponse
 from typing import Optional
 import json
 import io
 
 from .preset_model import Preset, CollisionPolicy
 from .preset_store import PresetStore
 from .ab_snapshot import ABSnapshot
 
 
 # Initialize stores (will be set by main app)
 preset_store: Optional[PresetStore] = None
 ab_manager: Optional[ABSnapshot] = None
 
 
-def create_preset_api(store, ab) :
+def create_preset_api(store: PresetStore, ab: ABSnapshot) -> FastAPI:
     """
     Create FastAPI application with preset endpoints
 
     Args:
         store: PresetStore instance
         ab: ABSnapshot manager instance
 
-    Returns, version="1.0")
+    Returns:
+        FastAPI application
+    """
+    app = FastAPI(title="Soundlab Preset API", version="1.0")
 
     # Store references
     global preset_store, ab_manager
     preset_store = store
     ab_manager = ab
 
     # --- CRUD Endpoints ---
 
     @app.get("/api/presets")
     async def list_presets(
-        query, description="Search query (name/notes)"),
-        tag, description="Filter by tag"),
-        limit, ge=1, le=200, description="Maximum results"):
+        query: Optional[str] = Query(None, description="Search query (name/notes)"),
+        tag: Optional[str] = Query(None, description="Filter by tag"),
+        limit: int = Query(50, ge=1, le=200, description="Maximum results")
+    ):
         """
         List presets with optional filtering
 
         Implements: GET /api/presets?query=&tag=&limit=50
         """
-        try, tag=tag, limit=limit)
+        try:
+            presets = preset_store.list(query=query, tag=tag, limit=limit)
             return JSONResponse(content=presets)
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.get("/api/presets/{preset_id}")
-    async def get_preset(preset_id):
+    async def get_preset(preset_id: str):
         """
         Get full preset by ID
 
-        Implements)
+        Implements: GET /api/presets/{id}
+        """
+        preset = preset_store.load(preset_id)
 
-        if preset is None, detail=f"Preset {preset_id} not found")
+        if preset is None:
+            raise HTTPException(status_code=404, detail=f"Preset {preset_id} not found")
 
         return JSONResponse(content=preset.to_dict())
 
     @app.post("/api/presets")
     async def create_preset(
-        preset_data,
-        collision, description="Collision resolution strategy"):
+        preset_data: dict,
+        collision: CollisionPolicy = Query("prompt", description="Collision resolution strategy")
+    ):
         """
         Create/save a new preset
 
         Implements: POST /api/presets
 
-        Body, OR
-            - {"name", "collision": "prompt|overwrite|new_copy|merge"}
+        Body:
+            - Full preset JSON, OR
+            - {"name": "...", "collision": "prompt|overwrite|new_copy|merge"}
         """
         try:
             # Check if this is a simple name-only request
-            if 'schema_version' not in preset_data, 'Untitled')
+            if 'schema_version' not in preset_data:
+                # Create new preset with just a name
+                name = preset_data.get('name', 'Untitled')
                 preset = Preset(name=name)
-            else)
+            else:
+                # Full preset data provided
+                preset = Preset.from_dict(preset_data)
 
             # Validate
             preset.validate()
 
             # Save
             preset_id, was_created = preset_store.create(preset, collision=collision)
 
             return JSONResponse(
                 content={
-                    'id',
-                    'created',
-                    'name',
+                    'id': preset_id,
+                    'created': was_created,
+                    'name': preset.name
+                },
                 status_code=201 if was_created else 200
+            )
 
-        except FileExistsError as e, detail=str(e))
-        except ValueError as e, detail=f"Validation error)
-        except Exception as e, detail=str(e))
+        except FileExistsError as e:
+            raise HTTPException(status_code=409, detail=str(e))
+        except ValueError as e:
+            raise HTTPException(status_code=400, detail=f"Validation error: {e}")
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.put("/api/presets/{preset_id}")
-    async def update_preset(preset_id, preset_data):
+    async def update_preset(preset_id: str, preset_data: dict):
         """
         Update existing preset
 
         Implements: PUT /api/presets/{id}
         """
-        try)
+        try:
+            preset = Preset.from_dict(preset_data)
 
             # Ensure ID matches
-            if preset.id != preset_id)
+            if preset.id != preset_id:
+                preset.id = preset_id
+
+            # Validate
+            preset.validate()
 
             # Update
             success = preset_store.update(preset)
 
-            if not success, detail=f"Preset {preset_id} not found")
+            if not success:
+                raise HTTPException(status_code=404, detail=f"Preset {preset_id} not found")
 
-            return JSONResponse(content={'ok', 'id')
+            return JSONResponse(content={'ok': True, 'id': preset_id})
 
-        except ValueError as e, detail=f"Validation error)
-        except Exception as e, detail=str(e))
+        except ValueError as e:
+            raise HTTPException(status_code=400, detail=f"Validation error: {e}")
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.delete("/api/presets/{preset_id}")
-    async def delete_preset(preset_id):
+    async def delete_preset(preset_id: str):
         """
         Delete preset by ID
 
-        Implements)
+        Implements: DELETE /api/presets/{id}
+        """
+        success = preset_store.delete(preset_id)
 
-        if not success, detail=f"Preset {preset_id} not found")
+        if not success:
+            raise HTTPException(status_code=404, detail=f"Preset {preset_id} not found")
 
-        return JSONResponse(content={'ok')
+        return JSONResponse(content={'ok': True})
 
     # --- Import/Export Endpoints ---
 
     @app.post("/api/presets/export")
     async def export_all_presets():
         """
         Export all presets as JSON bundle
 
         Implements: POST /api/presets/export
         Returns: File download
         """
-        try)
+        try:
+            bundle = preset_store.export_all()
             json_data = json.dumps(bundle, indent=2)
 
             # Create streaming response
             return StreamingResponse(
                 io.BytesIO(json_data.encode()),
                 media_type="application/json",
                 headers={
-                    "Content-Disposition")
+                    "Content-Disposition": f"attachment; filename=soundlab_presets_export.json"
+                }
+            )
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     @app.post("/api/presets/import")
     async def import_presets(
-        file),
-        dry_run, description="Validate only, don't save"),
-        collision, description="Collision resolution"):
+        file: UploadFile = File(...),
+        dry_run: bool = Query(False, description="Validate only, don't save"),
+        collision: CollisionPolicy = Query("prompt", description="Collision resolution")
+    ):
         """
         Import preset bundle from JSON file
 
         Implements: POST /api/presets/import?dry_run=true|false&collision=...
 
         Returns:
             {
-                'imported',
-                'updated',
-                'skipped',
+                'imported': int,
+                'updated': int,
+                'skipped': int,
                 'errors': List[str]
             }
         """
-        try)
+        try:
+            # Read uploaded file
+            contents = await file.read()
             bundle = json.loads(contents)
 
             # Import bundle
             results = preset_store.import_bundle(
                 bundle,
                 collision=collision,
                 dry_run=dry_run
+            )
 
             return JSONResponse(content=results)
 
-        except json.JSONDecodeError as e, detail=f"Invalid JSON)
-        except Exception as e, detail=str(e))
+        except json.JSONDecodeError as e:
+            raise HTTPException(status_code=400, detail=f"Invalid JSON: {e}")
+        except Exception as e:
+            raise HTTPException(status_code=500, detail=str(e))
 
     # --- A/B Snapshot Endpoints ---
 
     @app.post("/api/presets/ab/store/{slot}")
-    async def store_ab_snapshot(slot, preset_data):
+    async def store_ab_snapshot(slot: str, preset_data: dict):
         """
         Store current state in A or B slot
 
         Args:
             slot: 'A' or 'B'
-            preset_data, 'B'], detail="Slot must be 'A' or 'B'")
+            preset_data: Preset to store
+        """
+        if slot not in ['A', 'B']:
+            raise HTTPException(status_code=400, detail="Slot must be 'A' or 'B'")
 
-        try)
+        try:
+            preset = Preset.from_dict(preset_data)
 
-            if slot == 'A')
-            else)
+            if slot == 'A':
+                ab_manager.store_a(preset)
+            else:
+                ab_manager.store_b(preset)
 
             return JSONResponse(content={
-                'ok',
-                'slot',
-                'status')
+                'ok': True,
+                'slot': slot,
+                'status': ab_manager.get_status()
             })
 
-        except Exception as e, detail=str(e))
+        except Exception as e:
+            raise HTTPException(status_code=400, detail=str(e))
 
     @app.get("/api/presets/ab/get/{slot}")
-    async def get_ab_snapshot(slot):
+    async def get_ab_snapshot(slot: str):
         """
         Get snapshot from A or B slot
 
         Args:
-            slot, 'B'], detail="Slot must be 'A' or 'B'")
+            slot: 'A' or 'B'
+        """
+        if slot not in ['A', 'B']:
+            raise HTTPException(status_code=400, detail="Slot must be 'A' or 'B'")
 
         preset = ab_manager.get_a() if slot == 'A' else ab_manager.get_b()
 
-        if preset is None, detail=f"Slot {slot} is empty")
+        if preset is None:
+            raise HTTPException(status_code=404, detail=f"Slot {slot} is empty")
 
         return JSONResponse(content=preset.to_dict())
 
     @app.post("/api/presets/ab/toggle")
     async def toggle_ab():
         """
         Toggle between A and B snapshots
 
         Returns preset to apply or error if not possible
         """
-        try)
+        try:
+            preset = ab_manager.toggle()
 
-            if preset is None,
-                    detail=f"Toggle too fast (guard time)"
+            if preset is None:
+                raise HTTPException(
+                    status_code=429,
+                    detail=f"Toggle too fast (guard time: {ab_manager.CROSSFADE_GUARD_MS}ms)"
+                )
 
             return JSONResponse(content={
-                'ok',
-                'current_slot'),
-                'preset')
+                'ok': True,
+                'current_slot': ab_manager.get_current_slot(),
+                'preset': preset.to_dict()
             })
 
-        except ValueError as e, detail=str(e))
+        except ValueError as e:
+            raise HTTPException(status_code=400, detail=str(e))
 
     @app.get("/api/presets/ab/status")
-    async def get_ab_status())
+    async def get_ab_status():
+        """Get A/B comparison status"""
+        status = ab_manager.get_status()
         return JSONResponse(content=status)
 
     @app.get("/api/presets/ab/diff")
-    async def get_ab_diff())
+    async def get_ab_diff():
+        """Get differences between A and B"""
+        diff = ab_manager.get_diff()
 
-        if diff is None, detail="Cannot compare)
+        if diff is None:
+            raise HTTPException(status_code=400, detail="Cannot compare: A or B is empty")
 
         return JSONResponse(content=diff)
 
     @app.delete("/api/presets/ab/clear/{slot}")
-    async def clear_ab_snapshot(slot), B, or both slots"""
-        if slot == 'A')
-        elif slot == 'B')
-        elif slot == 'all')
-        else, detail="Slot must be 'A', 'B', or 'all'")
-
-        return JSONResponse(content={'ok')
+    async def clear_ab_snapshot(slot: str):
+        """Clear A, B, or both slots"""
+        if slot == 'A':
+            ab_manager.clear_a()
+        elif slot == 'B':
+            ab_manager.clear_b()
+        elif slot == 'all':
+            ab_manager.clear_all()
+        else:
+            raise HTTPException(status_code=400, detail="Slot must be 'A', 'B', or 'all'")
+
+        return JSONResponse(content={'ok': True})
 
     # --- Statistics Endpoint ---
 
     @app.get("/api/presets/stats")
-    async def get_preset_statistics())
+    async def get_preset_statistics():
+        """Get preset store statistics"""
+        stats = preset_store.get_statistics()
         return JSONResponse(content=stats)
 
     return app
 
 
 # Standalone server for testing
-if __name__ == "__main__")
+if __name__ == "__main__":
+    import uvicorn
+
+    # Initialize stores
+    store = PresetStore()
     ab = ABSnapshot()
 
     # Create app
     app = create_preset_api(store, ab)
 
-    logger.info("=" * 60)
-    logger.info("Preset API Test Server")
-    logger.info("=" * 60)
-    logger.info("\nStarting server at http://localhost)
-    logger.info("API docs at http://localhost)
-    logger.info("\nPress Ctrl+C to stop")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Preset API Test Server")
+    print("=" * 60)
+    print("\nStarting server at http://localhost:8000")
+    print("API docs at http://localhost:8000/docs")
+    print("\nPress Ctrl+C to stop")
+    print("=" * 60)
 
     # Run server
     uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
-
-"""  # auto-closed missing docstring
diff --git a/server/preset_model.py b/server/preset_model.py
index 2ea77aef0d4635c230d4b9a44c394d9f8a81d6c4..08253b4b102f3ad4a47ceb190b43cbe2ca58aa4b 100644
--- a/server/preset_model.py
+++ b/server/preset_model.py
@@ -1,314 +1,409 @@
 """
 Preset Data Model - Versioned State Container
 
-Implements FR-001, FR-006)
-
+Implements FR-001, FR-006: JSON schema v1 with validation and migration
+"""
 
 import uuid
 import json
 from typing import List, Optional, Dict, Literal
 from dataclasses import dataclass, field, asdict
 from datetime import datetime
 import copy
 
 
 StrategyType = Literal["spatial", "energy", "linear", "phi"]
 PhiModeType = Literal["manual", "audio", "midi", "sensor", "internal"]
 CollisionPolicy = Literal["prompt", "overwrite", "new_copy", "merge"]
 
 
 @dataclass
 class EngineState:
     """Engine parameters for D-ASE ChromaticFieldProcessor"""
     sample_rate: int = 48000
     num_channels: int = 8
-    frequencies: List[float] = field(default_factory=lambda, 0.81, 1.31, 2.12, 3.43, 5.55, 8.97, 14.52
+    frequencies: List[float] = field(default_factory=lambda: [
+        0.5, 0.81, 1.31, 2.12, 3.43, 5.55, 8.97, 14.52
     ])
-    amplitudes: List[float] = field(default_factory=lambda, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25
+    amplitudes: List[float] = field(default_factory=lambda: [
+        0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25
     ])
-    coupling_strength) :
-            raise ValueError(f"Invalid sample_rate)
+    coupling_strength: float = 1.0
+
+    def validate(self) -> bool:
+        """Validate engine state parameters"""
+        # Check array lengths match num_channels
+        if len(self.frequencies) != self.num_channels:
+            raise ValueError(f"frequencies length {len(self.frequencies)} != num_channels {self.num_channels}")
+        if len(self.amplitudes) != self.num_channels:
+            raise ValueError(f"amplitudes length {len(self.amplitudes)} != num_channels {self.num_channels}")
 
-        if self.coupling_strength < 0.0 or self.coupling_strength > 2.0, 2]")
+        # Check value ranges
+        if self.sample_rate not in [44100, 48000, 96000]:
+            raise ValueError(f"Invalid sample_rate: {self.sample_rate}")
+
+        if self.coupling_strength < 0.0 or self.coupling_strength > 2.0:
+            raise ValueError(f"coupling_strength {self.coupling_strength} out of range [0, 2]")
 
         return True
 
 
 @dataclass
 class PhiState:
     """Φ-modulation parameters"""
     mode: PhiModeType = "internal"
-    depth, 1.618]
+    depth: float = 0.618  # [0, 1.618]
     phase: float = 0.0  # radians
-    frequency)
+    frequency: Optional[float] = 0.1  # Hz (for internal/sensor modes)
 
-    def validate(self) :
+    def validate(self) -> bool:
         """Validate Φ parameters"""
         PHI = 1.618033988749895
 
-        if self.depth < 0.0 or self.depth > PHI, {PHI}]")
+        if self.depth < 0.0 or self.depth > PHI:
+            raise ValueError(f"phi.depth {self.depth} out of range [0, {PHI}]")
 
-        if self.phase < -6.28318 or self.phase > 6.28318, 2π]")
+        if self.phase < -6.28318 or self.phase > 6.28318:
+            raise ValueError(f"phi.phase {self.phase} out of range [-2π, 2π]")
 
         if self.frequency is not None:
-            if self.frequency < 0.01 or self.frequency > 10.0, 10]")
+            if self.frequency < 0.01 or self.frequency > 10.0:
+                raise ValueError(f"phi.frequency {self.frequency} out of range [0.01, 10]")
 
         return True
 
 
 @dataclass
 class DownmixState:
     """Downmix strategy and weights"""
     strategy: StrategyType = "spatial"
-    weights_l: List[float] = field(default_factory=lambda, 0.6, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0
+    weights_l: List[float] = field(default_factory=lambda: [
+        0.8, 0.6, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0
     ])
-    weights_r: List[float] = field(default_factory=lambda, 0.0, 0.0, 0.0, 0.2, 0.4, 0.6, 0.8
+    weights_r: List[float] = field(default_factory=lambda: [
+        0.0, 0.0, 0.0, 0.0, 0.2, 0.4, 0.6, 0.8
     ])
 
-    @lru_cache(maxsize=128)
-    def validate(self) :
+    def validate(self) -> bool:
+        """Validate downmix parameters"""
+        if len(self.weights_l) != 8:
+            raise ValueError(f"weights_l length {len(self.weights_l)} != 8")
+        if len(self.weights_r) != 8:
+            raise ValueError(f"weights_r length {len(self.weights_r)} != 8")
+
+        return True
+
+
+@dataclass
+class UIState:
     """UI configuration"""
     fft_size: int = 2048
-    meters: Dict = field(default_factory=lambda: {"show")
-    visualizer: Dict = field(default_factory=lambda: {"palette")
+    meters: Dict = field(default_factory=lambda: {"show": True})
+    visualizer: Dict = field(default_factory=lambda: {"palette": "chromatic"})
 
 
 @dataclass
 class Preset:
     """
     Complete preset with metadata and versioned schema
 
     Conforms to schema v1 requirements
     """
     # Metadata
     schema_version: int = 1
-    id: str = field(default_factory=lambda)))
+    id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = "Untitled Preset"
-    tags)
-    created_at: str = field(default_factory=lambda).isoformat() + 'Z')
-    modified_at: str = field(default_factory=lambda).isoformat() + 'Z')
+    tags: List[str] = field(default_factory=list)
+    created_at: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')
+    modified_at: str = field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')
     author: str = "user@local"
     notes: str = ""
 
     # State components
-    engine)
-    phi)
-    downmix)
-    ui)
+    engine: EngineState = field(default_factory=EngineState)
+    phi: PhiState = field(default_factory=PhiState)
+    downmix: DownmixState = field(default_factory=DownmixState)
+    ui: UIState = field(default_factory=UIState)
 
-    def validate(self) :
+    def validate(self) -> bool:
         """
         Validate entire preset
 
         Returns:
             True if valid
 
         Raises:
             ValueError: If validation fails
         """
         # Validate schema version
         if self.schema_version != 1:
-            raise ValueError(f"Unsupported schema_version)
+            raise ValueError(f"Unsupported schema_version: {self.schema_version}")
 
         # Validate name
-        if not self.name or len(self.name) == 0)
+        if not self.name or len(self.name) == 0:
+            raise ValueError("name cannot be empty")
 
         # Validate components
         self.engine.validate()
         self.phi.validate()
         self.downmix.validate()
 
         return True
 
-    def to_dict(self) :
+    def to_dict(self) -> Dict:
+        """Convert to dictionary for JSON serialization"""
+        return asdict(self)
+
+    def to_json(self, pretty: bool = False) -> str:
         """
         Convert to JSON string
 
         Args:
-            pretty, format with indentation
+            pretty: If True, format with indentation
 
-        if pretty, indent=2)
-        else)
+        Returns:
+            JSON string
+        """
+        data = self.to_dict()
+        if pretty:
+            return json.dumps(data, indent=2)
+        else:
+            return json.dumps(data)
 
     @classmethod
-    def from_dict(cls, data) :
+    def from_dict(cls, data: Dict) -> 'Preset':
         """
         Create Preset from dictionary
 
         Args:
             data: Dictionary representation
 
-        Returns, dict))
+        Returns:
+            Preset instance
+        """
+        # Handle nested objects
+        if 'engine' in data and isinstance(data['engine'], dict):
+            data['engine'] = EngineState(**data['engine'])
 
-        if 'phi' in data and isinstance(data['phi'], dict))
+        if 'phi' in data and isinstance(data['phi'], dict):
+            data['phi'] = PhiState(**data['phi'])
 
-        if 'downmix' in data and isinstance(data['downmix'], dict))
+        if 'downmix' in data and isinstance(data['downmix'], dict):
+            data['downmix'] = DownmixState(**data['downmix'])
 
-        if 'ui' in data and isinstance(data['ui'], dict))
+        if 'ui' in data and isinstance(data['ui'], dict):
+            data['ui'] = UIState(**data['ui'])
 
         # Filter to only valid fields
-        valid_fields = {k, v in data.items() if k in cls.__dataclass_fields__}
+        valid_fields = {k: v for k, v in data.items() if k in cls.__dataclass_fields__}
 
         return cls(**valid_fields)
 
     @classmethod
-    def from_json(cls, json_str) :
+    def from_json(cls, json_str: str) -> 'Preset':
         """
         Create Preset from JSON string
 
         Args:
             json_str: JSON representation
 
+        Returns:
+            Preset instance
+        """
+        data = json.loads(json_str)
         return cls.from_dict(data)
 
-    def clone(self) :
+    def clone(self) -> 'Preset':
         """
         Create a deep copy of this preset
 
+        Returns:
+            New Preset instance with same values but new ID
+        """
+        data = self.to_dict()
         data['id'] = str(uuid.uuid4())
         data['modified_at'] = datetime.utcnow().isoformat() + 'Z'
         return Preset.from_dict(data)
 
-    def update_timestamp(self) :
+    def update_timestamp(self):
+        """Update modified_at timestamp"""
+        self.modified_at = datetime.utcnow().isoformat() + 'Z'
+
+    def diff(self, other: 'Preset') -> Dict:
         """
         Calculate differences between two presets
 
         Args:
             other: Preset to compare with
 
         Returns:
-            Dictionary of changed fields with {field, new)}
+            Dictionary of changed fields with {field: (old, new)}
         """
         changes = {}
 
-        def compare_recursive(path: str, val1, val2) :
-                    if key in val1 and key in val2, val1[key], val2[key])
-                    elif key in val1, None)
-                    else, val2[key])
+        def compare_recursive(path: str, val1, val2):
+            if isinstance(val1, dict) and isinstance(val2, dict):
+                for key in set(val1.keys()) | set(val2.keys()):
+                    if key in val1 and key in val2:
+                        compare_recursive(f"{path}.{key}" if path else key, val1[key], val2[key])
+                    elif key in val1:
+                        changes[f"{path}.{key}" if path else key] = (val1[key], None)
+                    else:
+                        changes[f"{path}.{key}" if path else key] = (None, val2[key])
             elif isinstance(val1, list) and isinstance(val2, list):
-                if val1 != val2, val2)
+                if val1 != val2:
+                    changes[path] = (val1, val2)
             else:
-                if val1 != val2, val2)
+                if val1 != val2:
+                    changes[path] = (val1, val2)
 
         compare_recursive("", self.to_dict(), other.to_dict())
         return changes
 
 
-def migrate_v0_to_v1(data_v0) :
+def migrate_v0_to_v1(data_v0: Dict) -> Preset:
     """
     Migrate schema v0 to v1
 
-    Legacy field mappings)
+    Legacy field mappings:
+    - "lowGain" → engine.frequencies[0-2] (if present)
     - "midGain" → engine.frequencies[3-4]
     - "highGain" → engine.frequencies[5-7]
     - Add schema_version = 1
 
     Args:
         data_v0: Legacy preset dictionary
 
+    Returns:
+        Migrated Preset (v1)
     """
     # Start with defaults
     preset = Preset()
 
     # Copy metadata if present
     if 'name' in data_v0:
         preset.name = data_v0['name']
-    if 'created_at' in data_v0 or 'timestamp' in data_v0, data_v0.get('timestamp', preset.created_at))
+    if 'created_at' in data_v0 or 'timestamp' in data_v0:
+        preset.created_at = data_v0.get('created_at', data_v0.get('timestamp', preset.created_at))
 
     # Migrate engine parameters (legacy may have different structure)
-    if 'low' in data_v0)
+    if 'low' in data_v0:
+        # Map legacy EQ gains to frequencies (approximate)
         pass  # Would need actual legacy schema to implement
 
     # Migrate Φ parameters
-    if 'drive' in data_v0) / 10.0  # Example mapping
+    if 'drive' in data_v0:
+        preset.phi.depth = float(data_v0['drive']) / 10.0  # Example mapping
 
     # Always set schema_version to 1
     preset.schema_version = 1
 
     # Preserve extra fields if present (forward-compatible)
     # (stored but not validated)
 
     return preset
 
 
-def create_default_preset(name) :
+def create_default_preset(name: str = "Default") -> Preset:
     """
     Create a default preset with safe values
 
     Args:
         name: Preset name
 
-    Returns,
+    Returns:
+        Default Preset instance
+    """
+    return Preset(
+        name=name,
         tags=["default"],
         notes="Default safe configuration"
+    )
+
 
 # Self-test function
-def _self_test() : %s (ID)", preset.name, preset.id[)
-        logger.info("   ✓ Creation OK")
+def _self_test():
+    """Test Preset model functionality"""
+    print("=" * 60)
+    print("Preset Model Self-Test")
+    print("=" * 60)
+
+    try:
+        # Test basic creation
+        print("\n1. Creating preset...")
+        preset = Preset(
+            name="Test Preset",
+            tags=["test", "lab"],
+            notes="Test configuration"
+        )
+        print(f"   Created: {preset.name} (ID: {preset.id[:8]}...)")
+        print("   ✓ Creation OK")
 
         # Test validation
-        logger.info("\n2. Testing validation...")
+        print("\n2. Testing validation...")
         is_valid = preset.validate()
-        logger.info("   Valid, is_valid)
+        print(f"   Valid: {is_valid}")
         assert is_valid
-        logger.info("   ✓ Validation OK")
+        print("   ✓ Validation OK")
 
         # Test JSON serialization
-        logger.info("\n3. Testing JSON serialization...")
+        print("\n3. Testing JSON serialization...")
         json_str = preset.to_json()
-        logger.info("   JSON length, len(json_str))
+        print(f"   JSON length: {len(json_str)} bytes")
 
         preset_restored = Preset.from_json(json_str)
         assert preset_restored.name == preset.name
         assert preset_restored.id == preset.id
-        logger.info("   ✓ JSON round-trip OK")
+        print("   ✓ JSON round-trip OK")
 
         # Test cloning
-        logger.info("\n4. Testing cloning...")
+        print("\n4. Testing cloning...")
         preset_copy = preset.clone()
         assert preset_copy.name == preset.name
         assert preset_copy.id != preset.id  # New ID
-        logger.info("   Cloned with new ID, preset_copy.id[)
-        logger.info("   ✓ Cloning OK")
+        print(f"   Cloned with new ID: {preset_copy.id[:8]}...")
+        print("   ✓ Cloning OK")
 
         # Test diff
-        logger.info("\n5. Testing diff...")
+        print("\n5. Testing diff...")
         preset_copy.name = "Modified Preset"
         preset_copy.engine.coupling_strength = 1.5
         changes = preset.diff(preset_copy)
-        logger.info("   Changes detected, len(changes))
+        print(f"   Changes detected: {len(changes)}")
         for key, (old, new) in list(changes.items())[:3]:
-            logger.info("     %s, key, old, new)
+            print(f"     {key}: {old} → {new}")
         assert 'name' in changes
-        logger.info("   ✓ Diff OK")
+        print("   ✓ Diff OK")
 
         # Test invalid preset
-        logger.error("\n6. Testing validation errors...")
+        print("\n6. Testing validation errors...")
         invalid_preset = Preset(name="Invalid")
         invalid_preset.phi.depth = 99.0  # Out of range
-        try)
+        try:
+            invalid_preset.validate()
             assert False, "Should have raised ValueError"
         except ValueError as e:
-            logger.error("   Caught expected error, e)
-            logger.error("   ✓ Validation error handling OK")
+            print(f"   Caught expected error: {e}")
+            print("   ✓ Validation error handling OK")
 
         # Test default preset
-        logger.info("\n7. Testing default preset...")
+        print("\n7. Testing default preset...")
         default = create_default_preset()
         default.validate()
-        logger.info("   Default preset, default.name)
-        logger.info("   ✓ Default preset OK")
+        print(f"   Default preset: {default.name}")
+        print("   ✓ Default preset OK")
 
-        logger.info("\n" + "=" * 60)
-        logger.info("Self-Test PASSED ✓")
-        logger.info("=" * 60)
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
         return True
 
     except Exception as e:
-        logger.error("\n✗ Self-Test FAILED, e)
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/preset_store.py b/server/preset_store.py
index 69cae46fe7dc7adafdec473e5a10fa6ebd900ce3..6197a4140b8da16a9c2a25a136a1fea49bf7d9c0 100644
--- a/server/preset_store.py
+++ b/server/preset_store.py
@@ -1,333 +1,565 @@
 """
 PresetStore - Filesystem-backed Preset Management
 
-Implements FR-001, FR-002, FR-005, FR-007, FR-009, Optional, Dict, Tuple
+Implements FR-001, FR-002, FR-005, FR-007, FR-009:
+- CRUD operations on ./presets/
+- Search and filtering
+- Collision resolution
+- Audit logging
+- Import/Export with dry-run validation
+"""
+
+import os
+import json
+import shutil
+from pathlib import Path
+from typing import List, Optional, Dict, Tuple
 from datetime import datetime
 import logging
 
 from .preset_model import Preset, CollisionPolicy, create_default_preset
 
 
 class PresetStore:
     """
     Filesystem-backed preset storage and management
 
-    Features, Read, Update, Delete)
+    Features:
+    - CRUD operations (Create, Read, Update, Delete)
     - Search by name/tags
     - Collision resolution strategies
     - Audit logging
     - Import/Export with validation
     """
 
     def __init__(self,
-                 presets_dir,
-                 log_dir):
+                 presets_dir: Optional[str] = None,
+                 log_dir: Optional[str] = None):
         """
         Initialize preset store
 
         Args:
-            presets_dir)
-            log_dir)
+            presets_dir: Directory for preset files (None = ./presets/)
+            log_dir: Directory for audit logs (None = ./logs/presets/)
         """
         # Setup directories
-        if presets_dir is None), "..", "presets")
-        if log_dir is None), "..", "logs", "presets")
+        if presets_dir is None:
+            presets_dir = os.path.join(os.path.dirname(__file__), "..", "presets")
+        if log_dir is None:
+            log_dir = os.path.join(os.path.dirname(__file__), "..", "logs", "presets")
 
         self.presets_dir = Path(presets_dir)
         self.log_dir = Path(log_dir)
 
         # Create directories if they don't exist
         self.presets_dir.mkdir(parents=True, exist_ok=True)
         self.log_dir.mkdir(parents=True, exist_ok=True)
 
         # Audit logging
         self.audit_log = None
         self._init_audit_log()
 
         # Statistics
         self.stats = {
-            'created',
-            'updated',
-            'deleted',
-            'loaded',
-            'errors')
-        logger.info("[PresetStore] Presets, self.presets_dir)
-        logger.info("[PresetStore] Logs, self.log_dir)
-
-    def _init_audit_log(self) :
-            logger.warning("[PresetStore] WARNING: Could not init audit log, e)
+            'created': 0,
+            'updated': 0,
+            'deleted': 0,
+            'loaded': 0,
+            'errors': 0
+        }
+
+        print(f"[PresetStore] Initialized")
+        print(f"[PresetStore] Presets: {self.presets_dir}")
+        print(f"[PresetStore] Logs: {self.log_dir}")
+
+    def _init_audit_log(self):
+        """Initialize audit log file"""
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        log_path = self.log_dir / f"audit_{timestamp}.log"
+
+        try:
+            self.audit_log = open(log_path, 'a')
+            self._log_audit("SYSTEM", "INIT", "PresetStore initialized")
+        except Exception as e:
+            print(f"[PresetStore] WARNING: Could not init audit log: {e}")
             self.audit_log = None
 
-    def _log_audit(self, action: str, target: str, result: str, details: str) :
-            action, LOAD, DELETE, etc.)
+    def _log_audit(self, action: str, target: str, result: str, details: str = ""):
+        """
+        Log audit entry (FR-007)
+
+        Args:
+            action: Action performed (SAVE, LOAD, DELETE, etc.)
             target: Target preset ID or name
             result: SUCCESS or FAILURE
             details: Additional details
         """
-        if self.audit_log and not self.audit_log.closed).isoformat()
+        if self.audit_log and not self.audit_log.closed:
+            timestamp = datetime.now().isoformat()
             entry = f"{timestamp} | {action:10} | {target:30} | {result:10} | {details}\n"
-            try)
+            try:
+                self.audit_log.write(entry)
                 self.audit_log.flush()
-            except, preset_id) :
+            except:
+                pass
+
+    def _get_preset_path(self, preset_id: str) -> Path:
+        """Get filepath for preset ID"""
+        # Sanitize ID to prevent directory traversal
+        safe_id = "".join(c for c in preset_id if c.isalnum() or c in ('-', '_'))
+        return self.presets_dir / f"{safe_id}.json"
+
+    def create(self, preset: Preset, collision: CollisionPolicy = "prompt") -> Tuple[str, bool]:
         """
-        Create/save a new preset (FR-002)
+        Create/save a new preset (FR-002: POST /api/presets)
 
         Args:
             preset: Preset to save
             collision: Collision resolution strategy
 
-        Returns, was_created)
+        Returns:
+            Tuple of (preset_id, was_created)
                 was_created = False if existing preset was updated
 
         Raises:
             FileExistsError: If collision="prompt" and preset exists
             ValueError: If preset validation fails
         """
         # Validate preset
-        try)
-        except ValueError as e, preset.name, "FAILURE", f"Validation error)
+        try:
+            preset.validate()
+        except ValueError as e:
+            self._log_audit("CREATE", preset.name, "FAILURE", f"Validation error: {e}")
             self.stats['errors'] += 1
             raise
 
         preset_path = self._get_preset_path(preset.id)
         existed = preset_path.exists()
 
         # Handle collision
         if existed:
-            if collision == "prompt", preset.name, "FAILURE", "Preset exists, prompt required")
+            if collision == "prompt":
+                self._log_audit("CREATE", preset.name, "FAILURE", "Preset exists, prompt required")
                 raise FileExistsError(f"Preset {preset.id} already exists")
 
-            elif collision == "new_copy")
+            elif collision == "new_copy":
+                # Create new ID for copy
+                preset = preset.clone()
                 preset.name = f"{preset.name} (Copy)"
                 preset_path = self._get_preset_path(preset.id)
                 existed = False
 
-            elif collision == "merge")
+            elif collision == "merge":
+                # Load existing and merge (keep newer values)
                 existing = self.load(preset.id)
                 if existing:
-                    # Simple merge)
+                    # Simple merge: take non-default values from preset
+                    # (Full merge logic would be more complex)
                     pass
 
             # collision == "overwrite": just continue and overwrite
 
         # Save preset
-        try)
-            with open(preset_path, 'w') as f))
+        try:
+            preset.update_timestamp()
+            with open(preset_path, 'w') as f:
+                f.write(preset.to_json(pretty=True))
 
-            if existed, preset.name, "SUCCESS", f"ID)
+            if existed:
+                self._log_audit("UPDATE", preset.name, "SUCCESS", f"ID: {preset.id}")
                 self.stats['updated'] += 1
-            else, preset.name, "SUCCESS", f"ID)
+            else:
+                self._log_audit("CREATE", preset.name, "SUCCESS", f"ID: {preset.id}")
                 self.stats['created'] += 1
 
             return preset.id, not existed
 
-        except Exception as e, preset.name, "FAILURE", str(e))
+        except Exception as e:
+            self._log_audit("CREATE", preset.name, "FAILURE", str(e))
             self.stats['errors'] += 1
             raise
 
-    def load(self, preset_id) :
+    def load(self, preset_id: str) -> Optional[Preset]:
         """
-        Load preset by ID (FR-002)
+        Load preset by ID (FR-002: GET /api/presets/{id})
 
         Args:
             preset_id: Preset identifier
 
-        if not preset_path.exists(), preset_id, "FAILURE", "Not found")
+        Returns:
+            Preset instance or None if not found
+        """
+        preset_path = self._get_preset_path(preset_id)
+
+        if not preset_path.exists():
+            self._log_audit("LOAD", preset_id, "FAILURE", "Not found")
             return None
 
-        try, 'r') as f)
+        try:
+            with open(preset_path, 'r') as f:
+                data = json.load(f)
 
             preset = Preset.from_dict(data)
 
             # Validate loaded preset
             preset.validate()
 
-            self._log_audit("LOAD", preset.name, "SUCCESS", f"ID)
+            self._log_audit("LOAD", preset.name, "SUCCESS", f"ID: {preset_id}")
             self.stats['loaded'] += 1
 
             return preset
 
-        except json.JSONDecodeError as e, preset_id, "FAILURE", f"JSON error)
+        except json.JSONDecodeError as e:
+            self._log_audit("LOAD", preset_id, "FAILURE", f"JSON error: {e}")
             self.stats['errors'] += 1
             return None
 
-        except ValueError as e, preset_id, "FAILURE", f"Validation error)
+        except ValueError as e:
+            self._log_audit("LOAD", preset_id, "FAILURE", f"Validation error: {e}")
             self.stats['errors'] += 1
             return None
 
-        except Exception as e, preset_id, "FAILURE", str(e))
+        except Exception as e:
+            self._log_audit("LOAD", preset_id, "FAILURE", str(e))
             self.stats['errors'] += 1
             return None
 
-    def update(self, preset) :
+    def update(self, preset: Preset) -> bool:
         """
-        Update existing preset (FR-002)
+        Update existing preset (FR-002: PUT /api/presets/{id})
 
         Args:
             preset: Preset with updated values
 
-        if not preset_path.exists(), preset.id, "FAILURE", "Not found")
+        Returns:
+            True if updated successfully
+        """
+        preset_path = self._get_preset_path(preset.id)
+
+        if not preset_path.exists():
+            self._log_audit("UPDATE", preset.id, "FAILURE", "Not found")
             return False
 
-        try)
+        try:
+            preset.validate()
             preset.update_timestamp()
 
-            with open(preset_path, 'w') as f))
+            with open(preset_path, 'w') as f:
+                f.write(preset.to_json(pretty=True))
 
-            self._log_audit("UPDATE", preset.name, "SUCCESS", f"ID)
+            self._log_audit("UPDATE", preset.name, "SUCCESS", f"ID: {preset.id}")
             self.stats['updated'] += 1
             return True
 
-        except Exception as e, preset.name, "FAILURE", str(e))
+        except Exception as e:
+            self._log_audit("UPDATE", preset.name, "FAILURE", str(e))
             self.stats['errors'] += 1
             return False
 
-    def delete(self, preset_id) :
+    def delete(self, preset_id: str) -> bool:
         """
-        Delete preset by ID (FR-002)
+        Delete preset by ID (FR-002: DELETE /api/presets/{id})
 
         Args:
             preset_id: Preset identifier
 
-        if not preset_path.exists(), preset_id, "FAILURE", "Not found")
+        Returns:
+            True if deleted successfully
+        """
+        preset_path = self._get_preset_path(preset_id)
+
+        if not preset_path.exists():
+            self._log_audit("DELETE", preset_id, "FAILURE", "Not found")
             return False
 
-        try)
+        try:
+            # Load name before deleting for audit log
+            preset = self.load(preset_id)
             name = preset.name if preset else preset_id
 
             preset_path.unlink()
 
-            self._log_audit("DELETE", name, "SUCCESS", f"ID)
+            self._log_audit("DELETE", name, "SUCCESS", f"ID: {preset_id}")
             self.stats['deleted'] += 1
             return True
 
-        except Exception as e, preset_id, "FAILURE", str(e))
+        except Exception as e:
+            self._log_audit("DELETE", preset_id, "FAILURE", str(e))
             self.stats['errors'] += 1
             return False
 
     def list(self,
-             query,
-             tag,
-             limit) :
+             query: Optional[str] = None,
+             tag: Optional[str] = None,
+             limit: int = 50) -> List[Dict]:
         """
-        List presets with optional filtering (FR-002)
+        List presets with optional filtering (FR-002: GET /api/presets)
 
         Args:
-            query)
+            query: Search query (matches name or notes)
             tag: Filter by tag
             limit: Maximum results
 
-        Returns):
-            try, 'r') as f)
+        Returns:
+            List of preset metadata dictionaries
+        """
+        results = []
+
+        for preset_path in self.presets_dir.glob("*.json"):
+            try:
+                with open(preset_path, 'r') as f:
+                    data = json.load(f)
 
                 # Apply filters
-                if query) in data.get('name', '').lower()
+                if query:
+                    name_match = query.lower() in data.get('name', '').lower()
                     notes_match = query.lower() in data.get('notes', '').lower()
                     if not (name_match or notes_match):
                         continue
 
-                if tag, []):
+                if tag:
+                    if tag not in data.get('tags', []):
                         continue
 
                 # Extract metadata
                 metadata = {
-                    'id'),
-                    'name'),
-                    'tags', []),
-                    'modified_at'),
-                    'author'),
-                    'notes', '')[)
+                    'id': data.get('id'),
+                    'name': data.get('name'),
+                    'tags': data.get('tags', []),
+                    'modified_at': data.get('modified_at'),
+                    'author': data.get('author'),
+                    'notes': data.get('notes', '')[:100]  # Truncate notes
+                }
+
+                results.append(metadata)
 
                 if len(results) >= limit:
                     break
 
-            except)
-        results.sort(key=lambda x, reverse=True)
+            except:
+                continue  # Skip corrupted files
+
+        # Sort by modified_at (most recent first)
+        results.sort(key=lambda x: x['modified_at'], reverse=True)
 
         return results
 
-    def export_all(self) :
+    def export_all(self) -> Dict:
+        """
+        Export all presets as a single bundle (FR-009)
+
+        Returns:
             Dictionary with schema version and presets array
         """
         bundle = {
-            "schema_version",
-            "export_date").isoformat() + 'Z',
-            "presets"):
-            try, 'r') as f)
+            "schema_version": 1,
+            "export_date": datetime.utcnow().isoformat() + 'Z',
+            "presets": []
+        }
+
+        for preset_path in self.presets_dir.glob("*.json"):
+            try:
+                with open(preset_path, 'r') as f:
+                    data = json.load(f)
                 bundle["presets"].append(data)
-            except, "ALL", "SUCCESS", f"{len(bundle['presets'])} presets")
+            except:
+                continue
+
+        self._log_audit("EXPORT", "ALL", "SUCCESS", f"{len(bundle['presets'])} presets")
 
         return bundle
 
     def import_bundle(self,
-                     bundle,
-                     collision,
-                     dry_run) :
+                     bundle: Dict,
+                     collision: CollisionPolicy = "prompt",
+                     dry_run: bool = False) -> Dict:
+        """
+        Import preset bundle (FR-009)
+
+        Args:
             bundle: Export bundle dictionary
             collision: Collision resolution strategy
-            dry_run, validate only, don't save
+            dry_run: If True, validate only, don't save
 
         Returns:
             Dictionary with import results:
                 {
-                    'imported',
-                    'updated',
-                    'skipped',
+                    'imported': int,
+                    'updated': int,
+                    'skipped': int,
                     'errors': List[str]
                 }
         """
         results = {
-            'imported',
-            'updated',
-            'skipped',
+            'imported': 0,
+            'updated': 0,
+            'skipped': 0,
             'errors': []
         }
 
         # Validate bundle schema
         if 'presets' not in bundle:
-            results['errors'].append("Invalid bundle)
+            results['errors'].append("Invalid bundle: missing 'presets' array")
             return results
 
         for preset_data in bundle['presets']:
-            try)
+            try:
+                # Create Preset from data
+                preset = Preset.from_dict(preset_data)
 
                 # Validate
                 preset.validate()
 
-                if dry_run, was_created = self.create(preset, collision=collision)
+                if dry_run:
+                    results['imported'] += 1
+                    continue
+
+                # Save with collision handling
+                preset_id, was_created = self.create(preset, collision=collision)
 
                 if was_created:
                     results['imported'] += 1
                 else:
                     results['updated'] += 1
 
             except FileExistsError:
                 results['skipped'] += 1
-            except ValueError as e)}')
-            except Exception as e)}')
+            except ValueError as e:
+                results['errors'].append(f"Preset '{preset_data.get('name')}': {e}")
+            except Exception as e:
+                results['errors'].append(f"Preset '{preset_data.get('name')}': {e}")
 
         mode = "DRY_RUN" if dry_run else "IMPORT"
         self._log_audit(mode, "BUNDLE", "SUCCESS",
                        f"imported={results['imported']}, updated={results['updated']}, errors={len(results['errors'])}")
 
         return results
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
+        """Get store statistics"""
+        total_presets = len(list(self.presets_dir.glob("*.json")))
+
+        return {
+            **self.stats,
+            'total_presets': total_presets
+        }
+
+    def close(self):
         """Close audit log and cleanup"""
-        if self.audit_log and not self.audit_log.closed, "SHUTDOWN", "PresetStore closing")
+        if self.audit_log and not self.audit_log.closed:
+            self._log_audit("SYSTEM", "SHUTDOWN", "PresetStore closing")
             self.audit_log.close()
 
-        logger.info("[PresetStore] Closed")
+        print("[PresetStore] Closed")
 
-    def __del__(self))
+    def __del__(self):
+        """Ensure cleanup"""
+        self.close()
 
 
 # Self-test function
-def _self_test() :
-        logger.error("\n✗ Self-Test FAILED, e)
+def _self_test():
+    """Test PresetStore functionality"""
+    print("=" * 60)
+    print("PresetStore Self-Test")
+    print("=" * 60)
+
+    try:
+        import tempfile
+        from preset_model import Preset
+
+        # Create temporary directory for testing
+        temp_dir = tempfile.mkdtemp()
+        print(f"\n1. Using temp directory: {temp_dir}")
+
+        # Initialize store
+        print("\n2. Initializing store...")
+        store = PresetStore(presets_dir=temp_dir)
+        print("   ✓ Store initialized")
+
+        # Create test preset
+        print("\n3. Creating preset...")
+        preset = Preset(
+            name="Test Preset",
+            tags=["test", "lab"],
+            notes="Test configuration"
+        )
+        preset_id, was_created = store.create(preset, collision="overwrite")
+        print(f"   ✓ Created: {preset_id[:8]}... (was_created={was_created})")
+
+        # Load preset
+        print("\n4. Loading preset...")
+        loaded = store.load(preset_id)
+        assert loaded is not None
+        assert loaded.name == preset.name
+        print(f"   ✓ Loaded: {loaded.name}")
+
+        # List presets
+        print("\n5. Listing presets...")
+        presets = store.list()
+        print(f"   Found {len(presets)} presets")
+        assert len(presets) == 1
+        print("   ✓ List OK")
+
+        # Update preset
+        print("\n6. Updating preset...")
+        loaded.name = "Modified Preset"
+        success = store.update(loaded)
+        assert success
+        print("   ✓ Update OK")
+
+        # Search
+        print("\n7. Searching...")
+        results = store.list(query="modified")
+        print(f"   Found {len(results)} matches")
+        assert len(results) == 1
+        print("   ✓ Search OK")
+
+        # Export all
+        print("\n8. Exporting all...")
+        bundle = store.export_all()
+        print(f"   Exported {len(bundle['presets'])} presets")
+        assert len(bundle['presets']) == 1
+        print("   ✓ Export OK")
+
+        # Delete preset
+        print("\n9. Deleting preset...")
+        success = store.delete(preset_id)
+        assert success
+        presets_after = store.list()
+        assert len(presets_after) == 0
+        print("   ✓ Delete OK")
+
+        # Import bundle
+        print("\n10. Importing bundle...")
+        results = store.import_bundle(bundle, collision="overwrite")
+        print(f"   Imported: {results['imported']}, Errors: {len(results['errors'])}")
+        assert results['imported'] == 1
+        print("   ✓ Import OK")
+
+        # Statistics
+        print("\n11. Getting statistics...")
+        stats = store.get_statistics()
+        print(f"   Stats: {stats}")
+        print("   ✓ Statistics OK")
+
+        # Cleanup
+        print("\n12. Cleanup...")
+        store.close()
+        shutil.rmtree(temp_dir)
+        print("   ✓ Cleanup complete")
+
+        print("\n" + "=" * 60)
+        print("Self-Test PASSED ✓")
+        print("=" * 60)
+        return True
+
+    except Exception as e:
+        print(f"\n✗ Self-Test FAILED: {e}")
         import traceback
         traceback.print_exc()
         return False
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/privacy_manager.py b/server/privacy_manager.py
index 1cd21eb1dacb99e2e1e14f2347b30c0ab1132918..5d62725fce5f921e0513d9efe6451e170e78a53a 100644
--- a/server/privacy_manager.py
+++ b/server/privacy_manager.py
@@ -1,262 +1,378 @@
 """
 Privacy Manager - Feature 024 (FR-009, FR-010, FR-011)
 
 Handles data privacy, PII redaction, retention policies, and data purging.
 
 Requirements:
-- FR-009, no raw payloads
+- FR-009: Structured logging with PII redaction, no raw payloads
 - FR-010: Retention policy enforcement and purge CLI
-- FR-011, no secrets in localStorage
+- FR-011: Storage minimization, no secrets in localStorage
 """
 
 import re
 import json
 import logging
 import hashlib
 from pathlib import Path
 from datetime import datetime, timedelta
 from typing import Dict, List, Optional, Any
 import glob
 
 logger = logging.getLogger(__name__)
 
 
-class PIIRedactor)"""
+class PIIRedactor:
+    """PII redaction for logs and data (FR-009)"""
 
     # Regex patterns for PII detection
     EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
     PHONE_PATTERN = re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b')
     SSN_PATTERN = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
     CREDIT_CARD_PATTERN = re.compile(r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b')
     IP_ADDRESS_PATTERN = re.compile(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b')
 
     # Fields to always redact
     SENSITIVE_FIELDS = {
         'password', 'token', 'secret', 'api_key', 'authorization',
         'credit_card', 'ssn', 'social_security', 'passport'
     }
 
     @staticmethod
-    def redact_text(text, preserve_domain) :
+    def redact_text(text: str, preserve_domain: bool = False) -> str:
         """
         Redact PII from text.
 
         Args:
             text: Input text
-            preserve_domain, keep email domains
+            preserve_domain: If True, keep email domains
 
         Returns:
             Redacted text
         """
         if not text:
             return text
 
         # Redact emails
         if preserve_domain:
             text = PIIRedactor.EMAIL_PATTERN.sub(
-                lambda m).split('@')[1]}",
+                lambda m: f"***@{m.group().split('@')[1]}",
                 text
-
-        else, text)
+            )
+        else:
+            text = PIIRedactor.EMAIL_PATTERN.sub('[EMAIL]', text)
 
         # Redact phone numbers
         text = PIIRedactor.PHONE_PATTERN.sub('[PHONE]', text)
 
         # Redact SSN
         text = PIIRedactor.SSN_PATTERN.sub('[SSN]', text)
 
         # Redact credit cards
         text = PIIRedactor.CREDIT_CARD_PATTERN.sub('[CARD]', text)
 
         # Redact IP addresses (partial)
         text = PIIRedactor.IP_ADDRESS_PATTERN.sub(
-            lambda m).split('.')[, '***']),
+            lambda m: '.'.join(m.group().split('.')[:2] + ['***', '***']),
             text
+        )
 
         return text
 
     @staticmethod
-    def redact_dict(data, hash_fields) :
+    def redact_dict(data: Dict, hash_fields: bool = False) -> Dict:
         """
         Redact PII from dictionary.
 
         Args:
             data: Input dictionary
-            hash_fields, hash sensitive values instead of redacting
+            hash_fields: If True, hash sensitive values instead of redacting
+
+        Returns:
+            Redacted dictionary
+        """
+        if not isinstance(data, dict):
+            return data
+
+        redacted = {}
 
-        Returns, dict), value in data.items())
+        for key, value in data.items():
+            key_lower = key.lower()
 
             # Check if field is sensitive
             is_sensitive = any(
                 sensitive in key_lower
                 for sensitive in PIIRedactor.SENSITIVE_FIELDS
-
-            if is_sensitive, str))).hexdigest()[:16]
-                else, str))
-            elif isinstance(value, dict), hash_fields)
-            elif isinstance(value, list), hash_fields)
+            )
+
+            if is_sensitive:
+                if hash_fields and isinstance(value, str):
+                    # Hash instead of redact
+                    redacted[key] = hashlib.sha256(value.encode()).hexdigest()[:16]
+                else:
+                    redacted[key] = '[REDACTED]'
+            elif isinstance(value, str):
+                redacted[key] = PIIRedactor.redact_text(value)
+            elif isinstance(value, dict):
+                redacted[key] = PIIRedactor.redact_dict(value, hash_fields)
+            elif isinstance(value, list):
+                redacted[key] = [
+                    PIIRedactor.redact_dict(item, hash_fields)
                     if isinstance(item, dict)
                     else PIIRedactor.redact_text(str(item))
                     if isinstance(item, str)
                     else item
                     for item in value
                 ]
             else:
                 redacted[key] = value
 
         return redacted
 
     @staticmethod
-    def should_log_payload(endpoint, method) :
+    def should_log_payload(endpoint: str, method: str) -> bool:
         """
         Determine if request/response payload should be logged.
 
         Args:
             endpoint: API endpoint
             method: HTTP method
 
-        Returns, False otherwise
+        Returns:
+            True if payload should be logged, False otherwise
         """
         # Never log auth payloads
-        if 'auth' in endpoint.lower() or 'login' in endpoint.lower()) for x in ['audio', 'upload', 'stream']):
+        if 'auth' in endpoint.lower() or 'login' in endpoint.lower():
+            return False
+
+        # Never log raw audio/binary data
+        if any(x in endpoint.lower() for x in ['audio', 'upload', 'stream']):
             return False
 
         return True
 
 
-class RetentionPolicy)"""
+class RetentionPolicy:
+    """Data retention policy enforcement (FR-010)"""
 
-    def __init__(self, config_path: str) :
+    def __init__(self, config_path: str = 'config/privacy.json') -> None:
+        self.config_path = Path(config_path)
+        self.config = self._load_config()
+
+    def _load_config(self) -> Dict:
+        """Load retention policy configuration"""
+        if not self.config_path.exists():
             # Default policy
             return {
                 'retention_periods': {
-                    'logs',  # days
-                    'session_metrics',
-                    'audit_logs',
-                    'temp_files',
-                    'recordings',
-                'auto_purge_enabled',
-                'purge_schedule') as f)
-
-    def get_retention_period(self, data_type) : str) :
+                    'logs': 30,  # days
+                    'session_metrics': 90,
+                    'audit_logs': 365,
+                    'temp_files': 7,
+                    'recordings': 30
+                },
+                'auto_purge_enabled': True,
+                'purge_schedule': 'daily'
+            }
+
+        with open(self.config_path) as f:
+            return json.load(f)
+
+    def get_retention_period(self, data_type: str) -> int:
+        """Get retention period in days for data type"""
+        return self.config['retention_periods'].get(data_type, 30)
+
+    def is_expired(self, file_path: Path, data_type: str) -> bool:
+        """Check if file has exceeded retention period"""
+        retention_days = self.get_retention_period(data_type)
+
+        if not file_path.exists():
+            return False
+
+        file_age_days = (
+            datetime.now() - datetime.fromtimestamp(file_path.stat().st_mtime)
+        ).days
+
+        return file_age_days > retention_days
+
+    def find_expired_files(self, data_type: str, search_pattern: str) -> List[Path]:
+        """Find all expired files matching pattern"""
+        expired = []
+
+        for file_path in glob.glob(search_pattern, recursive=True):
+            path = Path(file_path)
+            if self.is_expired(path, data_type):
+                expired.append(path)
+
+        return expired
+
+
+class PrivacyManager:
+    """Privacy management and data purging (FR-010, FR-011)"""
+
+    def __init__(self, config_path: str = 'config/privacy.json') -> None:
+        self.retention = RetentionPolicy(config_path)
+        self.redactor = PIIRedactor()
+
+    def redact_log_entry(self, entry: Dict) -> Dict:
+        """Redact PII from log entry"""
+        return self.redactor.redact_dict(entry, hash_fields=True)
+
+    def should_log_payload(self, endpoint: str, method: str) -> bool:
+        """Check if payload should be logged"""
+        return self.redactor.should_log_payload(endpoint, method)
+
+    def purge_expired_data(
+        self,
+        data_type: str,
+        search_pattern: str,
+        dry_run: bool = False
+    ) -> Dict[str, Any]:
         """
         Purge expired data files.
 
         Args:
-            data_type, session_metrics, etc.)
+            data_type: Type of data (logs, session_metrics, etc.)
             search_pattern: Glob pattern to find files
-            dry_run, only report what would be deleted
+            dry_run: If True, only report what would be deleted
 
-        Returns, search_pattern)
+        Returns:
+            Purge statistics
+        """
+        expired_files = self.retention.find_expired_files(data_type, search_pattern)
 
         stats = {
-            'data_type',
-            'files_found'),
-            'files_deleted',
-            'bytes_freed',
-            'dry_run',
+            'data_type': data_type,
+            'files_found': len(expired_files),
+            'files_deleted': 0,
+            'bytes_freed': 0,
+            'dry_run': dry_run,
             'deleted_files': []
         }
 
         for file_path in expired_files:
-            try).st_size
+            try:
+                file_size = file_path.stat().st_size
                 stats['bytes_freed'] += file_size
 
-                if not dry_run)
+                if not dry_run:
+                    file_path.unlink()
                     stats['files_deleted'] += 1
 
                 stats['deleted_files'].append({
-                    'path'),
-                    'size',
-                    'age_days') - datetime.fromtimestamp(file_path.stat().st_mtime)
+                    'path': str(file_path),
+                    'size': file_size,
+                    'age_days': (
+                        datetime.now() - datetime.fromtimestamp(file_path.stat().st_mtime)
                     ).days if file_path.exists() else 0
                 })
 
             except Exception as e:
-                logger.error(f"Error purging {file_path})
+                logger.error(f"Error purging {file_path}: {e}")
 
         return stats
 
-    def purge_all_expired(self, dry_run) :
+    def purge_all_expired(self, dry_run: bool = False) -> Dict[str, Any]:
         """
         Purge all expired data based on retention policy.
 
         Args:
-            dry_run, only report what would be deleted
+            dry_run: If True, only report what would be deleted
 
-        Returns, 'logs/**/*.log'),
+        Returns:
+            Combined purge statistics
+        """
+        purge_targets = [
+            ('logs', 'logs/**/*.log'),
             ('session_metrics', 'logs/**/*_metrics.csv'),
             ('temp_files', 'temp/**/*'),
             ('recordings', 'recordings/**/*.wav')
         ]
 
         results = []
         total_files = 0
         total_bytes = 0
 
-        for data_type, pattern in purge_targets, pattern, dry_run)
+        for data_type, pattern in purge_targets:
+            stats = self.purge_expired_data(data_type, pattern, dry_run)
             results.append(stats)
             total_files += stats['files_deleted']
             total_bytes += stats['bytes_freed']
 
         return {
-            'timestamp').isoformat(),
-            'dry_run',
-            'total_files_deleted',
-            'total_bytes_freed',
-            'results', purge_stats) :
+            'timestamp': datetime.now().isoformat(),
+            'dry_run': dry_run,
+            'total_files_deleted': total_files,
+            'total_bytes_freed': total_bytes,
+            'results': results
+        }
+
+    def audit_log_purge(self, purge_stats: Dict) -> None:
         """Create audit log entry for purge action"""
         audit_entry = {
-            'timestamp').isoformat(),
-            'action',
-            'dry_run', False),
-            'files_deleted', 0),
-            'bytes_freed', 0),
-            'details', [])
+            'timestamp': datetime.now().isoformat(),
+            'action': 'data_purge',
+            'dry_run': purge_stats.get('dry_run', False),
+            'files_deleted': purge_stats.get('total_files_deleted', 0),
+            'bytes_freed': purge_stats.get('total_bytes_freed', 0),
+            'details': purge_stats.get('results', [])
         }
 
         # Write to audit log
         audit_log_path = Path('logs/audit.log')
         audit_log_path.parent.mkdir(parents=True, exist_ok=True)
 
-        with open(audit_log_path, 'a') as f) + '\n')
+        with open(audit_log_path, 'a') as f:
+            f.write(json.dumps(audit_entry) + '\n')
+
+        logger.info(f"Purge audit logged: {audit_entry}")
 
-        logger.info(f"Purge audit logged)
+    def validate_frontend_storage(self, storage_data: Dict) -> Dict:
+        """
+        Validate frontend localStorage data (FR-011).
 
-    def validate_frontend_storage(self, storage_data) :
+        Args:
             storage_data: Frontend localStorage contents
 
-        Returns, value in storage_data.items())
+        Returns:
+            Validation report with warnings
+        """
+        warnings = []
+        secrets_found = []
 
-            if any(s in key_lower for s in ['token', 'secret', 'password', 'key']))
-                warnings.append(f"Secret found in localStorage)
+        # Check for secrets
+        for key, value in storage_data.items():
+            key_lower = key.lower()
+
+            if any(s in key_lower for s in ['token', 'secret', 'password', 'key']):
+                secrets_found.append(key)
+                warnings.append(f"Secret found in localStorage: {key}")
 
         # Check data size
         total_size = sum(len(str(v)) for v in storage_data.values())
         if total_size > 5 * 1024 * 1024:  # 5MB
-            warnings.append(f"localStorage exceeds 5MB)
+            warnings.append(f"localStorage exceeds 5MB: {total_size} bytes")
 
         return {
-            'valid') == 0,
-            'warnings',
-            'secrets_found',
-            'total_size_bytes',
-            'recommendations',
+            'valid': len(secrets_found) == 0,
+            'warnings': warnings,
+            'secrets_found': secrets_found,
+            'total_size_bytes': total_size,
+            'recommendations': [
+                'Only store preset names and non-sensitive UI state',
                 'Use session storage for temporary data',
                 'Never store tokens or secrets in localStorage'
             ]
         }
 
 
-def create_audit_log_entry(action, user, details) :
+def create_audit_log_entry(action: str, user: str, details: Dict) -> Dict:
     """Create structured audit log entry"""
     return {
-        'timestamp').isoformat(),
-        'action',
-        'user',
-        'details',
-        'correlation_id').isoformat()}".encode()
+        'timestamp': datetime.now().isoformat(),
+        'action': action,
+        'user': user,
+        'details': details,
+        'correlation_id': hashlib.sha256(
+            f"{action}{user}{datetime.now().isoformat()}".encode()
         ).hexdigest()[:16]
     }
-
-"""  # auto-closed missing docstring
diff --git a/server/quick_test_feature_011.py b/server/quick_test_feature_011.py
index 91cab3e3472118f6c06669b3b18e09337b2e8356..baae3bffb01f7e301db1285ef35b8cd41ba94e30 100644
--- a/server/quick_test_feature_011.py
+++ b/server/quick_test_feature_011.py
@@ -1,91 +1,125 @@
 """
 Quick functionality test for Feature 011 - no delays
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
 
-
-from typing import Any, Dict, List, Optional, Tuple
 import time
 import numpy as np
 from .phi_sensor_bridge import SensorData, SensorType, AudioBeatDetector, SensorConfig
 from .phi_router import PhiRouter, PhiRouterConfig, PhiSourcePriority
 
-logger.info("=" * 60)
-logger.info("Feature 011)
-logger.info("=" * 60)
+print("=" * 60)
+print("Feature 011: Quick Functionality Test")
+print("=" * 60)
 
 PHI_MIN = 0.618033988749895
 PHI_MAX = 1.618033988749895
 
-# Test 1)...")
+# Test 1: Normalization
+print("\n1. Testing Phi normalization (FR-002)...")
 test_cases = [(0, 0.618), (64, 1.118), (127, 1.618)]
 all_ok = True
-for midi_val, expected in test_cases) * (PHI_MAX - PHI_MIN)
+for midi_val, expected in test_cases:
+    normalized = PHI_MIN + (midi_val / 127.0) * (PHI_MAX - PHI_MIN)
     ok = abs(normalized - expected) < 0.001
     all_ok = all_ok and ok
-    logger.error("   MIDI %s :
-    logger.error("   [FAIL] PhiRouter creation failed, e)
+    print(f"   MIDI {midi_val:3d} -> {normalized:.3f} (expected {expected:.3f}) {'[OK]' if ok else '[FAIL]'}")
+
+# Test 2: PhiRouter creation
+print("\n2. Testing PhiRouter creation...")
+try:
+    router = PhiRouter(PhiRouterConfig(enable_logging=False))
+    router.start()
+    print("   [OK] PhiRouter created and started")
+except Exception as e:
+    print(f"   [FAIL] PhiRouter creation failed: {e}")
     all_ok = False
 
-# Test 3)...")
-try, PhiSourcePriority.MIDI)
+# Test 3: Source registration
+print("\n3. Testing source registration (FR-001)...")
+try:
+    router.register_source("midi", PhiSourcePriority.MIDI)
     router.register_source("serial", PhiSourcePriority.SERIAL)
     status = router.get_status()
     ok = status.source_count == 2
     all_ok = all_ok and ok
-    logger.error("   Sources registered, status.source_count, '[OK]' if ok else '[FAIL]')
+    print(f"   Sources registered: {status.source_count} {'[OK]' if ok else '[FAIL]'}")
 except Exception as e:
-    logger.error("   [FAIL] Source registration failed, e)
+    print(f"   [FAIL] Source registration failed: {e}")
     all_ok = False
 
-# Test 4)
-try, time.time(), 64, 1.0, "midi")
+# Test 4: Source updates
+print("\n4. Testing source updates...")
+try:
+    data = SensorData(SensorType.MIDI_CC, time.time(), 64, 1.0, "midi")
     router.update_source("midi", data)
     phi, phase = router.get_current_phi()
     ok = PHI_MIN <= phi <= PHI_MAX
     all_ok = all_ok and ok
-    logger.error("   Phi value, phi, '[OK]' if ok else '[FAIL - out of range]')
+    print(f"   Phi value: {phi:.3f} {'[OK]' if ok else '[FAIL - out of range]'}")
 except Exception as e:
-    logger.error("   [FAIL] Source update failed, e)
+    print(f"   [FAIL] Source update failed: {e}")
     all_ok = False
 
-# Test 5)
-try)
-    def beat_cb(data) :
+# Test 5: Audio beat detector
+print("\n5. Testing audio beat detector...")
+try:
+    beats = []
+    def beat_cb(data):
+        beats.append(data)
+
+    config = SensorConfig(sensor_type=SensorType.AUDIO_BEAT)
+    detector = AudioBeatDetector(config, beat_cb)
+
+    # Quiet baseline
+    for _ in range(3):
+        quiet = np.random.randn(512).astype(np.float32) * 0.1
+        detector.process_audio(quiet)
+
+    # Loud beat
+    loud = np.random.randn(512).astype(np.float32) * 3.0
+    detector.process_audio(loud)
+
+    ok = len(beats) > 0
+    all_ok = all_ok and ok
+    print(f"   Beats detected: {len(beats)} {'[OK]' if ok else '[FAIL]'}")
+
+    if beats:
         phi_ok = PHI_MIN <= beats[0].normalized_value <= PHI_MAX
         all_ok = all_ok and phi_ok
-        logger.error("   Beat Phi, beats[0].normalized_value, '[OK]' if phi_ok else '[FAIL]')
+        print(f"   Beat Phi: {beats[0].normalized_value:.3f} {'[OK]' if phi_ok else '[FAIL]'}")
 except Exception as e:
-    logger.error("   [FAIL] Audio beat detector failed, e)
+    print(f"   [FAIL] Audio beat detector failed: {e}")
     all_ok = False
 
-# Test 6)...")
-try)
+# Test 6: Telemetry
+print("\n6. Testing telemetry (FR-004)...")
+try:
+    status = router.get_status()
     checks = [
         ("Timestamp", status.timestamp > 0),
         ("Active source", status.active_source is not None),
         ("Phi value", PHI_MIN <= status.phi_value <= PHI_MAX),
         ("Source count", status.source_count > 0)
     ]
     for name, ok in checks:
         all_ok = all_ok and ok
-        logger.error("   %s, name, '[OK]' if ok else '[FAIL]')
+        print(f"   {name}: {'[OK]' if ok else '[FAIL]'}")
 except Exception as e:
-    logger.error("   [FAIL] Telemetry failed, e)
+    print(f"   [FAIL] Telemetry failed: {e}")
     all_ok = False
 
 # Cleanup
 router.stop()
 
 # Summary
-logger.info("\n" + "=" * 60)
-if all_ok)
-    logger.info("\nFeature 011 components validated)
-    logger.info("  - Phi normalization (FR-002)")
-    logger.info("  - PhiRouter source management (FR-001)")
-    logger.info("  - Audio beat detection")
-    logger.info("  - Telemetry (FR-004)")
-else)
-logger.info("=" * 60)
+print("\n" + "=" * 60)
+if all_ok:
+    print("[PASS] All core functionality tests passed")
+    print("\nFeature 011 components validated:")
+    print("  - Phi normalization (FR-002)")
+    print("  - PhiRouter source management (FR-001)")
+    print("  - Audio beat detection")
+    print("  - Telemetry (FR-004)")
+else:
+    print("[FAIL] Some tests failed - see details above")
+print("=" * 60)
diff --git a/server/security_middleware.py b/server/security_middleware.py
index a579b0b387801a72fe00f65818b59e66187c2b62..a26a24a4959c3985bde0f78662c7822e7917f193 100644
--- a/server/security_middleware.py
+++ b/server/security_middleware.py
@@ -1,194 +1,383 @@
-"""
-Security Middleware - Feature 024 (FR-001, FR-003, FR-004, FR-005, FR-006, FR-007)
+"""Security Middleware - Feature 024 (FR-001, FR-003, FR-004, FR-005, FR-006, FR-007)
 
 Provides JWT authentication, security headers, CSRF protection, and rate limiting
 for REST and WebSocket endpoints.
 
 Requirements:
-- FR-001) auth with audience, exp ≤15m, nonce replay guard
-- FR-003, secure cookies, HSTS
+- FR-001: JWT (RS256/EdDSA) auth with audience, exp ≤15m, nonce replay guard
+- FR-003: TLS enforcement, secure cookies, HSTS
 - FR-004: CSP headers
 - FR-005: Additional security headers
 - FR-006: CSRF protection
-
+- FR-007: Rate limiting (token + IP buckets)
 """
 
 import jwt
 import time
 import hashlib
 import secrets
 from datetime import datetime, timedelta
-from typing import Optional, Dict, Set, Tuple
+from typing import Any, Dict, Optional, Set, Tuple
 from functools import wraps
 from fastapi import Request, HTTPException, status
 from fastapi.responses import JSONResponse
 from starlette.middleware.base import BaseHTTPMiddleware
 import logging
 
 logger = logging.getLogger(__name__)
 
 # Nonce replay protection (in-memory cache, use Redis in production)
-_nonce_cache)
+_nonce_cache: Set[str] = set()
 _nonce_cache_max_size = 10000
 
 
-class TokenValidator)"""
+class TokenValidator:
+    """JWT token validation (FR-001)"""
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        self.config = config
+        self.algorithm = config.get('jwt_algorithm', 'RS256')
+        self.public_key = config.get('jwt_public_key')
+        self.audience = config.get('jwt_audience', 'soundlab-api')
+        self.max_age_seconds = config.get('jwt_max_age', 900)  # 15 minutes
+        self.clock_skew_seconds = config.get('jwt_clock_skew', 60)
+
+    def verify_token(self, token: str) -> Dict:
+        """
+        Verify JWT token with audience, expiration, and nonce checks.
 
-    def __init__(self, config: dict) :
+        Args:
             token: JWT token string
 
         Returns:
             Decoded token payload
 
         Raises:
-            HTTPException, expired, or replayed
+            HTTPException: If token is invalid, expired, or replayed
         """
-        try,
+        try:
+            # Decode and verify signature
+            payload = jwt.decode(
+                token,
                 self.public_key,
                 algorithms=[self.algorithm],
                 audience=self.audience,
                 leeway=self.clock_skew_seconds  # ±60s clock skew tolerance
+            )
 
             # Check expiration (exp ≤ 15 minutes)
             exp = payload.get('exp')
             iat = payload.get('iat')
 
-            if not exp or not iat,
+            if not exp or not iat:
+                raise HTTPException(
+                    status_code=status.HTTP_401_UNAUTHORIZED,
                     detail="Token missing exp or iat"
+                )
 
             token_age = exp - iat
-            if token_age > self.max_age_seconds,
+            if token_age > self.max_age_seconds:
+                raise HTTPException(
+                    status_code=status.HTTP_401_UNAUTHORIZED,
                     detail=f"Token lifetime {token_age}s exceeds max {self.max_age_seconds}s"
+                )
 
             # Nonce/JTI replay protection
             nonce = payload.get('jti') or payload.get('nonce')
-            if not nonce,
+            if not nonce:
+                raise HTTPException(
+                    status_code=status.HTTP_401_UNAUTHORIZED,
                     detail="Token missing jti/nonce"
+                )
 
             # Check if nonce already used
             if nonce in _nonce_cache:
-                logger.warning(f"Replay attempt detected)
+                logger.warning(f"Replay attempt detected: nonce={nonce}")
                 raise HTTPException(
                     status_code=status.HTTP_401_UNAUTHORIZED,
                     detail="Token replay detected"
+                )
 
             # Add nonce to cache
             _nonce_cache.add(nonce)
 
             # Limit cache size (FIFO)
-            if len(_nonce_cache) > _nonce_cache_max_size)
+            if len(_nonce_cache) > _nonce_cache_max_size:
+                _nonce_cache.pop()
 
             return payload
 
-        except jwt.ExpiredSignatureError,
+        except jwt.ExpiredSignatureError:
+            raise HTTPException(
+                status_code=status.HTTP_401_UNAUTHORIZED,
                 detail="Token expired"
-
-        except jwt.InvalidAudienceError,
+            )
+        except jwt.InvalidAudienceError:
+            raise HTTPException(
+                status_code=status.HTTP_401_UNAUTHORIZED,
                 detail="Invalid audience"
+            )
+        except jwt.InvalidTokenError as e:
+            raise HTTPException(
+                status_code=status.HTTP_401_UNAUTHORIZED,
+                detail=f"Invalid token: {str(e)}"
+            )
+
+
+class RateLimiter:
+    """Rate limiting for REST and WebSocket (FR-007)"""
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        self.config = config
+        self.rest_limit = config.get('rate_limit_rest', 100)  # req/10s
+        self.rest_window = config.get('rate_limit_window', 10)
+        self.ws_handshake_limit = config.get('ws_handshake_limit', 10)  # per IP
 
-        except jwt.InvalidTokenError as e,
-                detail=f"Invalid token)}"
+        # Buckets: {(token_or_ip, endpoint): [(timestamp, count), ...]}
+        self.buckets: Dict[Tuple[str, str], list] = {}
 
-class RateLimiter)"""
+    def _get_bucket_key(self, identifier: str, endpoint: str) -> Tuple[str, str]:
+        """Get bucket key for rate limiting"""
+        return (identifier, endpoint)
 
-    def __init__(self, config: dict) :
+    def _cleanup_bucket(self, bucket: list, window: int) -> list:
+        """Remove expired entries from bucket"""
+        now = time.time()
+        return [(ts, count) for ts, count in bucket if now - ts < window]
+
+    def check_rate_limit(self, identifier: str, endpoint: str) -> bool:
         """
         Check if request is within rate limit.
 
         Args:
             identifier: Token hash or IP address
             endpoint: Endpoint path
 
-        Returns, False if exceeded
+        Returns:
+            True if within limit, False if exceeded
         """
         key = self._get_bucket_key(identifier, endpoint)
         now = time.time()
 
         # Get and clean bucket
         bucket = self.buckets.get(key, [])
         bucket = self._cleanup_bucket(bucket, self.rest_window)
 
         # Count requests in window
         total_requests = sum(count for _, count in bucket)
 
         if total_requests >= self.rest_limit:
-            logger.warning(f"Rate limit exceeded)
+            logger.warning(f"Rate limit exceeded: {identifier} on {endpoint}")
             return False
 
         # Add current request
         bucket.append((now, 1))
         self.buckets[key] = bucket
 
         return True
 
-    def get_retry_after(self, identifier, endpoint) : dict) :
+    def get_retry_after(self, identifier: str, endpoint: str) -> int:
+        """Get retry-after seconds for rate limited request"""
+        key = self._get_bucket_key(identifier, endpoint)
+        bucket = self.buckets.get(key, [])
+
+        if not bucket:
+            return 0
+
+        oldest_ts = min(ts for ts, _ in bucket)
+        retry_after = int(self.rest_window - (time.time() - oldest_ts))
+        return max(0, retry_after)
+
+
+class CSRFProtection:
+    """CSRF protection for state-changing requests (FR-006)"""
+
+    def __init__(self, config: Dict[str, Any]) -> None:
+        self.config = config
+        self.enabled = config.get('csrf_enabled', True)
+        self.header_name = config.get('csrf_header', 'X-CSRF-Token')
+        self.cookie_name = config.get('csrf_cookie', 'csrf_token')
+
+    def generate_token(self) -> str:
+        """Generate CSRF token"""
+        return secrets.token_urlsafe(32)
+
+    def verify_token(self, request: Request) -> bool:
         """
         Verify CSRF token using double-submit pattern.
 
         Args:
             request: FastAPI request object
 
-        Returns, False otherwise
+        Returns:
+            True if token valid, False otherwise
         """
-        if not self.enabled, 'HEAD', 'OPTIONS'])
+        if not self.enabled:
+            return True
+
+        # Skip CSRF for safe methods
+        if request.method in ['GET', 'HEAD', 'OPTIONS']:
+            return True
+
+        # Get token from header and cookie
+        header_token = request.headers.get(self.header_name)
         cookie_token = request.cookies.get(self.cookie_name)
 
-        if not header_token or not cookie_token)
+        if not header_token or not cookie_token:
+            return False
+
+        # Compare tokens (constant-time)
         return secrets.compare_digest(header_token, cookie_token)
 
 
-class SecurityHeadersMiddleware(BaseHTTPMiddleware), FR-004, FR-005)"""
+class SecurityHeadersMiddleware(BaseHTTPMiddleware):
+    """Security headers middleware (FR-003, FR-004, FR-005)"""
+
+    def __init__(self, app: Any, config: Dict[str, Any]) -> None:
+        super().__init__(app)
+        self.config = config
+
+    async def dispatch(self, request: Request, call_next):
+        response = await call_next(request)
+
+        # FR-003: HSTS (HTTP Strict Transport Security)
+        response.headers['Strict-Transport-Security'] = \
+            'max-age=31536000; includeSubDomains; preload'
 
-    def __init__(self, app, config: dict) : Additional security headers
+        # FR-004: Content Security Policy
+        csp = self.config.get('csp_policy',
+            "default-src 'self'; "
+            "script-src 'self' 'sha256-...'; "
+            "connect-src 'self' wss:; "
+            "object-src 'none'; "
+            "frame-ancestors 'none'; "
+            "base-uri 'self'; "
+            "form-action 'self'"
+        )
+        response.headers['Content-Security-Policy'] = csp
+
+        # FR-005: Additional security headers
         response.headers['X-Content-Type-Options'] = 'nosniff'
         response.headers['X-Frame-Options'] = 'DENY'
         response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
 
         # Cross-Origin policies
         response.headers['Cross-Origin-Opener-Policy'] = 'same-origin'
         response.headers['Cross-Origin-Embedder-Policy'] = 'require-corp'
 
-        # Permissions Policy (FR-005))
+        # Permissions Policy (FR-005: camera/mic=())
         permissions = self.config.get('permissions_policy',
             'camera=(), microphone=(), geolocation=(), payment=()'
-
+        )
         response.headers['Permissions-Policy'] = permissions
 
         # Remove server header
         response.headers.pop('Server', None)
 
         return response
 
 
 class AuthMiddleware:
+    """Authentication middleware for REST endpoints"""
+
+    def __init__(self, token_validator: TokenValidator) -> None:
+        self.token_validator = token_validator
+
+    def __call__(self, func):
+        @wraps(func)
+        async def wrapper(*args, **kwargs):
+            request = kwargs.get('request') or args[0]
+
+            # Extract token from Authorization header
+            auth_header = request.headers.get('Authorization')
+            if not auth_header or not auth_header.startswith('Bearer '):
+                raise HTTPException(
+                    status_code=status.HTTP_401_UNAUTHORIZED,
+                    detail="Missing or invalid Authorization header"
+                )
+
+            token = auth_header.split(' ')[1]
+
+            # Verify token
+            payload = self.token_validator.verify_token(token)
+
+            # Add user info to request state
+            request.state.user = payload
+
+            return await func(*args, **kwargs)
+
+        return wrapper
+
+
+class RateLimitMiddleware(BaseHTTPMiddleware):
+    """Rate limiting middleware"""
+
+    def __init__(self, app: Any, rate_limiter: RateLimiter) -> None:
+        super().__init__(app)
+        self.rate_limiter = rate_limiter
+
+    async def dispatch(self, request: Request, call_next):
+        # Get identifier (IP or token hash)
+        identifier = request.client.host
+
+        # Check Authorization header for token-based limiting
+        auth_header = request.headers.get('Authorization')
+        if auth_header and auth_header.startswith('Bearer '):
+            token = auth_header.split(' ')[1]
+            # Hash token for privacy
+            identifier = hashlib.sha256(token.encode()).hexdigest()[:16]
+
+        endpoint = request.url.path
+
+        # Check rate limit
+        if not self.rate_limiter.check_rate_limit(identifier, endpoint):
+            retry_after = self.rate_limiter.get_retry_after(identifier, endpoint)
+
+            return JSONResponse(
+                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
+                content={"detail": "Rate limit exceeded"},
+                headers={"Retry-After": str(retry_after)}
+            )
+
+        response = await call_next(request)
+        return response
+
+
+def require_auth(token_validator: TokenValidator) -> AuthMiddleware:
+    """Decorator for endpoints requiring authentication"""
+    return AuthMiddleware(token_validator)
+
+
+def init_security_middleware(app: Any, config: Dict[str, Any]) -> Dict[str, Any]:
     """
     Initialize all security middleware.
 
     Args:
         app: FastAPI application
-        config)
+        config: Security configuration dict
+    """
+    # Token validator
+    token_validator = TokenValidator(config)
 
     # Rate limiter
     rate_limiter = RateLimiter(config)
 
     # CSRF protection
     csrf = CSRFProtection(config)
 
     # Add middleware (order matters - last added runs first)
     app.add_middleware(SecurityHeadersMiddleware, config=config)
     app.add_middleware(RateLimitMiddleware, rate_limiter=rate_limiter)
 
     # Store in app state for use in routes
     app.state.token_validator = token_validator
     app.state.rate_limiter = rate_limiter
     app.state.csrf = csrf
 
     logger.info("Security middleware initialized")
 
     return {
-        'token_validator',
-        'rate_limiter',
+        'token_validator': token_validator,
+        'rate_limiter': rate_limiter,
         'csrf': csrf
     }
-
-"""  # auto-closed missing docstring
diff --git a/server/sensor_manager.py b/server/sensor_manager.py
index de19e4e7bf8de2f8319c340fa45c1a29fcda1aa5..468a55b41eeb68b88d5f7a3e45fb02d7e6a705a2 100644
--- a/server/sensor_manager.py
+++ b/server/sensor_manager.py
@@ -1,328 +1,465 @@
 """
 SensorManager - Hardware Sensor Integration (Feature 023, FR-004)
 
 Manages I²S bridge, Φ-sensors, and hybrid node telemetry integration
 Provides unified interface for hardware metrics streaming
 
 Requirements:
 - FR-004: Sensor manager for I²S + ADC + hybrid telemetry
 - FR-008: Watchdog recovery with auto-resync
 - SC-002: Φ-sensor sample rate 30 ± 2 Hz
 - SC-003: Hardware-software coherence > 0.9
-- SC-004, Any, Optional, List, Callable
+- SC-004: Uptime stability ≥ 4 hr continuous run
+"""
+
+import asyncio
+import time
+import json
+import logging
+from typing import Dict, Any, Optional, List, Callable
 from dataclasses import dataclass, asdict
 from pathlib import Path
 import numpy as np
 
 
 @dataclass
 class SensorReading:
     """Single sensor reading with timestamp"""
     timestamp: float
     phi_depth: float
     phi_phase: float
     coherence: float
     criticality: float
     ici: float
     sample_number: int
 
 
 @dataclass
 class SensorStatistics:
     """Sensor statistics and health metrics"""
     total_samples: int
     sample_rate_actual: float
     sample_rate_jitter: float
     dropped_samples: int
     signal_quality: List[float]
     calibrated: bool
     uptime_seconds: float
     coherence_avg: float
 
 
 @dataclass
 class HardwareMetrics:
     """Combined hardware metrics from all sources"""
     i2s_latency_us: float
     i2s_jitter_us: float
     i2s_link_status: str
     phi_sensor_rate_hz: float
     hybrid_node_connected: bool
     coherence_hw_sw: float  # SC-003
     timestamp: float
 
 
-class SensorManager)
-
+class SensorManager:
+    """
+    Hardware sensor manager integrating I²S bridge and Φ-sensors (FR-004)
 
+    Features:
+    - Real-time sensor data acquisition
+    - Hardware-software coherence monitoring (SC-003)
+    - Watchdog recovery with auto-resync (FR-008)
     - Statistics tracking and logging
     - WebSocket broadcasting support
     """
 
-    def __init__(self, config: Optional[Dict[str, Any]]) :
+    def __init__(self, config: Optional[Dict[str, Any]] = None):
         """
         Initialize sensor manager
 
         Args:
-            config)
+            config: Optional configuration dictionary
+        """
+        self.config = config or {}
+        self.logger = logging.getLogger(__name__)
 
         # Sensor state
         self.running = False
         self.current_reading: Optional[SensorReading] = None
         self.last_update_time = 0.0
         self.sample_counter = 0
 
         # Statistics
         self.start_time = 0.0
         self.total_samples = 0
         self.dropped_samples = 0
         self.sample_times: List[float] = []
 
         # Hardware metrics
         self.i2s_metrics = {
-            'latency_us',
-            'jitter_us',
-            'link_status',
+            'latency_us': 0.0,
+            'jitter_us': 0.0,
+            'link_status': 'disconnected',
         }
 
         # Coherence tracking (SC-003)
-        self.coherence_history)
+        self.coherence_history: List[float] = []
+        self.coherence_window = 100  # samples
+
+        # Watchdog state (FR-008)
         self.watchdog_enabled = self.config.get('enable_watchdog', True)
         self.watchdog_threshold_ms = self.config.get('watchdog_threshold_ms', 100)
         self.watchdog_last_sync = time.time()
         self.resync_count = 0
 
         # Callbacks
         self.data_callback: Optional[Callable] = None
-        self.error_callback, True)
+        self.error_callback: Optional[Callable] = None
+
+        # Simulation mode for testing
+        self.simulation_mode = self.config.get('simulation_mode', True)
 
         self.logger.info("SensorManager initialized (simulation_mode=%s)", self.simulation_mode)
 
-    async def start(self))"""
-        if self.running)
+    async def start(self):
+        """Start sensor acquisition (FR-004)"""
+        if self.running:
+            self.logger.warning("SensorManager already running")
             return
 
         self.running = True
         self.start_time = time.time()
         self.sample_counter = 0
         self.total_samples = 0
         self.dropped_samples = 0
         self.sample_times = []
 
         self.logger.info("SensorManager started")
 
         # Start acquisition tasks
         asyncio.create_task(self._acquisition_loop())
 
-        if self.watchdog_enabled))
+        if self.watchdog_enabled:
+            asyncio.create_task(self._watchdog_loop())
 
     async def stop(self):
         """Stop sensor acquisition"""
-        if not self.running, samples=%d)",
+        if not self.running:
+            return
+
+        self.running = False
+        self.logger.info("SensorManager stopped (uptime=%.1fs, samples=%d)",
                         time.time() - self.start_time, self.total_samples)
 
     async def _acquisition_loop(self):
-        """Main acquisition loop (SC-002)"""
+        """Main acquisition loop (SC-002: 30 Hz)"""
         target_interval = 1.0 / 30.0  # 30 Hz
 
-        while self.running)
+        while self.running:
+            loop_start = time.time()
 
-            try)
+            try:
+                # Acquire sensor data
+                reading = await self._read_sensors()
 
-                if reading)
+                if reading:
+                    self.current_reading = reading
+                    self.last_update_time = time.time()
                     self.total_samples += 1
                     self.sample_times.append(time.time())
 
                     # Keep sample times window
-                    if len(self.sample_times) > 100)
+                    if len(self.sample_times) > 100:
+                        self.sample_times.pop(0)
 
                     # Update coherence history (SC-003)
                     self.coherence_history.append(reading.coherence)
-                    if len(self.coherence_history) > self.coherence_window)
+                    if len(self.coherence_history) > self.coherence_window:
+                        self.coherence_history.pop(0)
 
                     # Call data callback if set
-                    if self.data_callback)
+                    if self.data_callback:
+                        await self.data_callback(reading)
 
                     # Update watchdog
                     self.watchdog_last_sync = time.time()
 
             except Exception as e:
-                self.logger.error("Sensor acquisition error, e)
+                self.logger.error("Sensor acquisition error: %s", e)
                 self.dropped_samples += 1
 
-                if self.error_callback)
+                if self.error_callback:
+                    await self.error_callback(e)
 
             # Sleep to maintain target rate
             elapsed = time.time() - loop_start
             sleep_time = max(0, target_interval - elapsed)
             await asyncio.sleep(sleep_time)
 
-    async def _read_sensors(self) :
+    async def _read_sensors(self) -> Optional[SensorReading]:
         """Read sensor data from hardware or simulation"""
 
         if self.simulation_mode:
-            # Simulation mode) - self.start_time
+            # Simulation mode: generate synthetic data
+            t = time.time() - self.start_time
 
             # Generate Φ-modulated signals
             phi = (1 + np.sqrt(5)) / 2  # Golden ratio
 
             reading = SensorReading(
                 timestamp=time.time(),
                 phi_depth=0.5 + 0.3 * np.sin(2 * np.pi * t / phi),
                 phi_phase=(t * phi) % (2 * np.pi),
                 coherence=0.85 + 0.1 * np.cos(2 * np.pi * t / (2 * phi)),
                 criticality=1.0 + 0.5 * np.sin(2 * np.pi * t / 3.0),
                 ici=0.5 + 0.3 * np.cos(2 * np.pi * t / 2.0),
                 sample_number=self.sample_counter
+            )
 
             self.sample_counter += 1
             return reading
 
         else:
-            # Real hardware mode, return None
+            # Real hardware mode: read from I²S bridge / Φ-sensors
+            # This would interface with C++ hardware layers via ctypes or similar
+            # For now, return None
             self.logger.warning("Real hardware mode not yet implemented")
             return None
 
-    async def _watchdog_loop(self))"""
+    async def _watchdog_loop(self):
+        """Watchdog monitor for link recovery (FR-008)"""
+
+        while self.running:
+            await asyncio.sleep(0.1)  # Check every 100 ms
 
-        while self.running)  # Check every 100 ms
+            if not self.current_reading:
+                continue
 
-            if not self.current_reading) - self.watchdog_last_sync) * 1000
+            # Check time since last sync
+            time_since_sync_ms = (time.time() - self.watchdog_last_sync) * 1000
 
             if time_since_sync_ms > self.watchdog_threshold_ms:
                 # Link loss detected - trigger resync
-                self.logger.warning("Watchdog), resyncing...",
+                self.logger.warning("Watchdog: Link loss detected (%.1f ms), resyncing...",
                                    time_since_sync_ms)
 
                 await self._resync()
                 self.resync_count += 1
                 self.watchdog_last_sync = time.time()
 
-    async def _resync(self))"""
+    async def _resync(self):
+        """Resync hardware links (FR-008)"""
         self.logger.info("Performing hardware resync...")
 
         # Reset I²S bridge
         self.i2s_metrics['link_status'] = 'syncing'
 
         # Simulate resync delay
         await asyncio.sleep(0.05)  # 50 ms resync time
 
         self.i2s_metrics['link_status'] = 'stable'
         self.logger.info("Hardware resync complete")
 
-    @lru_cache(maxsize=128)
-    def get_current_reading(self) : Callable) : Callable) :
+    def get_current_reading(self) -> Optional[SensorReading]:
+        """Get most recent sensor reading"""
+        return self.current_reading
+
+    def get_statistics(self) -> SensorStatistics:
+        """Get sensor statistics (SC-002, SC-003, SC-004)"""
+
+        # Calculate sample rate
+        if len(self.sample_times) >= 2:
+            intervals = np.diff(self.sample_times)
+            sample_rate = 1.0 / np.mean(intervals) if len(intervals) > 0 else 0.0
+            jitter = np.std(intervals) * 1000  # ms
+        else:
+            sample_rate = 0.0
+            jitter = 0.0
+
+        # Calculate coherence average (SC-003)
+        coherence_avg = np.mean(self.coherence_history) if self.coherence_history else 0.0
+
+        # Uptime (SC-004)
+        uptime = time.time() - self.start_time if self.start_time > 0 else 0.0
+
+        stats = SensorStatistics(
+            total_samples=self.total_samples,
+            sample_rate_actual=sample_rate,
+            sample_rate_jitter=jitter,
+            dropped_samples=self.dropped_samples,
+            signal_quality=[1.0] * 4,  # Stub
+            calibrated=True,  # Stub
+            uptime_seconds=uptime,
+            coherence_avg=coherence_avg
+        )
+
+        return stats
+
+    def get_hardware_metrics(self) -> HardwareMetrics:
+        """Get combined hardware metrics (FR-004)"""
+
+        stats = self.get_statistics()
+
+        # Calculate hardware-software coherence (SC-003)
+        # This compares hardware Φ-sensor coherence with software Φ-modulator
+        coherence_hw_sw = stats.coherence_avg  # Stub: would compare with software metrics
+
+        metrics = HardwareMetrics(
+            i2s_latency_us=self.i2s_metrics['latency_us'],
+            i2s_jitter_us=self.i2s_metrics['jitter_us'],
+            i2s_link_status=self.i2s_metrics['link_status'],
+            phi_sensor_rate_hz=stats.sample_rate_actual,
+            hybrid_node_connected=True,  # Stub
+            coherence_hw_sw=coherence_hw_sw,
+            timestamp=time.time()
+        )
+
+        return metrics
+
+    def set_data_callback(self, callback: Callable):
+        """Set callback for new sensor data"""
+        self.data_callback = callback
+
+    def set_error_callback(self, callback: Callable):
+        """Set callback for errors"""
+        self.error_callback = callback
+
+    async def calibrate(self, duration_ms: int = 5000) -> Dict[str, Any]:
+        """
+        Perform sensor calibration (FR-007)
+
+        Args:
             duration_ms: Calibration duration in milliseconds
 
-        Returns)", duration_ms)
+        Returns:
+            Calibration results dictionary
+        """
+        self.logger.info("Starting sensor calibration (duration=%d ms)", duration_ms)
 
         # Collect calibration samples
         calibration_samples = []
         start_time = time.time()
 
         while (time.time() - start_time) * 1000 < duration_ms:
-            if self.current_reading))
+            if self.current_reading:
+                calibration_samples.append(asdict(self.current_reading))
             await asyncio.sleep(0.01)
 
         # Calculate calibration parameters
-        if not calibration_samples)
+        if not calibration_samples:
+            self.logger.error("No calibration samples collected")
             return {}
 
         # Calculate voltage ranges for each channel
         phi_depths = [s['phi_depth'] for s in calibration_samples]
         phi_phases = [s['phi_phase'] for s in calibration_samples]
         coherences = [s['coherence'] for s in calibration_samples]
         criticalities = [s['criticality'] for s in calibration_samples]
 
         calibration = {
             'phi_depth': {
-                'min'),
-                'max'),
-                'mean'),
-                'std')
+                'min': min(phi_depths),
+                'max': max(phi_depths),
+                'mean': np.mean(phi_depths),
+                'std': np.std(phi_depths)
             },
             'phi_phase': {
-                'min'),
-                'max'),
-                'mean'),
-                'std')
+                'min': min(phi_phases),
+                'max': max(phi_phases),
+                'mean': np.mean(phi_phases),
+                'std': np.std(phi_phases)
             },
             'coherence': {
-                'min'),
-                'max'),
-                'mean'),
-                'std')
+                'min': min(coherences),
+                'max': max(coherences),
+                'mean': np.mean(coherences),
+                'std': np.std(coherences)
             },
             'criticality': {
-                'min'),
-                'max'),
-                'mean'),
-                'std')
+                'min': min(criticalities),
+                'max': max(criticalities),
+                'mean': np.mean(criticalities),
+                'std': np.std(criticalities)
             },
-            'samples'),
-            'duration_ms',
-            'residual_error',  # SC-005: < 2%
-            'timestamp')
+            'samples': len(calibration_samples),
+            'duration_ms': duration_ms,
+            'residual_error': 1.5,  # SC-005: < 2%
+            'timestamp': time.time()
         }
 
-        self.logger.info("Calibration complete, residual_error=%.2f%%",
+        self.logger.info("Calibration complete: %d samples, residual_error=%.2f%%",
                         len(calibration_samples), calibration['residual_error'])
 
         return calibration
 
-    async def save_calibration(self, calibration, Any], filepath), 'w') as f, f, indent=2)
+    async def save_calibration(self, calibration: Dict[str, Any], filepath: str):
+        """Save calibration to JSON file"""
+        with open(filepath, 'w') as f:
+            json.dump(calibration, f, indent=2)
 
         self.logger.info("Calibration saved to %s", filepath)
 
-    async def load_calibration(self, filepath) :
+    async def load_calibration(self, filepath: str) -> Dict[str, Any]:
+        """Load calibration from JSON file"""
+        with open(filepath, 'r') as f:
+            calibration = json.load(f)
+
+        self.logger.info("Calibration loaded from %s", filepath)
+        return calibration
+
+    def to_dict(self) -> Dict[str, Any]:
         """Convert current state to dictionary for serialization"""
         return {
-            'running',
-            'current_reading') if self.current_reading else None,
-            'statistics')),
-            'hardware_metrics')),
-            'resync_count',
+            'running': self.running,
+            'current_reading': asdict(self.current_reading) if self.current_reading else None,
+            'statistics': asdict(self.get_statistics()),
+            'hardware_metrics': asdict(self.get_hardware_metrics()),
+            'resync_count': self.resync_count,
             'simulation_mode': self.simulation_mode
         }
 
 
 # Example usage
-if __name__ == "__main__"):
+if __name__ == "__main__":
+    import asyncio
+
+    async def main():
         # Create sensor manager in simulation mode
         config = {
-            'simulation_mode',
-            'enable_watchdog',
-            'watchdog_threshold_ms')
+            'simulation_mode': True,
+            'enable_watchdog': True,
+            'watchdog_threshold_ms': 100
+        }
+
+        manager = SensorManager(config)
 
         # Set data callback
-        async def data_callback(reading):
-            print(f"[{reading.sample_number}] Φ-depth={reading.phi_depth, "
-                  f"coherence={reading.coherence)
+        async def data_callback(reading: SensorReading):
+            print(f"[{reading.sample_number}] Φ-depth={reading.phi_depth:.3f}, "
+                  f"coherence={reading.coherence:.3f}")
 
         manager.set_data_callback(data_callback)
 
         # Start acquisition
         await manager.start()
 
         # Run for 10 seconds
         await asyncio.sleep(10)
 
         # Get statistics
         stats = manager.get_statistics()
-        logger.info("\nStatistics)
-        logger.info("  Total samples, stats.total_samples)
-        logger.info("  Sample rate, stats.sample_rate_actual)
-        logger.info("  Jitter, stats.sample_rate_jitter)
-        logger.info("  Coherence avg, stats.coherence_avg)
-        logger.info("  Uptime, stats.uptime_seconds)
+        print(f"\nStatistics:")
+        print(f"  Total samples: {stats.total_samples}")
+        print(f"  Sample rate: {stats.sample_rate_actual:.2f} Hz")
+        print(f"  Jitter: {stats.sample_rate_jitter:.2f} ms")
+        print(f"  Coherence avg: {stats.coherence_avg:.3f}")
+        print(f"  Uptime: {stats.uptime_seconds:.1f}s")
 
         # Perform calibration
         calibration = await manager.calibrate(duration_ms=2000)
-        print(f"\nCalibration, "
-              f"residual_error={calibration['residual_error'])
+        print(f"\nCalibration: {calibration['samples']} samples, "
+              f"residual_error={calibration['residual_error']:.2f}%")
 
         # Stop
         await manager.stop()
 
     # Run example
     asyncio.run(main())
diff --git a/server/sensor_streamer.py b/server/sensor_streamer.py
index 8fbc1024f594dd031b90551f05445be3dadc8a94..314cb97bde52f9bfbb62973178f9c5fbc6d21895 100644
--- a/server/sensor_streamer.py
+++ b/server/sensor_streamer.py
@@ -1,167 +1,228 @@
 """
 Sensor WebSocket Streamer - Feature 023 (FR-003)
 
 Provides /ws/sensors WebSocket endpoint for real-time hardware sensor streaming
 Streams Φ-sensor data, I²S metrics, and hardware telemetry at 30 Hz
 
 Requirements:
 - FR-003: /ws/sensors WebSocket endpoint
-- SC-002, Dict, Any
+- SC-002: 30 Hz stream rate
+"""
+
+import asyncio
+import json
+import logging
+from typing import Set, Dict, Any
 from dataclasses import asdict
 from fastapi import WebSocket, WebSocketDisconnect, FastAPI
 from .sensor_manager import SensorManager, SensorReading
 
 
-class SensorStreamer)
+class SensorStreamer:
+    """
+    WebSocket streamer for hardware sensor data (FR-003)
 
     Broadcasts sensor readings to all connected WebSocket clients at 30 Hz
     """
 
-    def __init__(self, sensor_manager: SensorManager) :
+    def __init__(self, sensor_manager: SensorManager) -> None:
         """
         Initialize sensor streamer
 
         Args:
             sensor_manager: SensorManager instance to stream from
         """
         self.sensor_manager = sensor_manager
-        self.active_connections)
+        self.active_connections: Set[WebSocket] = set()
         self.logger = logging.getLogger(__name__)
         self.broadcast_task = None
         self.running = False
 
-    async def connect(self, websocket))
+    async def connect(self, websocket: WebSocket):
+        """Accept new WebSocket connection"""
+        await websocket.accept()
         self.active_connections.add(websocket)
-        self.logger.info("Sensor WebSocket connected (total)", len(self.active_connections))
+        self.logger.info("Sensor WebSocket connected (total: %d)", len(self.active_connections))
+
+    def disconnect(self, websocket: WebSocket) -> None:
+        """Remove WebSocket connection"""
+        self.active_connections.discard(websocket)
+        self.logger.info("Sensor WebSocket disconnected (total: %d)", len(self.active_connections))
 
-    def disconnect(self, websocket: WebSocket) :
+    async def start_broadcast(self):
         """Start broadcasting sensor data to all connections"""
-        if self.running))
+        if self.running:
+            return
+
+        self.running = True
+        self.broadcast_task = asyncio.create_task(self._broadcast_loop())
         self.logger.info("Sensor broadcast started")
 
     async def stop_broadcast(self):
         """Stop broadcasting"""
         if not self.running:
             return
 
         self.running = False
 
-        if self.broadcast_task)
+        if self.broadcast_task:
+            self.broadcast_task.cancel()
             try:
                 await self.broadcast_task
-            except asyncio.CancelledError)
+            except asyncio.CancelledError:
+                pass
 
-    async def _broadcast_loop(self))"""
+        self.logger.info("Sensor broadcast stopped")
+
+    async def _broadcast_loop(self):
+        """Main broadcast loop at 30 Hz (SC-002)"""
         target_interval = 1.0 / 30.0  # 30 Hz
 
-        while self.running).time()
+        while self.running:
+            loop_start = asyncio.get_event_loop().time()
 
-            try)
+            try:
+                # Get current sensor reading
+                reading = self.sensor_manager.get_current_reading()
 
-                if reading and self.active_connections)
+                if reading and self.active_connections:
+                    # Get additional metrics
+                    stats = self.sensor_manager.get_statistics()
                     hw_metrics = self.sensor_manager.get_hardware_metrics()
 
                     # Build message
                     message = {
-                        'type',
-                        'reading'),
+                        'type': 'sensor_data',
+                        'reading': asdict(reading),
                         'statistics': {
-                            'sample_rate',
-                            'jitter_hz',
-                            'uptime',
-                            'coherence_avg',
+                            'sample_rate': stats.sample_rate_actual,
+                            'jitter_hz': stats.sample_rate_jitter,
+                            'uptime': stats.uptime_seconds,
+                            'coherence_avg': stats.coherence_avg
+                        },
                         'hardware': {
-                            'i2s_latency_us',
-                            'i2s_link_status',
-                            'coherence_hw_sw',
+                            'i2s_latency_us': hw_metrics.i2s_latency_us,
+                            'i2s_link_status': hw_metrics.i2s_link_status,
+                            'coherence_hw_sw': hw_metrics.coherence_hw_sw,
                         }
                     }
 
                     # Broadcast to all connections
                     await self._broadcast(message)
 
             except Exception as e:
-                self.logger.error("Broadcast error, e)
+                self.logger.error("Broadcast error: %s", e)
 
             # Sleep to maintain target rate
             elapsed = asyncio.get_event_loop().time() - loop_start
             sleep_time = max(0, target_interval - elapsed)
             await asyncio.sleep(sleep_time)
 
-    async def _broadcast(self, message, Any]):
+    async def _broadcast(self, message: Dict[str, Any]):
         """Broadcast message to all connected clients"""
-        if not self.active_connections)
+        if not self.active_connections:
+            return
+
+        message_json = json.dumps(message)
         dead_connections = set()
 
         for connection in self.active_connections:
-            try)
+            try:
+                await connection.send_text(message_json)
             except Exception as e:
-                self.logger.warning("Failed to send to connection, e)
+                self.logger.warning("Failed to send to connection: %s", e)
                 dead_connections.add(connection)
 
         # Remove dead connections
-        for connection in dead_connections)
+        for connection in dead_connections:
+            self.disconnect(connection)
+
+    async def send_event(self, event: Dict[str, Any]):
+        """Send one-time event to all connections"""
+        await self._broadcast(event)
 
-    async def send_event(self, event, Any]))
 
+def create_sensor_websocket_endpoint(app: FastAPI, sensor_manager: SensorManager) -> SensorStreamer:
+    """
+    Create /ws/sensors WebSocket endpoint (FR-003)
 
-def create_sensor_websocket_endpoint(app, sensor_manager) :
+    Args:
         app: FastAPI application
         sensor_manager: SensorManager instance
 
+    Returns:
+        SensorStreamer instance
+    """
+    streamer = SensorStreamer(sensor_manager)
+
     @app.websocket("/ws/sensors")
-    async def websocket_sensors_endpoint(websocket))"""
+    async def websocket_sensors_endpoint(websocket: WebSocket):
+        """WebSocket endpoint for hardware sensor streaming (30 Hz)"""
         await streamer.connect(websocket)
 
         try:
             # Keep connection alive and handle client messages
             while True:
-                try, config, etc.)
+                try:
+                    # Wait for client messages (keep-alive, config, etc.)
                     data = await websocket.receive_text()
 
                     # Handle client commands
-                    try)
+                    try:
+                        msg = json.loads(data)
                         msg_type = msg.get('type')
 
-                        if msg_type == 'get_stats')
+                        if msg_type == 'get_stats':
+                            # Send statistics
+                            stats = sensor_manager.get_statistics()
                             await websocket.send_json({
-                                'type',
-                                'data')
+                                'type': 'statistics',
+                                'data': asdict(stats)
                             })
 
-                        elif msg_type == 'get_hardware_metrics')
+                        elif msg_type == 'get_hardware_metrics':
+                            # Send hardware metrics
+                            hw_metrics = sensor_manager.get_hardware_metrics()
                             await websocket.send_json({
-                                'type',
-                                'data')
+                                'type': 'hardware_metrics',
+                                'data': asdict(hw_metrics)
                             })
 
                     except json.JSONDecodeError:
                         pass
 
                 except WebSocketDisconnect:
                     break
 
-        finally)
+        finally:
+            streamer.disconnect(websocket)
 
     return streamer
 
 
 # Example usage
-if __name__ == "__main__")
+if __name__ == "__main__":
+    import uvicorn
+    from fastapi import FastAPI
+
+    # Create FastAPI app
+    app = FastAPI()
 
     # Create sensor manager
-    sensor_manager = SensorManager({'simulation_mode')
+    sensor_manager = SensorManager({'simulation_mode': True})
 
     # Create WebSocket endpoint
     streamer = create_sensor_websocket_endpoint(app, sensor_manager)
 
     @app.on_event("startup")
-    async def startup())
+    async def startup():
+        await sensor_manager.start()
         await streamer.start_broadcast()
 
     @app.on_event("shutdown")
-    async def shutdown())
+    async def shutdown():
+        await streamer.stop_broadcast()
         await sensor_manager.stop()
 
     # Run server
     uvicorn.run(app, host="0.0.0.0", port=8001)
diff --git a/server/session_comparator.py b/server/session_comparator.py
index a13244f72f192cc509cd134188457f5bf9d5cab2..3526d6f312a263eca5940fe21a6df509306cd3f1 100644
--- a/server/session_comparator.py
+++ b/server/session_comparator.py
@@ -1,291 +1,380 @@
 """
 SessionComparator - Feature 015: Multi-Session Comparative Analytics
 
 Loads and analyzes multiple recorded sessions for comparative insights.
 
 Features:
 - FR-001: Load and compare multiple sessions
-
+- FR-002: Compute statistical metrics (mean, variance, delta)
 - SC-001: Handle multiple sessions <= 1 GB RAM
 
 Requirements:
 - FR-001: System MUST load and compare multiple sessions simultaneously
+- FR-002: System MUST compute statistical metrics
+"""
 
 import numpy as np
 from typing import List, Dict, Optional, Tuple
 from dataclasses import dataclass, asdict
 from collections import defaultdict
 
 from .session_memory import MetricSnapshot
 
 
 @dataclass
 class SessionStats:
     """Statistical summary for a session"""
     session_id: str
     duration: float
     sample_count: int
 
     # ICI statistics
     mean_ici: float
     std_ici: float
     min_ici: float
     max_ici: float
 
     # Coherence statistics
     mean_coherence: float
     std_coherence: float
 
     # Criticality statistics
     mean_criticality: float
     std_criticality: float
 
     # Phi statistics
     mean_phi: float
     std_phi: float
     min_phi: float
     max_phi: float
 
 
 @dataclass
 class ComparisonResult:
     """Result of comparing two sessions"""
     session_a_id: str
-    session_b_id)
+    session_b_id: str
+
+    # Delta metrics (FR-002)
     delta_mean_ici: float
     delta_mean_coherence: float
     delta_mean_criticality: float
     delta_mean_phi: float
 
     # Correlation coefficients
     ici_correlation: float
     coherence_correlation: float
     criticality_correlation: float
     phi_correlation: float
 
     # Statistical significance
     ici_ttest_pvalue: float
     coherence_ttest_pvalue: float
 
 
 class SessionComparator:
     """
     SessionComparator - Multi-session analysis and comparison
 
-
-
+    Handles:
+    - Loading multiple sessions (FR-001)
+    - Computing comparative statistics (FR-002)
+    - Memory-efficient processing (SC-001)
     """
 
-    def __init__(self) :
+    def __init__(self):
         """Initialize SessionComparator"""
-        self.sessions, Dict] = {}
-        self.session_stats, SessionStats] = {}
+        self.sessions: Dict[str, Dict] = {}
+        self.session_stats: Dict[str, SessionStats] = {}
 
-    @lru_cache(maxsize=128)
-    def load_session(self, session_id, session_data) :
+    def load_session(self, session_id: str, session_data: Dict) -> bool:
+        """
+        Load a session for comparison (FR-001)
+
+        Args:
             session_id: Unique identifier for session
             session_data: Session data from StateRecorder
 
         Returns:
             True if loaded successfully
         """
-        try, session_data)
+        try:
+            # Store session data
+            self.sessions[session_id] = session_data
+
+            # Compute statistics
+            stats = self._compute_session_stats(session_id, session_data)
             self.session_stats[session_id] = stats
 
-            logger.info("[SessionComparator] Loaded session, session_id)
+            print(f"[SessionComparator] Loaded session: {session_id}")
             return True
 
         except Exception as e:
-            logger.error("[SessionComparator] Error loading session, e)
+            print(f"[SessionComparator] Error loading session: {e}")
             return False
 
-    @lru_cache(maxsize=128)
-    def unload_session(self, session_id: str) :
+    def unload_session(self, session_id: str):
+        """
+        Unload a session to free memory (SC-001)
+
+        Args:
             session_id: Session to unload
         """
         if session_id in self.sessions:
             del self.sessions[session_id]
             del self.session_stats[session_id]
-            logger.info("[SessionComparator] Unloaded session, session_id)
+            print(f"[SessionComparator] Unloaded session: {session_id}")
+
+    def _compute_session_stats(self, session_id: str, session_data: Dict) -> SessionStats:
+        """
+        Compute statistics for a session (FR-002)
 
-    @lru_cache(maxsize=128)
-    def _compute_session_stats(self, session_id, session_data) :
+        Args:
             session_id: Session identifier
             session_data: Session data
 
-        Returns, {})
+        Returns:
+            SessionStats object
+        """
+        metadata = session_data.get("metadata", {})
         samples_dict = session_data.get("samples", [])
 
         # Extract metric arrays
         icis = np.array([s.get("ici", 0.5) for s in samples_dict])
         coherences = np.array([s.get("coherence", 0.5) for s in samples_dict])
         criticalities = np.array([s.get("criticality", 0.5) for s in samples_dict])
         phis = np.array([s.get("phi_value", 1.0) for s in samples_dict])
 
         return SessionStats(
             session_id=session_id,
             duration=metadata.get("duration", 0.0),
             sample_count=len(samples_dict),
             mean_ici=float(np.mean(icis)),
             std_ici=float(np.std(icis)),
             min_ici=float(np.min(icis)),
             max_ici=float(np.max(icis)),
             mean_coherence=float(np.mean(coherences)),
             std_coherence=float(np.std(coherences)),
             mean_criticality=float(np.mean(criticalities)),
             std_criticality=float(np.std(criticalities)),
             mean_phi=float(np.mean(phis)),
             std_phi=float(np.std(phis)),
             min_phi=float(np.min(phis)),
             max_phi=float(np.max(phis))
+        )
 
-    def compare_sessions(self, session_a_id, session_b_id) :
+    def compare_sessions(self, session_a_id: str, session_b_id: str) -> Optional[ComparisonResult]:
+        """
+        Compare two sessions (FR-002)
+
+        Args:
             session_a_id: First session ID
             session_b_id: Second session ID
 
         Returns:
             ComparisonResult or None if comparison failed
         """
         if session_a_id not in self.sessions or session_b_id not in self.sessions:
             return None
 
-        try)
+        try:
+            # Get statistics
+            stats_a = self.session_stats[session_a_id]
+            stats_b = self.session_stats[session_b_id]
+
+            # Compute deltas (FR-002)
             delta_ici = stats_b.mean_ici - stats_a.mean_ici
             delta_coherence = stats_b.mean_coherence - stats_a.mean_coherence
             delta_criticality = stats_b.mean_criticality - stats_a.mean_criticality
             delta_phi = stats_b.mean_phi - stats_a.mean_phi
 
             # Extract time series for correlation
             samples_a = self.sessions[session_a_id].get("samples", [])
             samples_b = self.sessions[session_b_id].get("samples", [])
 
             icis_a = np.array([s.get("ici", 0.5) for s in samples_a])
             icis_b = np.array([s.get("ici", 0.5) for s in samples_b])
 
             coherences_a = np.array([s.get("coherence", 0.5) for s in samples_a])
             coherences_b = np.array([s.get("coherence", 0.5) for s in samples_b])
 
             criticalities_a = np.array([s.get("criticality", 0.5) for s in samples_a])
             criticalities_b = np.array([s.get("criticality", 0.5) for s in samples_b])
 
             phis_a = np.array([s.get("phi_value", 1.0) for s in samples_a])
             phis_b = np.array([s.get("phi_value", 1.0) for s in samples_b])
 
             # Align lengths for correlation (use shorter length)
             min_len = min(len(icis_a), len(icis_b))
             icis_a = icis_a[:min_len]
             icis_b = icis_b[:min_len]
             coherences_a = coherences_a[:min_len]
             coherences_b = coherences_b[:min_len]
             criticalities_a = criticalities_a[:min_len]
             criticalities_b = criticalities_b[:min_len]
             phis_a = phis_a[:min_len]
-            phis_b = phis_b[, icis_b)[0, 1]) if min_len > 1 else 0.0
+            phis_b = phis_b[:min_len]
+
+            # Compute correlations
+            ici_corr = float(np.corrcoef(icis_a, icis_b)[0, 1]) if min_len > 1 else 0.0
             coherence_corr = float(np.corrcoef(coherences_a, coherences_b)[0, 1]) if min_len > 1 else 0.0
             criticality_corr = float(np.corrcoef(criticalities_a, criticalities_b)[0, 1]) if min_len > 1 else 0.0
             phi_corr = float(np.corrcoef(phis_a, phis_b)[0, 1]) if min_len > 1 else 0.0
 
             # Statistical tests (t-test)
             from scipy import stats
             ici_ttest = stats.ttest_ind(icis_a, icis_b)
             coherence_ttest = stats.ttest_ind(coherences_a, coherences_b)
 
             return ComparisonResult(
                 session_a_id=session_a_id,
                 session_b_id=session_b_id,
                 delta_mean_ici=delta_ici,
                 delta_mean_coherence=delta_coherence,
                 delta_mean_criticality=delta_criticality,
                 delta_mean_phi=delta_phi,
                 ici_correlation=ici_corr,
                 coherence_correlation=coherence_corr,
                 criticality_correlation=criticality_corr,
                 phi_correlation=phi_corr,
                 ici_ttest_pvalue=float(ici_ttest.pvalue),
                 coherence_ttest_pvalue=float(coherence_ttest.pvalue)
+            )
 
         except Exception as e:
-            logger.error("[SessionComparator] Error comparing sessions, e)
+            print(f"[SessionComparator] Error comparing sessions: {e}")
             return None
 
-    def get_all_stats(self) :
+    def get_all_stats(self) -> Dict[str, SessionStats]:
         """
         Get statistics for all loaded sessions
 
-    def get_session_count(self) : str, ici_offset: float) :
+        Returns:
+            Dictionary mapping session IDs to stats
+        """
+        return self.session_stats.copy()
+
+    def get_session_count(self) -> int:
+        """Get number of loaded sessions"""
+        return len(self.sessions)
+
+    def get_memory_usage(self) -> Dict:
+        """
+        Estimate memory usage (SC-001)
+
+        Returns:
+            Dictionary with memory estimates
+        """
+        import sys
+
+        total_samples = sum(
+            len(session.get("samples", []))
+            for session in self.sessions.values()
+        )
+
+        # Rough estimate: each sample ~200 bytes
+        estimated_mb = (total_samples * 200) / (1024 * 1024)
+
+        return {
+            "session_count": len(self.sessions),
+            "total_samples": total_samples,
+            "estimated_mb": estimated_mb
+        }
+
+
+# Self-test
+def _self_test():
+    """Run basic self-test of SessionComparator"""
+    print("=" * 60)
+    print("SessionComparator Self-Test")
+    print("=" * 60)
+    print()
+
+    import time
+
+    # Create synthetic session data
+    print("1. Creating synthetic sessions...")
+
+    def create_session(session_id: str, ici_offset: float):
+        samples = []
+        for i in range(100):
             samples.append({
-                "timestamp") + i * 0.1,
-                "ici"),
-                "coherence",
-                "criticality",
-                "phi_value",
-                "phi_phase",
-                "phi_depth",
-                "active_source")
+                "timestamp": time.time() + i * 0.1,
+                "ici": 0.5 + ici_offset + 0.05 * np.sin(i * 0.1),
+                "coherence": 0.6,
+                "criticality": 0.4,
+                "phi_value": 1.0,
+                "phi_phase": 0.0,
+                "phi_depth": 0.5,
+                "active_source": "test"
+            })
 
         return {
             "metadata": {
-                "session_id",
-                "duration",
-                "sample_count",
-            "samples", 0.0)
+                "session_id": session_id,
+                "duration": 10.0,
+                "sample_count": 100
+            },
+            "samples": samples
+        }
+
+    session_a = create_session("session_a", 0.0)
     session_b = create_session("session_b", 0.1)  # Higher ICI
-    logger.info("   [OK] Created 2 synthetic sessions")
-    logger.info(str())
+    print("   [OK] Created 2 synthetic sessions")
+    print()
 
     # Create comparator
-    logger.info("2. Creating SessionComparator...")
+    print("2. Creating SessionComparator...")
     comparator = SessionComparator()
-    logger.info("   [OK] SessionComparator created")
-    logger.info(str())
+    print("   [OK] SessionComparator created")
+    print()
 
     # Load sessions
-    logger.info("3. Loading sessions...")
+    print("3. Loading sessions...")
     comparator.load_session("session_a", session_a)
     comparator.load_session("session_b", session_b)
-    logger.info("   [OK] Loaded %s sessions", comparator.get_session_count())
-    logger.info(str())
+    print(f"   [OK] Loaded {comparator.get_session_count()} sessions")
+    print()
 
     # Get statistics
-    logger.info("4. Computing statistics...")
+    print("4. Computing statistics...")
     stats = comparator.get_all_stats()
     for session_id, stat in stats.items():
-        logger.info("   %s, session_id)
-        logger.info("      Mean ICI, stat.mean_ici)
-        logger.info("      Mean Phi, stat.mean_phi)
-    logger.info("   [OK] Statistics computed")
-    logger.info(str())
+        print(f"   {session_id}:")
+        print(f"      Mean ICI: {stat.mean_ici:.3f}")
+        print(f"      Mean Phi: {stat.mean_phi:.3f}")
+    print("   [OK] Statistics computed")
+    print()
 
     # Compare sessions
-    logger.info("5. Comparing sessions...")
+    print("5. Comparing sessions...")
     result = comparator.compare_sessions("session_a", "session_b")
     if result:
-        logger.info("   Delta ICI, result.delta_mean_ici)
-        logger.info("   ICI correlation, result.ici_correlation)
-        logger.info("   ICI t-test p-value, result.ici_ttest_pvalue)
+        print(f"   Delta ICI: {result.delta_mean_ici:.3f}")
+        print(f"   ICI correlation: {result.ici_correlation:.3f}")
+        print(f"   ICI t-test p-value: {result.ici_ttest_pvalue:.6f}")
 
         # Check if delta is correct (should be ~0.1)
         delta_ok = abs(result.delta_mean_ici - 0.1) < 0.05
-        logger.error("   [%s] Delta accuracy (expected ~0.1)", 'OK' if delta_ok else 'FAIL')
-    logger.info(str())
+        print(f"   [{'OK' if delta_ok else 'FAIL'}] Delta accuracy (expected ~0.1)")
+    print()
 
     # Memory usage
-    logger.info("6. Checking memory usage...")
+    print("6. Checking memory usage...")
     mem_usage = comparator.get_memory_usage()
-    logger.info("   Sessions, mem_usage['session_count'])
-    logger.info("   Total samples, mem_usage['total_samples'])
-    logger.info("   Estimated memory, mem_usage['estimated_mb'])
+    print(f"   Sessions: {mem_usage['session_count']}")
+    print(f"   Total samples: {mem_usage['total_samples']}")
+    print(f"   Estimated memory: {mem_usage['estimated_mb']:.2f} MB")
     mem_ok = mem_usage['estimated_mb'] < 1024  # < 1 GB
-    logger.error("   [%s] Memory usage (SC-001)", 'OK' if mem_ok else 'FAIL')
-    logger.info(str())
+    print(f"   [{'OK' if mem_ok else 'FAIL'}] Memory usage (SC-001: < 1 GB)")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/session_memory.py b/server/session_memory.py
index 3342cea8d166dc063fbc0d519d2485df64fb3c10..f40af3cdd5af37ca7c3102ee9da8c94a891caf9c 100644
--- a/server/session_memory.py
+++ b/server/session_memory.py
@@ -1,307 +1,405 @@
 """
 SessionMemory - Feature 012: Predictive Phi-Adaptation Engine
 
 Stores metric logs and Phi adjustments for learning and replay.
 
 Features:
-- FR-003, coherence, criticality
+- FR-003: Record metric and Phi pairs for learning
+- Time-series storage of ICI, coherence, criticality
 - Session save/load functionality
 - Efficient circular buffer for real-time recording
 
 Requirements:
+- FR-003: System MUST record metric and Phi pairs for learning
+"""
 
 import time
 import threading
 import json
 from typing import Optional, List, Dict, Tuple
 from dataclasses import dataclass, asdict
 from collections import deque
 import numpy as np
 
 
 @dataclass
 class MetricSnapshot:
     """Single snapshot of system metrics"""
     timestamp: float
     ici: float                    # Integrated Chroma Intensity
     coherence: float              # Coherence metric
     criticality: float            # Criticality metric
     phi_value: float              # Current Phi value
     phi_phase: float              # Current Phi phase
     phi_depth: float              # Phi modulation depth
     active_source: str            # Active Phi source
 
 
 @dataclass
 class SessionStats:
     """Statistics for a session"""
     start_time: float
     end_time: float
     duration: float
     sample_count: int
     avg_ici: float
     std_ici: float
     avg_coherence: float
     avg_criticality: float
     avg_phi: float
     ici_stability_score: float    # How stable ICI was around 0.5
 
 
 class SessionMemory:
     """
     SessionMemory - Stores metric and Phi history for learning
 
+    Handles:
+    - Real-time metric recording (FR-003)
     - Session save/load
     - Circular buffer for efficient storage
     - Statistical analysis of sessions
     """
 
-    def __init__(self, max_samples: int) :
+    def __init__(self, max_samples: int = 10000):
         """
         Initialize SessionMemory
 
         Args:
-            max_samples)
+            max_samples: Maximum samples to store (circular buffer)
         """
         self.max_samples = max_samples
 
         # Circular buffer for real-time recording
-        self.samples)
+        self.samples: deque = deque(maxlen=max_samples)
         self.lock = threading.Lock()
 
         # Session metadata
         self.session_id: Optional[str] = None
-        self.session_start_time, session_id: Optional[str]) :
+        self.session_start_time: Optional[float] = None
+        self.is_recording = False
+
+    def start_session(self, session_id: Optional[str] = None):
         """
         Start a new recording session
 
         Args:
             session_id: Optional session identifier
         """
-        with self.lock))}"
+        with self.lock:
+            self.session_id = session_id or f"session_{int(time.time())}"
             self.session_start_time = time.time()
             self.samples.clear()
             self.is_recording = True
 
-    def stop_session(self) :
+    def stop_session(self):
         """Stop current recording session"""
-        with self.lock, snapshot: MetricSnapshot) :
+        with self.lock:
+            self.is_recording = False
+
+    def record_snapshot(self, snapshot: MetricSnapshot):
+        """
+        Record a metric snapshot (FR-003)
+
+        Args:
             snapshot: MetricSnapshot to record
         """
         if not self.is_recording:
             return
 
-        with self.lock)
+        with self.lock:
+            self.samples.append(snapshot)
 
-    def get_recent_samples(self, count) :
+    def get_recent_samples(self, count: int = 100) -> List[MetricSnapshot]:
         """
         Get most recent samples
 
         Args:
             count: Number of recent samples to retrieve
 
         Returns:
             List of recent MetricSnapshot objects
         """
-        with self.lock)[-count)
-    def get_all_samples(self) :
+        with self.lock:
+            # Get last N samples
+            recent = list(self.samples)[-count:]
+            return recent
+
+    def get_all_samples(self) -> List[MetricSnapshot]:
         """
         Get all samples from current session
 
         Returns:
             List of all MetricSnapshot objects
         """
-        with self.lock)
+        with self.lock:
+            return list(self.samples)
 
-    @lru_cache(maxsize=128)
-    def get_sample_count(self) :
+    def get_sample_count(self) -> int:
         """Get total number of samples in current session"""
-        with self.lock)
+        with self.lock:
+            return len(self.samples)
 
-    @lru_cache(maxsize=128)
-    def compute_stats(self) :
+    def compute_stats(self) -> Optional[SessionStats]:
         """
         Compute statistics for current session
 
         Returns:
             SessionStats object or None if no data
         """
-        with self.lock) == 0)
+        with self.lock:
+            if len(self.samples) == 0:
+                return None
+
+            samples = list(self.samples)
 
             # Extract arrays
             timestamps = np.array([s.timestamp for s in samples])
             icis = np.array([s.ici for s in samples])
             coherences = np.array([s.coherence for s in samples])
             criticalities = np.array([s.criticality for s in samples])
             phis = np.array([s.phi_value for s in samples])
 
             # Compute statistics
             start_time = timestamps[0]
             end_time = timestamps[-1]
             duration = end_time - start_time
 
-            # ICI stability)
+            # ICI stability: measure how close ICI stays to target (0.5)
             # Lower deviation from 0.5 = higher stability
             ici_deviations = np.abs(icis - 0.5)
             ici_stability = 1.0 - np.mean(ici_deviations)  # 1.0 = perfect stability
 
             return SessionStats(
                 start_time=start_time,
                 end_time=end_time,
                 duration=duration,
                 sample_count=len(samples),
                 avg_ici=float(np.mean(icis)),
                 std_ici=float(np.std(icis)),
                 avg_coherence=float(np.mean(coherences)),
                 avg_criticality=float(np.mean(criticalities)),
                 avg_phi=float(np.mean(phis)),
                 ici_stability_score=float(ici_stability)
+            )
 
-    @lru_cache(maxsize=128)
-    def save_session(self, filepath) :
+    def save_session(self, filepath: str) -> bool:
         """
         Save current session to file
 
         Args:
             filepath: Path to save file
 
         Returns:
             True if saved successfully
         """
         try:
-            with self.lock) == 0) for s in self.samples]
+            with self.lock:
+                if len(self.samples) == 0:
+                    return False
+
+                # Convert samples to dict format
+                samples_dict = [asdict(s) for s in self.samples]
 
                 # Compute stats
                 stats = self.compute_stats()
                 stats_dict = asdict(stats) if stats else {}
 
                 # Create session data
                 session_data = {
-                    "session_id",
-                    "session_start_time",
-                    "stats",
-                    "samples", 'w') as f, f, indent=2)
+                    "session_id": self.session_id,
+                    "session_start_time": self.session_start_time,
+                    "stats": stats_dict,
+                    "samples": samples_dict
+                }
+
+                # Write to file
+                with open(filepath, 'w') as f:
+                    json.dump(session_data, f, indent=2)
 
                 return True
 
         except Exception as e:
-            logger.error("[SessionMemory] Error saving session, e)
+            print(f"[SessionMemory] Error saving session: {e}")
             return False
 
-    def load_session(self, filepath) :
+    def load_session(self, filepath: str) -> bool:
         """
         Load session from file
 
         Args:
             filepath: Path to session file
 
         Returns:
             True if loaded successfully
         """
-        try, 'r') as f)
+        try:
+            with open(filepath, 'r') as f:
+                session_data = json.load(f)
 
-            with self.lock)
+            with self.lock:
+                # Restore metadata
+                self.session_id = session_data.get("session_id")
                 self.session_start_time = session_data.get("session_start_time")
 
                 # Restore samples
                 self.samples.clear()
-                for sample_dict in session_data.get("samples", []))
+                for sample_dict in session_data.get("samples", []):
+                    snapshot = MetricSnapshot(**sample_dict)
                     self.samples.append(snapshot)
 
                 self.is_recording = False
 
             return True
 
         except Exception as e:
-            logger.error("[SessionMemory] Error loading session, e)
+            print(f"[SessionMemory] Error loading session: {e}")
             return False
 
-    @lru_cache(maxsize=128)
-    def get_time_series(self, metric) :
+    def get_time_series(self, metric: str = "ici") -> Tuple[np.ndarray, np.ndarray]:
         """
         Get time series for a specific metric
 
         Args:
-            metric, "coherence", "criticality", "phi_value")
+            metric: Metric name ("ici", "coherence", "criticality", "phi_value")
 
-        Returns, values) tuple
+        Returns:
+            (timestamps, values) tuple
         """
-        with self.lock) == 0), np.array([])
+        with self.lock:
+            if len(self.samples) == 0:
+                return np.array([]), np.array([])
 
             samples = list(self.samples)
             timestamps = np.array([s.timestamp for s in samples])
 
-            if metric == "ici")
-            elif metric == "coherence")
-            elif metric == "criticality")
-            elif metric == "phi_value")
-            elif metric == "phi_phase")
-            elif metric == "phi_depth")
-            else), np.array([])
+            if metric == "ici":
+                values = np.array([s.ici for s in samples])
+            elif metric == "coherence":
+                values = np.array([s.coherence for s in samples])
+            elif metric == "criticality":
+                values = np.array([s.criticality for s in samples])
+            elif metric == "phi_value":
+                values = np.array([s.phi_value for s in samples])
+            elif metric == "phi_phase":
+                values = np.array([s.phi_phase for s in samples])
+            elif metric == "phi_depth":
+                values = np.array([s.phi_depth for s in samples])
+            else:
+                return np.array([]), np.array([])
 
             return timestamps, values
 
-    @lru_cache(maxsize=128)
-    def get_correlation(self, metric1, metric2) :
+    def get_correlation(self, metric1: str = "ici", metric2: str = "phi_value") -> float:
         """
         Compute correlation between two metrics
 
         Args:
             metric1: First metric name
             metric2: Second metric name
 
+        Returns:
+            Correlation coefficient (-1 to 1)
         """
         _, values1 = self.get_time_series(metric1)
         _, values2 = self.get_time_series(metric2)
 
-        if len(values1) < 2 or len(values2) < 2, values2)[0, 1])
+        if len(values1) < 2 or len(values2) < 2:
+            return 0.0
+
+        return float(np.corrcoef(values1, values2)[0, 1])
 
 
 # Self-test function
-def _self_test() :
-        logger.info("   Duration, stats.duration)
-        logger.info("   Avg ICI, stats.avg_ici)
-        logger.info("   Std ICI, stats.std_ici)
-        logger.info("   ICI Stability, stats.ici_stability_score)
-        logger.info("   Avg Coherence, stats.avg_coherence)
-        logger.info("   Avg Phi, stats.avg_phi)
-        logger.info("   [OK] Statistics computed")
-    logger.info(str())
+def _self_test():
+    """Run basic self-test of SessionMemory"""
+    print("=" * 60)
+    print("SessionMemory Self-Test")
+    print("=" * 60)
+    print()
+
+    # Create memory
+    print("1. Creating SessionMemory...")
+    memory = SessionMemory(max_samples=1000)
+    print("   [OK] SessionMemory created")
+    print()
+
+    # Start session
+    print("2. Starting session...")
+    memory.start_session("test_session")
+    print(f"   [OK] Session started: {memory.session_id}")
+    print()
+
+    # Record samples
+    print("3. Recording 100 samples...")
+    base_time = time.time()
+    for i in range(100):
+        snapshot = MetricSnapshot(
+            timestamp=base_time + i * 0.1,
+            ici=0.5 + 0.1 * np.sin(i * 0.1),
+            coherence=0.7 + 0.1 * np.cos(i * 0.15),
+            criticality=0.3 + 0.05 * np.sin(i * 0.2),
+            phi_value=1.0 + 0.2 * np.sin(i * 0.1),
+            phi_phase=i * 0.1,
+            phi_depth=0.5,
+            active_source="test"
+        )
+        memory.record_snapshot(snapshot)
+
+    count = memory.get_sample_count()
+    print(f"   [OK] Recorded {count} samples")
+    print()
+
+    # Compute stats
+    print("4. Computing statistics...")
+    stats = memory.compute_stats()
+    if stats:
+        print(f"   Duration: {stats.duration:.2f} s")
+        print(f"   Avg ICI: {stats.avg_ici:.3f}")
+        print(f"   Std ICI: {stats.std_ici:.3f}")
+        print(f"   ICI Stability: {stats.ici_stability_score:.3f}")
+        print(f"   Avg Coherence: {stats.avg_coherence:.3f}")
+        print(f"   Avg Phi: {stats.avg_phi:.3f}")
+        print("   [OK] Statistics computed")
+    print()
 
     # Test time series
-    logger.info("5. Testing time series extraction...")
+    print("5. Testing time series extraction...")
     timestamps, icis = memory.get_time_series("ici")
-    logger.info("   [OK] Extracted %s ICI values", len(timestamps))
-    logger.info(str())
+    print(f"   [OK] Extracted {len(timestamps)} ICI values")
+    print()
 
     # Test correlation
-    logger.info("6. Testing correlation...")
+    print("6. Testing correlation...")
     corr = memory.get_correlation("ici", "phi_value")
-    logger.info("   ICI-Phi correlation, corr)
-    logger.info("   [OK] Correlation computed")
-    logger.info(str())
+    print(f"   ICI-Phi correlation: {corr:.3f}")
+    print("   [OK] Correlation computed")
+    print()
 
     # Test save/load
-    logger.info("7. Testing save/load...")
+    print("7. Testing save/load...")
     save_path = "test_session.json"
-    if memory.save_session(save_path), save_path)
+    if memory.save_session(save_path):
+        print(f"   [OK] Session saved to {save_path}")
 
         # Load it back
         memory2 = SessionMemory()
-        if memory2.load_session(save_path))
-            logger.info("   [OK] Session loaded, loaded_count)
+        if memory2.load_session(save_path):
+            loaded_count = memory2.get_sample_count()
+            print(f"   [OK] Session loaded: {loaded_count} samples")
 
             # Cleanup
             import os
             os.remove(save_path)
-            logger.info("   [OK] Cleanup complete")
-    logger.info(str())
+            print("   [OK] Cleanup complete")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/session_recorder.py b/server/session_recorder.py
index c041e312c24e1a5b2f338a92529873e3979491f2..a29cb127395ba4707826b870970b2007e948f325 100644
--- a/server/session_recorder.py
+++ b/server/session_recorder.py
@@ -1,532 +1,719 @@
 """
 Session Recorder - Feature 017
 Records all key data streams (audio, metrics, Φ-modulation, controls) into synchronized logs
 
 Implements:
 - FR-001: SessionRecorder class
-- FR-002, metrics, phi, controls
+- FR-002: Capture from audio, metrics, phi, controls
 - FR-003: Store to session folder /sessions/YYYYMMDD_HHMMSS/
-
-
+- FR-004: File outputs (audio.wav, metrics.jsonl, controls.jsonl, phi.jsonl)
+- FR-005: Optional MP4 export (future enhancement)
 - FR-006: REST endpoints
 - FR-007: Size estimation
 
 Success Criteria:
 - SC-001: Synchronization drift ≤5ms
 - SC-002: Recording stability ≥30 min
-
+- SC-003: Export reproducible (100% round-trip)
+- SC-004: Replay latency <50ms difference
+"""
 
 import os
 import json
 import time
 import wave
 import threading
 import numpy as np
 from pathlib import Path
 from typing import Optional, Dict, List, Any, Callable
 from dataclasses import dataclass
 from queue import Queue, Empty
 from datetime import datetime
 
 
 @dataclass
 class SessionRecorderConfig:
     """Configuration for Session Recorder"""
 
     # Base directory for sessions
     sessions_dir: str = "sessions"
 
     # Sample rate for audio recording
-    sample_rate)
+    sample_rate: int = 48000
+
+    # Number of audio channels (stereo downmix)
     audio_channels: int = 2
 
     # Queue sizes for buffering
     audio_queue_size: int = 100
     metrics_queue_size: int = 1000
     phi_queue_size: int = 1000
-    controls_queue_size) for file writes
+    controls_queue_size: int = 1000
+
+    # Flush interval (seconds) for file writes
     flush_interval: float = 1.0
 
     # Enable logging
     enable_logging: bool = True
 
 
 class SessionRecorder:
     """
     Session Recorder for capturing synchronized data streams
 
-
-
+    Records:
+    - Audio output buffer (WAV format)
+    - Metrics stream (JSONL format)
+    - Phi parameters (JSONL format)
+    - Control events (JSONL format)
 
     All streams synchronized with microsecond-precision timestamps.
     """
 
-    def __init__(self, config: Optional[SessionRecorderConfig]) :
+    def __init__(self, config: Optional[SessionRecorderConfig] = None):
         """
         Initialize Session Recorder
 
         Args:
-            config)
+            config: SessionRecorderConfig (uses defaults if None)
         """
         self.config = config or SessionRecorderConfig()
 
         # Recording state
         self.is_recording: bool = False
         self.session_path: Optional[Path] = None
         self.start_time: Optional[float] = None  # perf_counter for precise timing
-        self.start_timestamp)
-        self.audio_queue)
-        self.metrics_queue)
-        self.phi_queue)
-        self.controls_queue)
+        self.start_timestamp: Optional[datetime] = None  # datetime for folder name
+
+        # Data queues (thread-safe)
+        self.audio_queue: Queue = Queue(maxsize=self.config.audio_queue_size)
+        self.metrics_queue: Queue = Queue(maxsize=self.config.metrics_queue_size)
+        self.phi_queue: Queue = Queue(maxsize=self.config.phi_queue_size)
+        self.controls_queue: Queue = Queue(maxsize=self.config.controls_queue_size)
 
         # File handles
         self.audio_file: Optional[wave.Wave_write] = None
         self.metrics_file: Optional[Any] = None
         self.phi_file: Optional[Any] = None
         self.controls_file: Optional[Any] = None
 
         # Writer threads
         self.audio_writer_thread: Optional[threading.Thread] = None
         self.data_writer_thread: Optional[threading.Thread] = None
-        self.writer_stop_event)
+        self.writer_stop_event: threading.Event = threading.Event()
 
         # Statistics
         self.audio_frames_written: int = 0
         self.metrics_frames_written: int = 0
         self.phi_frames_written: int = 0
         self.controls_events_written: int = 0
         self.total_bytes_written: int = 0
 
         # Last flush time
         self.last_flush_time: float = 0.0
 
         # Error state
-        self.last_error)
-        logger.info("[SessionRecorder]   sessions_dir=%s", self.config.sessions_dir)
-        logger.info("[SessionRecorder]   sample_rate=%s", self.config.sample_rate)
+        self.last_error: Optional[str] = None
 
-    def start_recording(self) :
+        print("[SessionRecorder] Initialized")
+        print(f"[SessionRecorder]   sessions_dir={self.config.sessions_dir}")
+        print(f"[SessionRecorder]   sample_rate={self.config.sample_rate}")
+
+    def start_recording(self) -> bool:
+        """
+        Start recording session (FR-002, FR-003)
+
+        Returns:
+            True if recording started successfully, False otherwise
+        """
+        if self.is_recording:
             self.last_error = "Already recording"
             return False
 
-        try)
+        try:
+            # Create session folder with timestamp (FR-003)
             self.start_timestamp = datetime.now()
             session_name = self.start_timestamp.strftime("%Y%m%d_%H%M%S")
 
             sessions_dir = Path(self.config.sessions_dir)
             sessions_dir.mkdir(parents=True, exist_ok=True)
 
             self.session_path = sessions_dir / session_name
             self.session_path.mkdir(parents=True, exist_ok=True)
 
             # Reset start time for synchronized timestamps (SC-001)
             self.start_time = time.perf_counter()
 
             # Open files (FR-004)
             self._open_files()
 
             # Start writer threads
             self.writer_stop_event.clear()
             self.audio_writer_thread = threading.Thread(target=self._audio_writer_loop, daemon=True)
             self.data_writer_thread = threading.Thread(target=self._data_writer_loop, daemon=True)
 
             self.audio_writer_thread.start()
             self.data_writer_thread.start()
 
             # Reset statistics
             self.audio_frames_written = 0
             self.metrics_frames_written = 0
             self.phi_frames_written = 0
             self.controls_events_written = 0
             self.total_bytes_written = 0
             self.last_flush_time = time.time()
 
             self.is_recording = True
             self.last_error = None
 
             if self.config.enable_logging:
-                logger.info("[SessionRecorder] Recording started, self.session_path)
+                print(f"[SessionRecorder] Recording started: {self.session_path}")
 
             return True
 
         except Exception as e:
             self.last_error = f"Failed to start recording: {e}"
-            logger.error("[SessionRecorder] ERROR, self.last_error)
+            print(f"[SessionRecorder] ERROR: {self.last_error}")
             self._cleanup()
             return False
 
-    def stop_recording(self) :
+    def stop_recording(self) -> bool:
         """
         Stop recording session and close files
 
-        Returns, False otherwise
+        Returns:
+            True if stopped successfully, False otherwise
         """
         if not self.is_recording:
             self.last_error = "Not recording"
             return False
 
         try:
-            if self.config.enable_logging)
+            if self.config.enable_logging:
+                print("[SessionRecorder] Stopping recording...")
 
             # Signal writer threads to stop
             self.writer_stop_event.set()
 
             # Wait for threads to finish (with timeout)
-            if self.audio_writer_thread)
-            if self.data_writer_thread)
+            if self.audio_writer_thread:
+                self.audio_writer_thread.join(timeout=5.0)
+            if self.data_writer_thread:
+                self.data_writer_thread.join(timeout=5.0)
 
             # Close files
             self._close_files()
 
             # Write session metadata
             self._write_metadata()
 
             self.is_recording = False
 
-            if self.config.enable_logging)
-                logger.info("[SessionRecorder]   audio_frames, self.audio_frames_written)
-                logger.info("[SessionRecorder]   metrics_frames, self.metrics_frames_written)
-                logger.info("[SessionRecorder]   phi_frames, self.phi_frames_written)
-                logger.info("[SessionRecorder]   control_events, self.controls_events_written)
-                logger.info("[SessionRecorder]   total_bytes, self.total_bytes_written)
+            if self.config.enable_logging:
+                print(f"[SessionRecorder] Recording stopped")
+                print(f"[SessionRecorder]   audio_frames: {self.audio_frames_written}")
+                print(f"[SessionRecorder]   metrics_frames: {self.metrics_frames_written}")
+                print(f"[SessionRecorder]   phi_frames: {self.phi_frames_written}")
+                print(f"[SessionRecorder]   control_events: {self.controls_events_written}")
+                print(f"[SessionRecorder]   total_bytes: {self.total_bytes_written}")
 
             return True
 
         except Exception as e:
             self.last_error = f"Failed to stop recording: {e}"
-            logger.error("[SessionRecorder] ERROR, self.last_error)
+            print(f"[SessionRecorder] ERROR: {self.last_error}")
             return False
 
-    @lru_cache(maxsize=128)
-    def record_audio(self, audio_buffer: np.ndarray) :
-            audio_buffer)
+    def record_audio(self, audio_buffer: np.ndarray):
+        """
+        Record audio buffer (FR-002)
+
+        Args:
+            audio_buffer: Audio samples (multi-channel or mono)
         """
         if not self.is_recording:
             return
 
-        try)
+        try:
+            # Calculate relative timestamp (SC-001)
             timestamp = time.perf_counter() - self.start_time
 
             # Put in queue (non-blocking to avoid audio glitches)
             self.audio_queue.put_nowait({
-                'timestamp',
-                'buffer')
+                'timestamp': timestamp,
+                'buffer': audio_buffer.copy()
             })
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[SessionRecorder] Audio queue full or error, e)
+                print(f"[SessionRecorder] Audio queue full or error: {e}")
 
-    def record_metrics(self, metrics_frame: Dict[str, Any]) :
+    def record_metrics(self, metrics_frame: Dict[str, Any]):
+        """
+        Record metrics frame (FR-002)
+
+        Args:
             metrics_frame: Metrics data dictionary
         """
         if not self.is_recording:
             return
 
-        try) - self.start_time
+        try:
+            # Calculate relative timestamp
+            timestamp = time.perf_counter() - self.start_time
 
             # Add timestamp to frame
             frame_with_time = {
-                'timestamp',
+                'timestamp': timestamp,
                 **metrics_frame
             }
 
             self.metrics_queue.put_nowait(frame_with_time)
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[SessionRecorder] Metrics queue full or error, e)
+                print(f"[SessionRecorder] Metrics queue full or error: {e}")
 
-    def record_phi(self, phi_data: Dict[str, Any]) :
+    def record_phi(self, phi_data: Dict[str, Any]):
+        """
+        Record Phi parameters (FR-002)
+
+        Args:
             phi_data: Phi parameter data
         """
         if not self.is_recording:
             return
 
-        try) - self.start_time
+        try:
+            timestamp = time.perf_counter() - self.start_time
 
             frame_with_time = {
-                'timestamp',
+                'timestamp': timestamp,
                 **phi_data
             }
 
             self.phi_queue.put_nowait(frame_with_time)
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[SessionRecorder] Phi queue full or error, e)
+                print(f"[SessionRecorder] Phi queue full or error: {e}")
+
+    def record_control(self, control_event: Dict[str, Any]):
+        """
+        Record control event (FR-002)
 
-    def record_control(self, control_event: Dict[str, Any]) :
+        Args:
             control_event: Control event data
         """
         if not self.is_recording:
             return
 
-        try) - self.start_time
+        try:
+            timestamp = time.perf_counter() - self.start_time
 
             event_with_time = {
-                'timestamp',
+                'timestamp': timestamp,
                 **control_event
             }
 
             self.controls_queue.put_nowait(event_with_time)
 
         except Exception as e:
             if self.config.enable_logging:
-                logger.error("[SessionRecorder] Controls queue full or error, e)
-
-    def _open_files(self) :
+                print(f"[SessionRecorder] Controls queue full or error: {e}")
+
+    def _open_files(self):
+        """Open output files (FR-004)"""
+        # Open audio WAV file
+        audio_path = self.session_path / "audio.wav"
+        self.audio_file = wave.open(str(audio_path), 'wb')
+        self.audio_file.setnchannels(self.config.audio_channels)
+        self.audio_file.setsampwidth(2)  # 16-bit
+        self.audio_file.setframerate(self.config.sample_rate)
+
+        # Open JSONL files
+        self.metrics_file = open(self.session_path / "metrics.jsonl", 'w')
+        self.phi_file = open(self.session_path / "phi.jsonl", 'w')
+        self.controls_file = open(self.session_path / "controls.jsonl", 'w')
+
+    def _close_files(self):
         """Close output files"""
-        if self.audio_file)
+        if self.audio_file:
+            self.audio_file.close()
             self.audio_file = None
 
-        if self.metrics_file)
+        if self.metrics_file:
+            self.metrics_file.close()
             self.metrics_file = None
 
-        if self.phi_file)
+        if self.phi_file:
+            self.phi_file.close()
             self.phi_file = None
 
-        if self.controls_file)
+        if self.controls_file:
+            self.controls_file.close()
             self.controls_file = None
 
-    @lru_cache(maxsize=128)
-    def _audio_writer_loop(self) :
-            try)
+    def _audio_writer_loop(self):
+        """Background thread for writing audio data (SC-002)"""
+        while not self.writer_stop_event.is_set() or not self.audio_queue.empty():
+            try:
+                # Get audio data with timeout
+                data = self.audio_queue.get(timeout=0.1)
 
                 buffer = data['buffer']
 
                 # Convert to stereo if needed
-                if buffer.ndim == 1, buffer], axis=0)
-                elif buffer.shape[0] > 2)
+                if buffer.ndim == 1:
+                    # Mono to stereo
+                    stereo = np.stack([buffer, buffer], axis=0)
+                elif buffer.shape[0] > 2:
+                    # Multi-channel to stereo (downmix)
                     stereo = buffer[:2]
-                else, samples)
-                if stereo.shape[0] != 2, stereo[0]], axis=0)
+                else:
+                    stereo = buffer
+
+                # Ensure stereo shape is (2, samples)
+                if stereo.shape[0] != 2:
+                    stereo = np.stack([stereo[0], stereo[0]], axis=0)
 
                 # Interleave channels and convert to int16
                 interleaved = np.empty((2 * stereo.shape[1],), dtype=np.int16)
-                interleaved[0:).astype(np.int16)
-                interleaved[1:).astype(np.int16)
+                interleaved[0::2] = (stereo[0] * 32767).astype(np.int16)
+                interleaved[1::2] = (stereo[1] * 32767).astype(np.int16)
 
                 # Write to WAV file
-                if self.audio_file))
+                if self.audio_file:
+                    self.audio_file.writeframes(interleaved.tobytes())
                     self.audio_frames_written += stereo.shape[1]
                     self.total_bytes_written += len(interleaved.tobytes())
 
                 # Periodic flush
                 current_time = time.time()
                 if current_time - self.last_flush_time >= self.config.flush_interval:
-                    if self.audio_file)
+                    if self.audio_file:
+                        self.audio_file._file.flush()
                     self.last_flush_time = current_time
 
             except Empty:
                 continue
             except Exception as e:
                 self.last_error = f"Audio writer error: {e}"
-                if self.config.enable_logging, self.last_error)
+                if self.config.enable_logging:
+                    print(f"[SessionRecorder] {self.last_error}")
                 break
 
-    def _data_writer_loop(self) :
-            try))
-                    if self.metrics_file) + '\n'
+    def _data_writer_loop(self):
+        """Background thread for writing metrics/phi/controls (SC-002)"""
+        while not self.writer_stop_event.is_set() or not self._all_data_queues_empty():
+            try:
+                # Write metrics
+                while not self.metrics_queue.empty():
+                    frame = self.metrics_queue.get_nowait()
+                    if self.metrics_file:
+                        line = json.dumps(frame) + '\n'
                         self.metrics_file.write(line)
                         self.metrics_frames_written += 1
                         self.total_bytes_written += len(line.encode('utf-8'))
 
                 # Write phi
-                while not self.phi_queue.empty())
-                    if self.phi_file) + '\n'
+                while not self.phi_queue.empty():
+                    frame = self.phi_queue.get_nowait()
+                    if self.phi_file:
+                        line = json.dumps(frame) + '\n'
                         self.phi_file.write(line)
                         self.phi_frames_written += 1
                         self.total_bytes_written += len(line.encode('utf-8'))
 
                 # Write controls
-                while not self.controls_queue.empty())
-                    if self.controls_file) + '\n'
+                while not self.controls_queue.empty():
+                    event = self.controls_queue.get_nowait()
+                    if self.controls_file:
+                        line = json.dumps(event) + '\n'
                         self.controls_file.write(line)
                         self.controls_events_written += 1
                         self.total_bytes_written += len(line.encode('utf-8'))
 
                 # Periodic flush
                 current_time = time.time()
                 if current_time - self.last_flush_time >= self.config.flush_interval:
-                    if self.metrics_file)
-                    if self.phi_file)
-                    if self.controls_file)
+                    if self.metrics_file:
+                        self.metrics_file.flush()
+                    if self.phi_file:
+                        self.phi_file.flush()
+                    if self.controls_file:
+                        self.controls_file.flush()
                     self.last_flush_time = current_time
 
                 # Small sleep to avoid busy-waiting
                 time.sleep(0.01)
 
             except Exception as e:
                 self.last_error = f"Data writer error: {e}"
-                if self.config.enable_logging, self.last_error)
+                if self.config.enable_logging:
+                    print(f"[SessionRecorder] {self.last_error}")
                 break
 
-    def _all_data_queues_empty(self) :
+    def _all_data_queues_empty(self) -> bool:
+        """Check if all data queues are empty"""
+        return (self.metrics_queue.empty() and
+                self.phi_queue.empty() and
+                self.controls_queue.empty())
+
+    def _write_metadata(self):
+        """Write session metadata file (FR-004)"""
+        if not self.session_path:
             return
 
         metadata = {
-            'session_name',
-            'start_timestamp') if self.start_timestamp else None,
-            'duration_seconds') - self.start_time if self.start_time else 0,
-            'sample_rate',
-            'audio_channels',
+            'session_name': self.session_path.name,
+            'start_timestamp': self.start_timestamp.isoformat() if self.start_timestamp else None,
+            'duration_seconds': time.perf_counter() - self.start_time if self.start_time else 0,
+            'sample_rate': self.config.sample_rate,
+            'audio_channels': self.config.audio_channels,
             'statistics': {
-                'audio_frames',
-                'metrics_frames',
-                'phi_frames',
-                'control_events',
-                'total_bytes',
+                'audio_frames': self.audio_frames_written,
+                'metrics_frames': self.metrics_frames_written,
+                'phi_frames': self.phi_frames_written,
+                'control_events': self.controls_events_written,
+                'total_bytes': self.total_bytes_written
+            },
             'files': {
-                'audio',
-                'metrics',
-                'phi',
-                'controls', 'w') as f, f, indent=2)
+                'audio': 'audio.wav',
+                'metrics': 'metrics.jsonl',
+                'phi': 'phi.jsonl',
+                'controls': 'controls.jsonl'
+            }
+        }
+
+        metadata_path = self.session_path / "session.json"
+        with open(metadata_path, 'w') as f:
+            json.dump(metadata, f, indent=2)
 
-    def _cleanup(self) :
+    def _cleanup(self):
+        """Cleanup resources on error"""
+        self.is_recording = False
+        self._close_files()
+
+    def get_status(self) -> Dict[str, Any]:
         """
         Get recording status
 
         Returns:
             Status dictionary
         """
         if not self.is_recording:
             return {
-                'is_recording',
-                'session_path',
-                'duration',
-                'statistics',
-                'last_error') - self.start_time if self.start_time else 0
+                'is_recording': False,
+                'session_path': None,
+                'duration': 0,
+                'statistics': {},
+                'last_error': self.last_error
+            }
+
+        duration = time.perf_counter() - self.start_time if self.start_time else 0
 
         return {
-            'is_recording',
-            'session_path') if self.session_path else None,
-            'session_name',
-            'duration',
+            'is_recording': True,
+            'session_path': str(self.session_path) if self.session_path else None,
+            'session_name': self.session_path.name if self.session_path else None,
+            'duration': duration,
             'statistics': {
-                'audio_frames',
-                'metrics_frames',
-                'phi_frames',
-                'control_events',
-                'total_bytes',
-                'estimated_size_mb')
+                'audio_frames': self.audio_frames_written,
+                'metrics_frames': self.metrics_frames_written,
+                'phi_frames': self.phi_frames_written,
+                'control_events': self.controls_events_written,
+                'total_bytes': self.total_bytes_written,
+                'estimated_size_mb': self.total_bytes_written / (1024 * 1024)
             },
-            'last_error', duration_seconds) :
+            'last_error': self.last_error
+        }
+
+    def get_size_estimate(self, duration_seconds: float) -> Dict[str, float]:
+        """
+        Estimate recording size (FR-007)
+
+        Args:
             duration_seconds: Expected duration in seconds
 
         Returns:
             Size estimates in MB
         """
         # Audio: sample_rate * channels * 2 bytes * duration
         audio_size = self.config.sample_rate * self.config.audio_channels * 2 * duration_seconds
 
         # Metrics: ~30 Hz * ~500 bytes per frame
         metrics_size = 30 * 500 * duration_seconds
 
         # Phi: ~30 Hz * ~200 bytes per frame
         phi_size = 30 * 200 * duration_seconds
 
         # Controls: ~1 Hz * ~100 bytes per event
         controls_size = 1 * 100 * duration_seconds
 
         total_size = audio_size + metrics_size + phi_size + controls_size
 
         return {
-            'audio_mb'),
-            'metrics_mb'),
-            'phi_mb'),
-            'controls_mb'),
-            'total_mb')
+            'audio_mb': audio_size / (1024 * 1024),
+            'metrics_mb': metrics_size / (1024 * 1024),
+            'phi_mb': phi_size / (1024 * 1024),
+            'controls_mb': controls_size / (1024 * 1024),
+            'total_mb': total_size / (1024 * 1024)
         }
 
-    def list_sessions(self) :
+    def list_sessions(self) -> List[Dict[str, Any]]:
         """
         List all recorded sessions
 
-        if not sessions_dir.exists()), reverse=True)):
-                try, 'r') as f)
+        Returns:
+            List of session information dictionaries
+        """
+        sessions_dir = Path(self.config.sessions_dir)
+
+        if not sessions_dir.exists():
+            return []
+
+        sessions = []
+
+        for session_path in sorted(sessions_dir.iterdir(), reverse=True):
+            if not session_path.is_dir():
+                continue
+
+            # Try to read metadata
+            metadata_path = session_path / "session.json"
+            if metadata_path.exists():
+                try:
+                    with open(metadata_path, 'r') as f:
+                        metadata = json.load(f)
                         sessions.append(metadata)
-                except Exception, provide basic info
+                except Exception:
+                    # If metadata can't be read, provide basic info
                     sessions.append({
-                        'session_name',
-                        'path')
+                        'session_name': session_path.name,
+                        'path': str(session_path)
                     })
             else:
                 sessions.append({
-                    'session_name',
-                    'path')
+                    'session_name': session_path.name,
+                    'path': str(session_path)
                 })
 
         return sessions
 
 
 # Self-test function
-def _self_test() :
+def _self_test():
+    """Test Session Recorder"""
+    print("=" * 60)
+    print("Session Recorder Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
+    config = SessionRecorderConfig(
+        sessions_dir="test_sessions",
+        sample_rate=48000
+    )
+    recorder = SessionRecorder(config)
+
+    assert not recorder.is_recording
+    print("   OK: Initialization")
+
+    # Test 2: Start recording
+    print("\n2. Testing start recording...")
+    success = recorder.start_recording()
+    assert success, "Should start recording successfully"
+    assert recorder.is_recording
+    assert recorder.session_path is not None
+    print(f"   Session path: {recorder.session_path}")
+    print("   OK: Start recording")
+
+    # Test 3: Record data
+    print("\n3. Testing data recording...")
+
+    # Record audio
+    for i in range(10):
+        audio_buffer = np.random.randn(2, 512).astype(np.float32) * 0.1
+        recorder.record_audio(audio_buffer)
+        time.sleep(0.01)
+
+    # Record metrics
+    for i in range(10):
         metrics = {
-            'ici',
-            'coherence',
-            'criticality')
+            'ici': 0.5 + i * 0.01,
+            'coherence': 0.7,
+            'criticality': 1.0
+        }
+        recorder.record_metrics(metrics)
 
     # Record phi
     for i in range(10):
         phi = {
-            'phi_depth',
-            'phi_phase')
+            'phi_depth': 0.5,
+            'phi_phase': 0.0
+        }
+        recorder.record_phi(phi)
 
     # Record controls
     control = {
-        'type',
-        'param',
-        'value')
+        'type': 'set_param',
+        'param': 'phi_depth',
+        'value': 0.6
+    }
+    recorder.record_control(control)
 
     time.sleep(0.5)  # Let writers process
 
-    logger.info("   OK)
+    print("   OK: Data recording")
 
-    # Test 4)
+    # Test 4: Get status
+    print("\n4. Testing status...")
     status = recorder.get_status()
-    logger.info("   Duration, status['duration'])
-    logger.info("   Audio frames, status['statistics']['audio_frames'])
-    logger.info("   Metrics frames, status['statistics']['metrics_frames'])
-    logger.info("   OK)
+    print(f"   Duration: {status['duration']:.2f}s")
+    print(f"   Audio frames: {status['statistics']['audio_frames']}")
+    print(f"   Metrics frames: {status['statistics']['metrics_frames']}")
+    print("   OK: Status")
 
-    # Test 5)
+    # Test 5: Size estimate
+    print("\n5. Testing size estimate...")
     estimate = recorder.get_size_estimate(60.0)
-    logger.info("   Estimated size for 60s, estimate['total_mb'])
-    logger.info("   OK)
+    print(f"   Estimated size for 60s: {estimate['total_mb']:.2f} MB")
+    print("   OK: Size estimate")
 
-    # Test 6)
+    # Test 6: Stop recording
+    print("\n6. Testing stop recording...")
     success = recorder.stop_recording()
     assert success, "Should stop recording successfully"
     assert not recorder.is_recording
-    logger.info("   OK)
+    print("   OK: Stop recording")
 
-    # Test 7)
+    # Test 7: Verify files
+    print("\n7. Testing file verification...")
     assert (recorder.session_path / "audio.wav").exists()
     assert (recorder.session_path / "metrics.jsonl").exists()
     assert (recorder.session_path / "phi.jsonl").exists()
     assert (recorder.session_path / "controls.jsonl").exists()
     assert (recorder.session_path / "session.json").exists()
-    logger.info("   OK)
+    print("   OK: Files exist")
 
-    # Test 8)
+    # Test 8: List sessions
+    print("\n8. Testing list sessions...")
     sessions = recorder.list_sessions()
     assert len(sessions) > 0
-    logger.info("   Found %s session(s)", len(sessions))
-    logger.info("   OK)
+    print(f"   Found {len(sessions)} session(s)")
+    print("   OK: List sessions")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     # Cleanup test session
     import shutil
-    if Path("test_sessions").exists())
+    if Path("test_sessions").exists():
+        shutil.rmtree("test_sessions")
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/state_classifier.py b/server/state_classifier.py
index 5fbeb4b45aa130eccc3014fcffb59503dc60f62e..fec80e068c022a7ea632d6317a76e2e445eeda00 100644
--- a/server/state_classifier.py
+++ b/server/state_classifier.py
@@ -1,450 +1,580 @@
 """
 State Classifier Graph - Adaptive Consciousness State Classification
-Feature 015, coherence, and spectral metrics
+Feature 015: Real-time state classification using ICI, coherence, and spectral metrics
 
 Implements:
 - FR-001: StateClassifierGraph class
 - FR-002: Metrics-based classification
 - FR-003: Classification tree with 6 states
 - FR-004: Markov-like transition probabilities
 - FR-005: WebSocket event broadcasting
 - FR-006: Visualization support
 - FR-007: 512 transition history
 
 States:
 - COMA: ICI < 0.1 & coherence < 0.2
 - SLEEP: centroid < 10 & coherence < 0.4
 - DROWSY: ICI < 0.3
 - AWAKE: ICI 0.3-0.7 & coherence >= 0.4
 - ALERT: ICI > 0.7 & coherence > 0.7
 - HYPERSYNC: ICI > 0.9 & coherence > 0.9
 
 Success Criteria:
 - SC-001: Classification accuracy >= 90%
 - SC-002: Transition latency < 100ms
-
+- SC-003: Stable output (< 2 transitions/sec)
+- SC-004: Graph sync within ±1 frame
+"""
 
 import time
 import numpy as np
 from typing import Optional, Callable, Dict, List, Tuple
 from dataclasses import dataclass
 from collections import deque
 from enum import Enum
 
 
 class ConsciousnessState(Enum):
     """Consciousness states"""
     COMA = "COMA"
     SLEEP = "SLEEP"
     DROWSY = "DROWSY"
     AWAKE = "AWAKE"
     ALERT = "ALERT"
     HYPERSYNC = "HYPERSYNC"
 
 
 @dataclass
 class StateTransition:
     """Single state transition record"""
     timestamp: float
     from_state: ConsciousnessState
     to_state: ConsciousnessState
     ici: float
     coherence: float
     spectral_centroid: float
     probability: float
 
 
 @dataclass
-class StateClassifierConfig)
+class StateClassifierConfig:
+    """Configuration for State Classifier"""
+
+    # Hysteresis for stability (SC-003)
     hysteresis_threshold: float = 0.1  # Must change by this much to switch
-    min_state_duration)
+    min_state_duration: float = 0.5    # Minimum time in state (seconds)
 
     # Transition history (FR-007)
     history_size: int = 512
 
     # Smoothing for input metrics
     smoothing_window: int = 10  # Rolling average over 10 frames
 
     # Logging
     enable_logging: bool = True
     log_interval: float = 2.0
 
 
 class StateClassifierGraph:
+    """
+    Adaptive state classification system
+
+    Analyzes real-time metrics (ICI, coherence, spectral centroid)
     to classify consciousness states and track transitions.
 
-    Features, config: Optional[StateClassifierConfig]) :
+    Features:
+    - 6-state classification tree
+    - Markov transition probabilities
+    - Hysteresis for stability
+    - WebSocket event broadcasting
+    - Transition history
+    """
+
+    def __init__(self, config: Optional[StateClassifierConfig] = None):
         """
         Initialize State Classifier
 
         Args:
-            config)
+            config: StateClassifierConfig (uses defaults if None)
         """
         self.config = config or StateClassifierConfig()
 
         # Current state
         self.current_state: ConsciousnessState = ConsciousnessState.COMA
         self.previous_state: ConsciousnessState = ConsciousnessState.COMA
-        self.state_entry_time)
+        self.state_entry_time: float = time.time()
 
         # Smoothed metrics (for stability)
-        self.ici_history)
-        self.coherence_history)
-        self.centroid_history)
+        self.ici_history: deque = deque(maxlen=self.config.smoothing_window)
+        self.coherence_history: deque = deque(maxlen=self.config.smoothing_window)
+        self.centroid_history: deque = deque(maxlen=self.config.smoothing_window)
 
         # Transition tracking (FR-004, FR-007)
-        self.transition_history)
-        self.transition_counts, ConsciousnessState], int] = {}
+        self.transition_history: deque = deque(maxlen=self.config.history_size)
+        self.transition_counts: Dict[Tuple[ConsciousnessState, ConsciousnessState], int] = {}
 
         # Statistics
         self.total_transitions: int = 0
-        self.state_durations, List[float]] = {
-            state)
-        self.state_change_callback, None]] = None
+        self.state_durations: Dict[ConsciousnessState, List[float]] = {
+            state: [] for state in ConsciousnessState
+        }
+
+        # Callback for state change events (FR-005)
+        self.state_change_callback: Optional[Callable[[Dict], None]] = None
 
         # Logging
-        self.last_log_time)
-        logger.info("[StateClassifier]   hysteresis_threshold=%s", self.config.hysteresis_threshold)
-        logger.info("[StateClassifier]   min_state_duration=%ss", self.config.min_state_duration)
-        logger.info("[StateClassifier]   initial_state=%s", self.current_state.value)
-
-    def classify_state(self, ici, coherence, spectral_centroid) :
-            ici, 1]
-            coherence, 1]
+        self.last_log_time: float = 0.0
+
+        print("[StateClassifier] Initialized")
+        print(f"[StateClassifier]   hysteresis_threshold={self.config.hysteresis_threshold}")
+        print(f"[StateClassifier]   min_state_duration={self.config.min_state_duration}s")
+        print(f"[StateClassifier]   initial_state={self.current_state.value}")
+
+    def classify_state(self, ici: float, coherence: float, spectral_centroid: float) -> bool:
+        """
+        Classify current consciousness state (FR-002, FR-003)
+
+        Args:
+            ici: Integrated Chromatic Information [0, 1]
+            coherence: Phase coherence [0, 1]
             spectral_centroid: Spectral centroid in Hz
 
+        Returns:
+            True if state changed
+        """
+        current_time = time.time()
+
         # Add to history for smoothing
         self.ici_history.append(ici)
         self.coherence_history.append(coherence)
         self.centroid_history.append(spectral_centroid)
 
         # Use smoothed values for classification
-        if len(self.ici_history) < self.config.smoothing_window)
+        if len(self.ici_history) < self.config.smoothing_window:
+            # Not enough history yet
+            return False
+
+        ici_smooth = np.mean(self.ici_history)
         coherence_smooth = np.mean(self.coherence_history)
         centroid_smooth = np.mean(self.centroid_history)
 
         # Apply classification tree (FR-003)
         new_state = self._apply_classification_tree(
             ici_smooth, coherence_smooth, centroid_smooth
+        )
 
         # Check if state should change (with hysteresis)
         if new_state != self.current_state:
-            # Check minimum duration (SC-003)
+            # Check minimum duration (SC-003: stability)
             time_in_state = current_time - self.state_entry_time
 
-            if time_in_state >= self.config.min_state_duration, ici_smooth, coherence_smooth,
+            if time_in_state >= self.config.min_state_duration:
+                # State change is valid
+                return self._change_state(
+                    new_state, ici_smooth, coherence_smooth,
                     centroid_smooth, current_time
+                )
 
         # Log periodically
-        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval)
+        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval:
+            self._log_stats()
             self.last_log_time = current_time
 
         return False
 
-    def _apply_classification_tree(self, ici, coherence,
-                                   centroid) :
-        - COMA: ICI < 0.1 & coherence < 0.2
-
+    def _apply_classification_tree(self, ici: float, coherence: float,
+                                   centroid: float) -> ConsciousnessState:
+        """
+        Apply classification decision tree (FR-003)
 
+        Classification rules:
+        - COMA: ICI < 0.1 & coherence < 0.2
+        - SLEEP: centroid < 10 & coherence < 0.4 (and not COMA)
+        - DROWSY: ICI < 0.3 (and not COMA/SLEEP)
         - AWAKE: ICI 0.3-0.7 & coherence >= 0.4
         - ALERT: ICI > 0.7 & coherence > 0.7
         - HYPERSYNC: ICI > 0.9 & coherence > 0.9
 
         Args:
             ici: Smoothed ICI value
             coherence: Smoothed coherence value
             centroid: Smoothed spectral centroid
 
         Returns:
             Classified state
         """
         # Priority order: check most specific conditions first
 
         # HYPERSYNC: Very high ICI and coherence
         if ici > 0.9 and coherence > 0.9:
             return ConsciousnessState.HYPERSYNC
 
         # ALERT: High ICI and coherence
         if ici > 0.7 and coherence > 0.7:
             return ConsciousnessState.ALERT
 
         # COMA: Very low ICI and coherence
         if ici < 0.1 and coherence < 0.2:
             return ConsciousnessState.COMA
 
-        # SLEEP)
+        # SLEEP: Low centroid and coherence (but not COMA)
         if centroid < 10 and coherence < 0.4:
             return ConsciousnessState.SLEEP
 
         # AWAKE: Moderate ICI with good coherence
         if 0.3 <= ici <= 0.7 and coherence >= 0.4:
             return ConsciousnessState.AWAKE
 
-        # DROWSY)
+        # DROWSY: Low ICI (fallback)
         if ici < 0.3:
             return ConsciousnessState.DROWSY
 
-        # Default, new_state, ici,
-                     coherence, centroid, timestamp) :
+        # Default: stay in current state if no clear match
+        return self.current_state
+
+    def _change_state(self, new_state: ConsciousnessState, ici: float,
+                     coherence: float, centroid: float, timestamp: float) -> bool:
+        """
+        Change to new state and record transition (FR-004, FR-005)
+
+        Args:
             new_state: New state to transition to
             ici: Current ICI value
             coherence: Current coherence value
             centroid: Current spectral centroid
             timestamp: Transition timestamp
 
+        Returns:
+            True (state changed)
         """
         # Record duration in previous state
         duration = timestamp - self.state_entry_time
         self.state_durations[self.current_state].append(duration)
 
         # Create transition record
         transition = StateTransition(
             timestamp=timestamp,
             from_state=self.current_state,
             to_state=new_state,
             ici=ici,
             coherence=coherence,
             spectral_centroid=centroid,
             probability=self._compute_transition_probability(self.current_state, new_state)
+        )
 
         # Update history
         self.transition_history.append(transition)
 
         # Update transition counts
         transition_key = (self.current_state, new_state)
         self.transition_counts[transition_key] = self.transition_counts.get(transition_key, 0) + 1
         self.total_transitions += 1
 
         # Update state
         self.previous_state = self.current_state
         self.current_state = new_state
         self.state_entry_time = timestamp
 
         # Broadcast event (FR-005)
         if self.state_change_callback:
             event = {
-                'type',
-                'current',
-                'prev',
-                'prob',
-                'timestamp',
-                'ici',
-                'coherence',
-                'spectral_centroid')
+                'type': 'state',
+                'current': new_state.value,
+                'prev': self.previous_state.value,
+                'prob': transition.probability,
+                'timestamp': timestamp,
+                'ici': ici,
+                'coherence': coherence,
+                'spectral_centroid': centroid
+            }
+            self.state_change_callback(event)
 
         if self.config.enable_logging:
-            print(f"[StateClassifier] State transition: {self.previous_state.value} :
+            print(f"[StateClassifier] State transition: {self.previous_state.value} -> {new_state.value} "
+                  f"(p={transition.probability:.3f})")
+
+        return True
+
+    def _compute_transition_probability(self, from_state: ConsciousnessState,
+                                       to_state: ConsciousnessState) -> float:
+        """
+        Compute transition probability (FR-004)
+
+        Uses historical transition frequencies to estimate probability
+
+        Args:
             from_state: Current state
             to_state: Target state
 
-        Returns, 1]
+        Returns:
+            Probability [0, 1]
         """
         # Count transitions from this state
         from_count = 0
         transition_key = (from_state, to_state)
 
         for key in self.transition_counts:
             if key[0] == from_state:
                 from_count += self.transition_counts[key]
 
-        if from_count == 0, use uniform prior
+        if from_count == 0:
+            # No history, use uniform prior
             return 1.0 / len(ConsciousnessState)
 
         # Compute empirical probability
         transition_count = self.transition_counts.get(transition_key, 0)
         probability = transition_count / from_count
 
         return probability
 
-    @lru_cache(maxsize=128)
-    def get_current_state(self) :
+    def get_current_state(self) -> Dict:
         """
         Get current state information
 
+        Returns:
+            Dictionary with current state details
+        """
+        current_time = time.time()
         time_in_state = current_time - self.state_entry_time
 
         return {
-            'current_state',
-            'previous_state',
-            'time_in_state',
-            'total_transitions')
-    def get_transition_matrix(self) :
+            'current_state': self.current_state.value,
+            'previous_state': self.previous_state.value,
+            'time_in_state': time_in_state,
+            'total_transitions': self.total_transitions
+        }
+
+    def get_transition_matrix(self) -> np.ndarray:
         """
         Get transition probability matrix
 
+        Returns:
+            6x6 matrix of transition probabilities
+        """
+        states = list(ConsciousnessState)
         n_states = len(states)
         matrix = np.zeros((n_states, n_states))
 
         # Fill matrix with empirical probabilities
         for i, from_state in enumerate(states):
             from_count = 0
 
             # Count all transitions from this state
             for key in self.transition_counts:
                 if key[0] == from_state:
                     from_count += self.transition_counts[key]
 
-            if from_count > 0, to_state in enumerate(states), to_state)
+            if from_count > 0:
+                # Compute probabilities for each target state
+                for j, to_state in enumerate(states):
+                    key = (from_state, to_state)
                     count = self.transition_counts.get(key, 0)
                     matrix[i, j] = count / from_count
 
         return matrix
 
-    @lru_cache(maxsize=128)
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
         """
         Get classifier statistics
 
-        Returns, durations in self.state_durations.items()) > 0))
-            else)[-60) >= 2) / time_span if time_span > 0 else 0.0
+        Returns:
+            Dictionary with statistics
+        """
+        # Compute average durations
+        avg_durations = {}
+        for state, durations in self.state_durations.items():
+            if len(durations) > 0:
+                avg_durations[state.value] = float(np.mean(durations))
+            else:
+                avg_durations[state.value] = 0.0
+
+        # Get recent transition rate
+        recent_transitions = list(self.transition_history)[-60:]  # Last 60 transitions
+        if len(recent_transitions) >= 2:
+            time_span = recent_transitions[-1].timestamp - recent_transitions[0].timestamp
+            transition_rate = len(recent_transitions) / time_span if time_span > 0 else 0.0
         else:
             transition_rate = 0.0
 
         return {
-            'current_state',
-            'previous_state',
-            'total_transitions',
-            'transition_rate',
-            'avg_durations',
-            'history_size')
+            'current_state': self.current_state.value,
+            'previous_state': self.previous_state.value,
+            'total_transitions': self.total_transitions,
+            'transition_rate': transition_rate,
+            'avg_durations': avg_durations,
+            'history_size': len(self.transition_history)
         }
 
-    def get_transition_history(self, n) :
+    def get_transition_history(self, n: int = 512) -> List[Dict]:
+        """
+        Get recent transition history (FR-007)
+
+        Args:
             n: Number of recent transitions to return
 
-        Returns)[-n:]
+        Returns:
+            List of transition dictionaries
+        """
+        recent = list(self.transition_history)[-n:]
 
         return [
             {
-                'timestamp',
-                'from_state',
-                'to_state',
-                'ici',
-                'coherence',
-                'spectral_centroid',
-                'probability') -> None)
-
-        print(f"[StateClassifier] Stats, "
+                'timestamp': t.timestamp,
+                'from_state': t.from_state.value,
+                'to_state': t.to_state.value,
+                'ici': t.ici,
+                'coherence': t.coherence,
+                'spectral_centroid': t.spectral_centroid,
+                'probability': t.probability
+            }
+            for t in recent
+        ]
+
+    def _log_stats(self):
+        """Log classifier statistics"""
+        stats = self.get_statistics()
+
+        print(f"[StateClassifier] Stats: "
+              f"state={stats['current_state']}, "
               f"transitions={stats['total_transitions']}, "
-              f"rate={stats['transition_rate'])
+              f"rate={stats['transition_rate']:.2f}/s")
 
-    def reset(self) -> None)
+    def reset(self):
+        """Reset classifier state"""
+        self.current_state = ConsciousnessState.COMA
+        self.previous_state = ConsciousnessState.COMA
+        self.state_entry_time = time.time()
 
         self.ici_history.clear()
         self.coherence_history.clear()
         self.centroid_history.clear()
 
         self.transition_history.clear()
         self.transition_counts.clear()
         self.total_transitions = 0
 
-        for state in ConsciousnessState)
+        for state in ConsciousnessState:
+            self.state_durations[state].clear()
 
-        logger.info("[StateClassifier] State reset")
+        print("[StateClassifier] State reset")
 
 
 # Self-test function
-def _self_test() -> None)
-    logger.info("State Classifier Self-Test")
-    logger.info("=" * 60)
-
-    # Test 1)
+def _self_test():
+    """Test StateClassifierGraph"""
+    print("=" * 60)
+    print("State Classifier Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = StateClassifierConfig(
         hysteresis_threshold=0.1,
         min_state_duration=0.1,  # Short for testing
         enable_logging=True
-
+    )
     classifier = StateClassifierGraph(config)
 
     assert classifier.current_state == ConsciousnessState.COMA
     assert len(classifier.transition_history) == 0
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Test 2)
+    # Test 2: State classification
+    print("\n2. Testing state classification...")
 
-    # COMA), 0.1, 1000)
+    # COMA: low ICI and coherence
+    for i in range(15):
+        changed = classifier.classify_state(0.05, 0.1, 1000)
 
     assert classifier.current_state == ConsciousnessState.COMA
-    logger.info("   State)", classifier.current_state.value)
+    print(f"   State: {classifier.current_state.value} (expected COMA)")
 
     # Transition to DROWSY
     time.sleep(0.15)
-    for i in range(15), 0.3, 2000)
+    for i in range(15):
+        changed = classifier.classify_state(0.2, 0.3, 2000)
 
     assert classifier.current_state == ConsciousnessState.DROWSY
-    logger.info("   State)", classifier.current_state.value)
+    print(f"   State: {classifier.current_state.value} (expected DROWSY)")
 
     # Transition to AWAKE
     time.sleep(0.15)
-    for i in range(15), 0.6, 3000)
+    for i in range(15):
+        changed = classifier.classify_state(0.5, 0.6, 3000)
 
     assert classifier.current_state == ConsciousnessState.AWAKE
-    logger.info("   State)", classifier.current_state.value)
+    print(f"   State: {classifier.current_state.value} (expected AWAKE)")
 
     # Transition to ALERT
     time.sleep(0.15)
-    for i in range(15), 0.8, 5000)
+    for i in range(15):
+        changed = classifier.classify_state(0.8, 0.8, 5000)
 
     assert classifier.current_state == ConsciousnessState.ALERT
-    logger.info("   State)", classifier.current_state.value)
+    print(f"   State: {classifier.current_state.value} (expected ALERT)")
 
-    logger.info("   OK)
+    print("   OK: State transitions working")
 
-    # Test 3)
+    # Test 3: Transition tracking
+    print("\n3. Testing transition tracking...")
 
     assert classifier.total_transitions >= 3, "Should have at least 3 transitions"
     assert len(classifier.transition_history) >= 3
 
     history = classifier.get_transition_history(10)
-    logger.info("   Transitions recorded, len(history))
+    print(f"   Transitions recorded: {len(history)}")
 
-    for i, trans in enumerate(history[))", i+1, trans['from_state'], trans['to_state'], trans['probability'])
+    for i, trans in enumerate(history[:3]):
+        print(f"     {i+1}. {trans['from_state']} -> {trans['to_state']} (p={trans['probability']:.3f})")
 
-    logger.info("   OK)
+    print("   OK: Transition tracking")
 
-    # Test 4)
+    # Test 4: Transition matrix
+    print("\n4. Testing transition matrix...")
 
     matrix = classifier.get_transition_matrix()
     assert matrix.shape == (6, 6), "Matrix should be 6x6"
 
     # Row sums should be <= 1 (may not visit all states)
-    for i in range(6))
+    for i in range(6):
+        row_sum = np.sum(matrix[i])
         assert 0 <= row_sum <= 1.01, f"Row {i} sum should be in [0, 1], got {row_sum}"
 
-    logger.info("   Matrix shape, matrix.shape)
-    logger.info("   Non-zero entries, np.count_nonzero(matrix))
-    logger.info("   OK)
+    print(f"   Matrix shape: {matrix.shape}")
+    print(f"   Non-zero entries: {np.count_nonzero(matrix)}")
+    print("   OK: Transition matrix")
 
-    # Test 5)
+    # Test 5: Statistics
+    print("\n5. Testing statistics...")
 
     stats = classifier.get_statistics()
 
     assert 'current_state' in stats
     assert 'total_transitions' in stats
     assert 'transition_rate' in stats
 
-    logger.info("   Current state, stats['current_state'])
-    logger.info("   Total transitions, stats['total_transitions'])
-    logger.info("   Transition rate, stats['transition_rate'])
-    logger.info("   OK)
+    print(f"   Current state: {stats['current_state']}")
+    print(f"   Total transitions: {stats['total_transitions']}")
+    print(f"   Transition rate: {stats['transition_rate']:.2f}/s")
+    print("   OK: Statistics")
 
-    # Test 6)
+    # Test 6: Reset
+    print("\n6. Testing reset...")
 
     classifier.reset()
 
     assert classifier.current_state == ConsciousnessState.COMA
     assert classifier.total_transitions == 0
     assert len(classifier.transition_history) == 0
 
-    logger.info("   OK)
+    print("   OK: Reset")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/state_memory.py b/server/state_memory.py
index 513d05ec069a1fab21152cc52c5fbab379ee1397..6bd6efa33a8ba425a8f102a82990d4db12ea30a2 100644
--- a/server/state_memory.py
+++ b/server/state_memory.py
@@ -1,343 +1,562 @@
 """
 State Memory Loop - Temporal Memory for Consciousness States
-Feature 013, hysteresis, and prediction
+Feature 013: Short-term memory for learning, hysteresis, and prediction
 
 Implements:
 - FR-001: StateMemory module
 - FR-002: Rolling buffer of N=256 metric frames
 - FR-003: Trend computation for prediction
 - FR-004: Adaptive bias for Auto-Phi Learner
-
+- FR-005: WebSocket summary (mean, std, trend)
 - FR-006: Enable/disable setting
+- FR-007: Buffer resets each session (no persistence)
 
 Success Criteria:
-
+- SC-001: Hysteresis visible (smooth transitions)
 - SC-002: Predictive bias reduces overshoot by >=30%
 - SC-003: No increase in latency > 2ms
+- SC-004: Disable flag restores baseline
+"""
 
 import time
 import numpy as np
 from typing import Optional, Callable, Dict, List, Tuple
 from dataclasses import dataclass, field
 from collections import deque
 
 
 @dataclass
 class StateMemoryConfig:
     """Configuration for State Memory Loop"""
-    enabled)
-    buffer_size)
-    trend_window)
-    prediction_horizon)
+    enabled: bool = False
+
+    # Buffer size (FR-002)
+    buffer_size: int = 256  # Store last 256 frames
+
+    # Prediction parameters (FR-003)
+    trend_window: int = 30  # Use last 30 samples for trend (1s @ 30Hz)
+    prediction_horizon: float = 1.0  # Predict 1s ahead
+
+    # Bias parameters (FR-004)
     bias_gain: float = 0.3  # Gain for predictive bias
     bias_threshold: float = 0.15  # Minimum trend magnitude to apply bias
 
     # Safety limits
-    max_bias)
+    max_bias: float = 0.2  # Maximum bias magnitude
+
+    # Hysteresis parameters (SC-001)
     smoothing_alpha: float = 0.1  # Exponential smoothing factor
 
     # Logging
     enable_logging: bool = True
     log_interval: float = 2.0  # Log stats every 2s
 
 
 @dataclass
 class MetricsFrame:
     """Single frame of metrics"""
     timestamp: float
     criticality: float
     coherence: float
     ici: float  # Integrated information
     phi_depth: float
     phi_phase: float
 
 
 @dataclass
 class TrendVector:
     """Computed trend information"""
     d_criticality_dt: float = 0.0
     d_coherence_dt: float = 0.0
-    d_phi_depth_dt)
+    d_phi_depth_dt: float = 0.0
+
+    # Predicted values (current + trend * horizon)
     predicted_criticality: float = 0.0
     predicted_coherence: float = 0.0
 
     # Confidence based on trend stability
     confidence: float = 0.0
 
 
 class StateMemory:
     """
     Temporal memory for consciousness states
 
-    Stores recent metric frames and computes trends for)
+    Stores recent metric frames and computes trends for:
+    - Hysteresis (smooth transitions)
+    - Prediction (pre-adjustment)
+    - Learning (pattern recognition)
 
+    Features:
+    - Rolling buffer of 256 frames
+    - Trend computation using linear regression
+    - Adaptive bias for feed-forward control
+    - Enable/disable toggle
+    """
 
-    Features, config: Optional[StateMemoryConfig]) :
+    def __init__(self, config: Optional[StateMemoryConfig] = None):
         """
         Initialize State Memory
 
         Args:
-            config)
+            config: StateMemoryConfig (uses defaults if None)
         """
         self.config = config or StateMemoryConfig()
 
         # Rolling buffer (FR-002)
-        self.buffer)
+        self.buffer: deque[MetricsFrame] = deque(maxlen=self.config.buffer_size)
 
         # Current trend vector
-        self.trend)
+        self.trend: TrendVector = TrendVector()
 
         # Smoothed values (for hysteresis, SC-001)
         self.smoothed_criticality: float = 1.0
         self.smoothed_coherence: float = 0.0
-        self.smoothed_phi_depth)
+        self.smoothed_phi_depth: float = 0.5
+
+        # Predictive bias (FR-004)
         self.current_bias: float = 0.0
 
         # Statistics
         self.total_frames: int = 0
         self.prediction_errors: List[float] = []
         self.bias_history: List[float] = []
 
         # Logging
-        self.last_log_time)
-        self.bias_callback, None]] = None
+        self.last_log_time: float = 0.0
 
-        logger.info("[StateMemory] Initialized")
-        logger.info("[StateMemory]   buffer_size=%s", self.config.buffer_size)
-        logger.info("[StateMemory]   trend_window=%s", self.config.trend_window)
-        logger.info("[StateMemory]   enabled=%s", self.config.enabled)
+        # Callback for bias updates (feeds Auto-Phi Learner)
+        self.bias_callback: Optional[Callable[[float], None]] = None
 
-    def set_enabled(self, enabled: bool) :
-            enabled, False to disable
+        print("[StateMemory] Initialized")
+        print(f"[StateMemory]   buffer_size={self.config.buffer_size}")
+        print(f"[StateMemory]   trend_window={self.config.trend_window}")
+        print(f"[StateMemory]   enabled={self.config.enabled}")
+
+    def set_enabled(self, enabled: bool):
+        """
+        Enable or disable state memory (FR-006, SC-004)
+
+        Args:
+            enabled: True to enable, False to disable
         """
         if self.config.enabled == enabled:
             return
 
         self.config.enabled = enabled
 
-        if enabled)
-        else)
+        if enabled:
+            print("[StateMemory] ENABLED - temporal memory active")
+        else:
+            print("[StateMemory] DISABLED - baseline behavior restored")
             # Reset bias when disabled (SC-004)
             self.current_bias = 0.0
-            if self.bias_callback)
+            if self.bias_callback:
+                self.bias_callback(0.0)
+
+    def add_frame(self, criticality: float, coherence: float, ici: float,
+                  phi_depth: float, phi_phase: float) -> bool:
+        """
+        Add metrics frame to buffer (FR-002)
 
-    @lru_cache(maxsize=128)
-    def add_frame(self, criticality, coherence, ici,
-                  phi_depth, phi_phase) :
+        Args:
             criticality: Current criticality value
             coherence: Current phase coherence
             ici: Integrated information
             phi_depth: Current phi depth
             phi_phase: Current phi phase
 
         Returns:
             True if processing occurred
         """
-        if not self.config.enabled)
+        if not self.config.enabled:
+            return False
+
+        current_time = time.time()
 
         # Create frame
         frame = MetricsFrame(
             timestamp=current_time,
             criticality=criticality,
             coherence=coherence,
             ici=ici,
             phi_depth=phi_depth,
             phi_phase=phi_phase
+        )
 
         # Add to buffer
         self.buffer.append(frame)
         self.total_frames += 1
 
-        # Apply exponential smoothing (SC-001)
+        # Apply exponential smoothing (SC-001: hysteresis)
         alpha = self.config.smoothing_alpha
         self.smoothed_criticality = alpha * criticality + (1 - alpha) * self.smoothed_criticality
         self.smoothed_coherence = alpha * coherence + (1 - alpha) * self.smoothed_coherence
         self.smoothed_phi_depth = alpha * phi_depth + (1 - alpha) * self.smoothed_phi_depth
 
         # Compute trends if we have enough history (FR-003)
-        if len(self.buffer) >= self.config.trend_window)
+        if len(self.buffer) >= self.config.trend_window:
+            self._compute_trends()
 
             # Calculate predictive bias (FR-004)
             self._compute_bias()
 
             # Send bias update
-            if self.bias_callback)
+            if self.bias_callback:
+                self.bias_callback(self.current_bias)
 
         # Log if needed
-        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval)
+        if self.config.enable_logging and (current_time - self.last_log_time) >= self.config.log_interval:
+            self._log_stats()
             self.last_log_time = current_time
 
         return True
 
-    @lru_cache(maxsize=128)
-    def _compute_trends(self) :
+    def _compute_trends(self):
+        """
+        Compute trend vectors using linear regression (FR-003)
+
+        Uses last trend_window samples to compute:
+        - d(criticality)/dt
+        - d(coherence)/dt
+        - d(phi_depth)/dt
+        """
+        if len(self.buffer) < self.config.trend_window:
+            return
+
+        # Get recent frames
+        recent = list(self.buffer)[-self.config.trend_window:]
+
+        # Extract data
+        timestamps = np.array([f.timestamp for f in recent])
+        criticalities = np.array([f.criticality for f in recent])
+        coherences = np.array([f.coherence for f in recent])
+        phi_depths = np.array([f.phi_depth for f in recent])
+
+        # Normalize time to start at 0
+        t = timestamps - timestamps[0]
+
+        # Linear regression for each signal
+        # y = a + b*t, where b is the slope (derivative)
+
+        # Criticality trend
+        if len(t) > 1 and np.std(t) > 0:
+            crit_slope, _ = np.polyfit(t, criticalities, 1)
+            coh_slope, _ = np.polyfit(t, coherences, 1)
+            depth_slope, _ = np.polyfit(t, phi_depths, 1)
+
+            self.trend.d_criticality_dt = float(crit_slope)
+            self.trend.d_coherence_dt = float(coh_slope)
+            self.trend.d_phi_depth_dt = float(depth_slope)
+
+            # Predict future values (FR-003)
+            horizon = self.config.prediction_horizon
+            current_crit = criticalities[-1]
+            current_coh = coherences[-1]
+
+            self.trend.predicted_criticality = current_crit + crit_slope * horizon
+            self.trend.predicted_coherence = current_coh + coh_slope * horizon
+
+            # Compute confidence based on R² (goodness of fit)
+            # Higher R² = more confident in trend
+            crit_r2 = self._compute_r_squared(t, criticalities, crit_slope)
+            self.trend.confidence = float(crit_r2)
+
+    def _compute_r_squared(self, x: np.ndarray, y: np.ndarray, slope: float) -> float:
         """
         Compute R² coefficient of determination
 
         Args:
             x: Independent variable
             y: Dependent variable
             slope: Linear slope
 
+        Returns:
+            R² value (0 to 1)
         """
-        if len(x) < 2) - slope * np.mean(x)
+        if len(x) < 2:
+            return 0.0
+
+        # Predicted values
+        intercept = np.mean(y) - slope * np.mean(x)
         y_pred = slope * x + intercept
 
         # Total and residual sum of squares
         ss_tot = np.sum((y - np.mean(y)) ** 2)
         ss_res = np.sum((y - y_pred) ** 2)
 
-        if ss_tot == 0)
+        if ss_tot == 0:
+            return 0.0
+
+        r_squared = 1.0 - (ss_res / ss_tot)
         return max(0.0, min(1.0, r_squared))  # Clamp to [0, 1]
 
-    @lru_cache(maxsize=128)
-    def _compute_bias(self) :
+    def _compute_bias(self):
+        """
+        Compute predictive bias for Auto-Phi Learner (FR-004, SC-002)
+
+        Bias logic:
+        - If criticality trending toward hypersync (>1.1), apply negative bias
+        - If criticality trending toward coma (<0.4), apply positive bias
+        - Magnitude proportional to trend slope and confidence
+        """
+        # Check if trend is significant
+        if abs(self.trend.d_criticality_dt) < self.config.bias_threshold:
+            self.current_bias = 0.0
+            return
+
+        # Get predicted criticality
+        predicted = self.trend.predicted_criticality
+
+        # Calculate bias based on predicted state
+        bias = 0.0
+
+        # Approaching hypersync (>1.1)
+        if predicted > 1.1:
+            # Negative bias to reduce phi_depth
+            overshoot = predicted - 1.1
+            bias = -self.config.bias_gain * overshoot
+
+        # Approaching coma (<0.4)
+        elif predicted < 0.4:
+            # Positive bias to increase phi_depth
+            undershoot = 0.4 - predicted
+            bias = self.config.bias_gain * undershoot
+
+        # Scale by confidence
+        bias *= self.trend.confidence
+
+        # Clamp to max bias
+        bias = np.clip(bias, -self.config.max_bias, self.config.max_bias)
+
+        self.current_bias = float(bias)
+        self.bias_history.append(self.current_bias)
+
+    def get_bias(self) -> float:
+        """
+        Get current predictive bias (FR-004)
+
+        Returns:
+            Current bias value
+        """
+        return self.current_bias
+
+    def get_smoothed_values(self) -> Dict[str, float]:
+        """
+        Get smoothed values for hysteresis (SC-001)
+
+        Returns:
             Dictionary with smoothed values
         """
         return {
-            'criticality',
-            'coherence',
-            'phi_depth') :
+            'criticality': self.smoothed_criticality,
+            'coherence': self.smoothed_coherence,
+            'phi_depth': self.smoothed_phi_depth
+        }
+
+    def get_trend_summary(self) -> Dict:
+        """
+        Get trend summary (FR-005)
+
+        Returns:
             Dictionary with trend information
         """
         return {
-            'd_criticality_dt',
-            'd_coherence_dt',
-            'd_phi_depth_dt',
-            'predicted_criticality',
-            'predicted_coherence',
-            'confidence',
-            'current_bias') :
+            'd_criticality_dt': self.trend.d_criticality_dt,
+            'd_coherence_dt': self.trend.d_coherence_dt,
+            'd_phi_depth_dt': self.trend.d_phi_depth_dt,
+            'predicted_criticality': self.trend.predicted_criticality,
+            'predicted_coherence': self.trend.predicted_coherence,
+            'confidence': self.trend.confidence,
+            'current_bias': self.current_bias
+        }
+
+    def get_statistics(self) -> Dict:
+        """
+        Get memory statistics (FR-005)
+
+        Returns:
+            Dictionary with statistics
+        """
+        if len(self.buffer) == 0:
             return {
-                'enabled',
-                'buffer_size',
-                'total_frames',
-                'criticality_mean',
-                'criticality_std',
-                'coherence_mean',
-                'coherence_std',
-                'trend_summary')
+                'enabled': self.config.enabled,
+                'buffer_size': 0,
+                'total_frames': self.total_frames,
+                'criticality_mean': 0.0,
+                'criticality_std': 0.0,
+                'coherence_mean': 0.0,
+                'coherence_std': 0.0,
+                'trend_summary': self.get_trend_summary()
             }
 
         # Extract buffer data
         criticalities = [f.criticality for f in self.buffer]
         coherences = [f.coherence for f in self.buffer]
 
         return {
-            'enabled',
-            'buffer_size'),
-            'total_frames',
-            'criticality_mean')),
-            'criticality_std')),
-            'coherence_mean')),
-            'coherence_std')),
-            'smoothed_criticality',
-            'smoothed_coherence',
-            'trend_summary')
+            'enabled': self.config.enabled,
+            'buffer_size': len(self.buffer),
+            'total_frames': self.total_frames,
+            'criticality_mean': float(np.mean(criticalities)),
+            'criticality_std': float(np.std(criticalities)),
+            'coherence_mean': float(np.mean(coherences)),
+            'coherence_std': float(np.std(coherences)),
+            'smoothed_criticality': self.smoothed_criticality,
+            'smoothed_coherence': self.smoothed_coherence,
+            'trend_summary': self.get_trend_summary()
         }
 
-    def reset_buffer(self) :.3f}+/-{stats['criticality_std'], "
-              f"trend={self.trend.d_criticality_dt, "
-              f"bias={self.current_bias)
+    def reset_buffer(self):
+        """
+        Reset memory buffer (FR-007)
+
+        Clears all stored frames
+        """
+        self.buffer.clear()
+        self.total_frames = 0
+        self.current_bias = 0.0
+        self.trend = TrendVector()
+        self.prediction_errors.clear()
+        self.bias_history.clear()
+
+        # Reset smoothed values to neutral
+        self.smoothed_criticality = 1.0
+        self.smoothed_coherence = 0.0
+        self.smoothed_phi_depth = 0.5
+
+        print("[StateMemory] Buffer reset")
+
+    def _log_stats(self):
+        """Log memory statistics"""
+        if len(self.buffer) == 0:
+            return
+
+        stats = self.get_statistics()
+
+        print(f"[StateMemory] Stats: "
+              f"buffer={stats['buffer_size']}/{self.config.buffer_size}, "
+              f"criticality={stats['criticality_mean']:.3f}+/-{stats['criticality_std']:.3f}, "
+              f"trend={self.trend.d_criticality_dt:.4f}, "
+              f"bias={self.current_bias:.3f}")
 
-    def export_buffer(self) :
+    def export_buffer(self) -> Dict:
         """
         Export buffer contents for analysis
 
-        Returns) == 0:
+        Returns:
+            Dictionary with time-series data
+        """
+        if len(self.buffer) == 0:
             return {
-                'timestamps',
-                'criticality',
-                'coherence',
-                'ici',
-                'phi_depth',
+                'timestamps': [],
+                'criticality': [],
+                'coherence': [],
+                'ici': [],
+                'phi_depth': [],
                 'phi_phase': []
             }
 
         return {
-            'timestamps',
-            'criticality',
-            'coherence',
-            'ici',
-            'phi_depth',
-            'phi_phase',
-            'bias_history': self.bias_history[-256) -> None)
-    logger.info("StateMemory Self-Test")
-    logger.info("=" * 60)
-
-    # Test 1)
+            'timestamps': [f.timestamp for f in self.buffer],
+            'criticality': [f.criticality for f in self.buffer],
+            'coherence': [f.coherence for f in self.buffer],
+            'ici': [f.ici for f in self.buffer],
+            'phi_depth': [f.phi_depth for f in self.buffer],
+            'phi_phase': [f.phi_phase for f in self.buffer],
+            'bias_history': self.bias_history[-256:]  # Last 256 bias values
+        }
+
+
+# Self-test function
+def _self_test():
+    """Test StateMemory"""
+    print("=" * 60)
+    print("StateMemory Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = StateMemoryConfig(enabled=True, buffer_size=256, trend_window=30)
     memory = StateMemory(config)
 
     assert memory.config.enabled == True
     assert memory.config.buffer_size == 256
     assert len(memory.buffer) == 0
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Test 2)
+    # Test 2: Add frames
+    print("\n2. Testing frame addition...")
 
     # Add some frames with increasing criticality
-    for i in range(50)) * 0.02
+    for i in range(50):
+        criticality = 0.9 + i * 0.01  # Increasing trend
+        coherence = 0.8 + np.random.randn() * 0.02
         ici = 1.5
         phi_depth = 0.5
         phi_phase = 0.3
 
         memory.add_frame(criticality, coherence, ici, phi_depth, phi_phase)
         time.sleep(0.01)
 
     assert len(memory.buffer) == 50
     assert memory.total_frames == 50
-    logger.info("   OK, buffer size = %s", len(memory.buffer))
+    print(f"   OK: Added 50 frames, buffer size = {len(memory.buffer)}")
 
-    # Test 3)
+    # Test 3: Trend computation
+    print("\n3. Testing trend computation...")
     stats = memory.get_statistics()
     trend_summary = stats['trend_summary']
 
-    logger.error("   d(criticality)/dt = %s", trend_summary['d_criticality_dt'])
-    logger.error("   Predicted criticality = %s", trend_summary['predicted_criticality'])
-    logger.info("   Confidence = %s", trend_summary['confidence'])
-    logger.info("   Current bias = %s", trend_summary['current_bias'])
+    print(f"   d(criticality)/dt = {trend_summary['d_criticality_dt']:.4f}")
+    print(f"   Predicted criticality = {trend_summary['predicted_criticality']:.3f}")
+    print(f"   Confidence = {trend_summary['confidence']:.3f}")
+    print(f"   Current bias = {trend_summary['current_bias']:.3f}")
 
     # Should detect upward trend
     assert trend_summary['d_criticality_dt'] > 0, "Should detect upward trend"
-    logger.info("   OK)
+    print("   OK: Trend detection working")
 
-    # Test 4)
+    # Test 4: Bias computation
+    print("\n4. Testing bias computation...")
 
     # Add frames that push toward hypersync
-    for i in range(30), 0.8, 1.5, 0.5, 0.3)
+    for i in range(30):
+        criticality = 1.0 + i * 0.02  # Rapidly increasing
+        memory.add_frame(criticality, 0.8, 1.5, 0.5, 0.3)
         time.sleep(0.01)
 
     bias = memory.get_bias()
-    logger.info("   Current bias, bias)
+    print(f"   Current bias: {bias:.3f}")
 
     # Should have negative bias to counteract upward trend
     assert bias < 0, "Should provide negative bias for hypersync trend"
-    logger.info("   OK)
+    print("   OK: Predictive bias working")
 
-    # Test 5)
+    # Test 5: Reset
+    print("\n5. Testing reset...")
     memory.reset_buffer()
     assert len(memory.buffer) == 0
     assert memory.current_bias == 0.0
-    logger.info("   OK)
+    print("   OK: Reset working")
 
-    # Test 6)
+    # Test 6: Disable
+    print("\n6. Testing enable/disable...")
     memory.set_enabled(False)
     assert memory.config.enabled == False
 
     # Should not add frames when disabled
     result = memory.add_frame(1.0, 0.8, 1.5, 0.5, 0.3)
     assert result == False
     assert len(memory.buffer) == 0
-    logger.info("   OK)
+    print("   OK: Enable/disable working")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/state_recorder.py b/server/state_recorder.py
index 12fa23a0e66d8e5fb1288a22e42fe37e019c3266..08454e4ba0deade74cf80d568cd2f0feb68ac17f 100644
--- a/server/state_recorder.py
+++ b/server/state_recorder.py
@@ -1,317 +1,457 @@
 """
 StateRecorder - Feature 014: Consciousness State Recorder & Timeline Playback
 
 Continuous recording of consciousness state data with persistence and metadata.
 
 Features:
-- FR-001, ICI, coherence, criticality) with timestamps
-
+- FR-001: Record all metrics (Phi, ICI, coherence, criticality) with timestamps
+- FR-002: Store session metadata (date, duration, preset, notes)
 - SC-001: Record at >= 30 Hz without data loss
 - Auto-segmentation for long recordings
 
 Requirements:
 - FR-001: System MUST record metrics with timestamps
 - FR-002: System MUST store session metadata
 - SC-001: All metrics recorded at >= 30 Hz without data loss
+- SC-004: CPU overhead during recording <= 10%
+"""
 
 import time
 import threading
 import json
 import os
 from typing import Optional, Dict, List
 from dataclasses import dataclass, asdict
 from datetime import datetime
 from pathlib import Path
 
 from .session_memory import SessionMemory, MetricSnapshot
 
 
 @dataclass
 class RecordingMetadata:
     """Metadata for a recording session"""
     session_id: str
     start_time: float
     end_time: Optional[float]
     duration: float
     sample_count: int
     sample_rate_hz: float
     preset_name: Optional[str]
     user_notes: Optional[str]
     software_version: str
     file_format_version: str
 
 
 class StateRecorder:
     """
     StateRecorder - Continuous recording of consciousness state data
 
-    Handles, SC-001)
-
+    Handles:
+    - Real-time metric recording (FR-001, SC-001)
+    - Persistent storage with metadata (FR-002)
     - Auto-segmentation for long recordings
-
+    - Buffered writes for performance (SC-004)
     """
 
     FILE_FORMAT_VERSION = "1.0"
     SOFTWARE_VERSION = "Soundlab 1.0"
     MAX_SEGMENT_DURATION = 3600.0  # 60 minutes per segment
 
-    def __init__(self, output_dir: str) :
+    def __init__(self, output_dir: str = "recordings"):
         """
         Initialize StateRecorder
 
         Args:
-            output_dir)
+            output_dir: Directory for recording files
+        """
+        self.output_dir = Path(output_dir)
         self.output_dir.mkdir(parents=True, exist_ok=True)
 
         # Current recording state
         self.is_recording = False
         self.session_memory = SessionMemory(max_samples=1000000)  # Large buffer
         self.session_id: Optional[str] = None
         self.start_time: Optional[float] = None
         self.preset_name: Optional[str] = None
-        self.user_notes)
+        self.user_notes: Optional[str] = None
+
+        # Auto-segmentation
+        self.segment_index = 0
+        self.last_segment_time = 0.0
+
+        # Performance tracking
+        self.write_count = 0
+        self.last_write_time = time.time()
         self.recording_rate_hz = 0.0
 
         # Threading
         self.lock = threading.Lock()
-        self.save_thread, session_id,
-                       preset_name,
-                       user_notes))
+        self.save_thread: Optional[threading.Thread] = None
+
+    def start_recording(self, session_id: Optional[str] = None,
+                       preset_name: Optional[str] = None,
+                       user_notes: Optional[str] = None):
+        """
+        Start a new recording session (FR-001)
 
         Args:
             session_id: Optional session identifier
             preset_name: Name of preset used
             user_notes: User notes for this session
         """
         with self.lock:
-            if self.is_recording)
+            if self.is_recording:
+                raise RuntimeError("Recording already in progress")
 
             # Generate session ID with timestamp
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
             self.session_id = session_id or f"session_{timestamp}"
             self.start_time = time.time()
             self.preset_name = preset_name
             self.user_notes = user_notes
 
             # Reset state
             self.segment_index = 0
             self.last_segment_time = self.start_time
             self.write_count = 0
 
             # Start recording in session memory
             self.session_memory.start_session(self.session_id)
             self.is_recording = True
 
-            logger.info("[StateRecorder] Started recording, self.session_id)
+            print(f"[StateRecorder] Started recording: {self.session_id}")
 
-    def stop_recording(self) :
+    def stop_recording(self) -> Optional[str]:
+        """
+        Stop recording and save to file (FR-002)
+
+        Returns:
             Path to saved file or None if failed
         """
         with self.lock:
-            if not self.is_recording)
+            if not self.is_recording:
+                return None
+
+            self.is_recording = False
+            self.session_memory.stop_session()
 
             # Save final segment
             filepath = self._save_segment()
 
-            logger.info("[StateRecorder] Stopped recording, self.session_id)
-            logger.info("[StateRecorder] Saved to, filepath)
+            print(f"[StateRecorder] Stopped recording: {self.session_id}")
+            print(f"[StateRecorder] Saved to: {filepath}")
 
             return filepath
 
-    def record_state(self, snapshot: MetricSnapshot) :
+    def record_state(self, snapshot: MetricSnapshot):
+        """
+        Record a state snapshot (FR-001, SC-001)
+
+        Args:
             snapshot: MetricSnapshot to record
         """
-        if not self.is_recording)
+        if not self.is_recording:
+            return
+
+        # Record to memory
+        self.session_memory.record_snapshot(snapshot)
 
         # Update performance metrics
         self.write_count += 1
         current_time = time.time()
         time_delta = current_time - self.last_write_time
 
-        if time_delta >= 1.0) >= self.MAX_SEGMENT_DURATION)
+        if time_delta >= 1.0:  # Update rate every second
+            self.recording_rate_hz = self.write_count / time_delta
+            self.write_count = 0
+            self.last_write_time = current_time
+
+        # Check for auto-segmentation
+        if (current_time - self.last_segment_time) >= self.MAX_SEGMENT_DURATION:
+            self._auto_segment()
 
-    def get_recording_status(self) :
+    def get_recording_status(self) -> Dict:
         """
         Get current recording status
 
         Returns:
             Dictionary with recording status
         """
         with self.lock:
             if not self.is_recording:
                 return {
-                    "is_recording",
-                    "session_id",
-                    "duration",
-                    "sample_count",
-                    "recording_rate_hz")
+                    "is_recording": False,
+                    "session_id": None,
+                    "duration": 0.0,
+                    "sample_count": 0,
+                    "recording_rate_hz": 0.0
+                }
+
+            current_time = time.time()
             duration = current_time - self.start_time if self.start_time else 0.0
 
             return {
-                "is_recording",
-                "session_id",
-                "duration",
-                "sample_count"),
-                "recording_rate_hz",
-                "segment_index") :
+                "is_recording": True,
+                "session_id": self.session_id,
+                "duration": duration,
+                "sample_count": self.session_memory.get_sample_count(),
+                "recording_rate_hz": self.recording_rate_hz,
+                "segment_index": self.segment_index
+            }
+
+    def _auto_segment(self):
+        """Auto-segment long recording into multiple files"""
+        print(f"[StateRecorder] Auto-segmenting at {self.segment_index + 1}")
+
+        # Save current segment
+        self._save_segment()
+
+        # Reset for next segment
+        self.segment_index += 1
+        self.last_segment_time = time.time()
+
+    def _save_segment(self) -> str:
         """
         Save current recording segment to file
 
         Returns:
             Path to saved file
         """
         # Generate filename
         if self.segment_index == 0:
             filename = f"{self.session_id}.json"
         else:
-            filename = f"{self.session_id}_seg{self.segment_index)
+            filename = f"{self.session_id}_seg{self.segment_index:03d}.json"
+
+        filepath = self.output_dir / filename
 
-        if len(samples) == 0)
+        # Get samples
+        samples = self.session_memory.get_all_samples()
+
+        if len(samples) == 0:
+            return str(filepath)
 
         # Compute stats
         stats = self.session_memory.compute_stats()
 
         # Create metadata
         current_time = time.time()
         metadata = RecordingMetadata(
             session_id=self.session_id,
             start_time=self.start_time,
             end_time=current_time,
             duration=current_time - self.start_time,
             sample_count=len(samples),
             sample_rate_hz=len(samples) / (current_time - self.start_time) if (current_time - self.start_time) > 0 else 0.0,
             preset_name=self.preset_name,
             user_notes=self.user_notes,
             software_version=self.SOFTWARE_VERSION,
             file_format_version=self.FILE_FORMAT_VERSION
+        )
 
         # Convert samples to dict format
         samples_dict = [asdict(s) for s in samples]
 
         # Create recording data
         recording_data = {
-            "metadata"),
-            "stats") if stats else {},
-            "samples")
+            "metadata": asdict(metadata),
+            "stats": asdict(stats) if stats else {},
+            "samples": samples_dict
+        }
+
+        # Write to file (in background thread if not already saving)
         self._write_to_file(filepath, recording_data)
 
         return str(filepath)
 
-    def _write_to_file(self, filepath: Path, data: Dict) :
+    def _write_to_file(self, filepath: Path, data: Dict):
+        """
+        Write data to file (buffered for performance)
+
+        Args:
             filepath: Path to write to
-            data)
-        try, 'w') as f, f, indent=2)
+            data: Data to write
+        """
+        # Write synchronously for now (could be async for better performance)
+        try:
+            with open(filepath, 'w') as f:
+                json.dump(data, f, indent=2)
         except Exception as e:
-            logger.error("[StateRecorder] Error writing file, e)
+            print(f"[StateRecorder] Error writing file: {e}")
 
-    def list_recordings(self) :
+    def list_recordings(self) -> List[Dict]:
         """
         List all available recordings
 
-        Returns):
-            try, 'r') as f)
+        Returns:
+            List of recording metadata
+        """
+        recordings = []
+
+        for file_path in self.output_dir.glob("*.json"):
+            try:
+                with open(file_path, 'r') as f:
+                    data = json.load(f)
                     metadata = data.get("metadata", {})
                     recordings.append({
-                        "filename",
-                        "session_id"),
-                        "start_time"),
-                        "duration"),
-                        "sample_count"),
-                        "preset_name"),
-                        "user_notes")
+                        "filename": file_path.name,
+                        "session_id": metadata.get("session_id"),
+                        "start_time": metadata.get("start_time"),
+                        "duration": metadata.get("duration"),
+                        "sample_count": metadata.get("sample_count"),
+                        "preset_name": metadata.get("preset_name"),
+                        "user_notes": metadata.get("user_notes")
                     })
             except Exception as e:
-                logger.error("[StateRecorder] Error reading %s, file_path, e)
+                print(f"[StateRecorder] Error reading {file_path}: {e}")
 
         # Sort by start time (newest first)
-        recordings.sort(key=lambda x, 0), reverse=True)
+        recordings.sort(key=lambda x: x.get("start_time", 0), reverse=True)
 
         return recordings
 
-    def load_recording(self, filename) :
+    def load_recording(self, filename: str) -> Optional[Dict]:
         """
         Load a recording from file
 
         Args:
             filename: Name of recording file
 
         Returns:
             Recording data or None if failed
         """
         filepath = self.output_dir / filename
 
-        try, 'r') as f)
+        try:
+            with open(filepath, 'r') as f:
+                return json.load(f)
         except Exception as e:
-            logger.error("[StateRecorder] Error loading recording, e)
+            print(f"[StateRecorder] Error loading recording: {e}")
             return None
 
-    def delete_recording(self, filename) :
+    def delete_recording(self, filename: str) -> bool:
         """
         Delete a recording file
 
         Args:
             filename: Name of recording file
 
         Returns:
             True if deleted successfully
         """
         filepath = self.output_dir / filename
 
-        try)
+        try:
+            filepath.unlink()
             return True
         except Exception as e:
-            logger.error("[StateRecorder] Error deleting recording, e)
+            print(f"[StateRecorder] Error deleting recording: {e}")
             return False
 
 
 # Self-test function
-@lru_cache(maxsize=128)
-def _self_test() : %s Hz (target)", actual_rate)
-    logger.info(str())
+def _self_test():
+    """Run basic self-test of StateRecorder"""
+    print("=" * 60)
+    print("StateRecorder Self-Test")
+    print("=" * 60)
+    print()
+
+    import numpy as np
+
+    # Create recorder
+    print("1. Creating StateRecorder...")
+    recorder = StateRecorder(output_dir="test_recordings")
+    print("   [OK] StateRecorder created")
+    print()
+
+    # Start recording
+    print("2. Starting recording...")
+    recorder.start_recording(
+        session_id="test_session",
+        preset_name="test_preset",
+        user_notes="Self-test recording"
+    )
+
+    status = recorder.get_recording_status()
+    print(f"   [OK] Recording started: {status['session_id']}")
+    print()
+
+    # Record samples at 30 Hz
+    print("3. Recording samples at 30 Hz for 2 seconds...")
+    sample_count = 60  # 2 seconds at 30 Hz
+    start_time = time.time()
+
+    for i in range(sample_count):
+        snapshot = MetricSnapshot(
+            timestamp=time.time(),
+            ici=0.5 + 0.1 * np.sin(i * 0.1),
+            coherence=0.6 + 0.05 * np.cos(i * 0.15),
+            criticality=0.4,
+            phi_value=1.0 + 0.2 * np.sin(i * 0.1),
+            phi_phase=i * 0.1,
+            phi_depth=0.5,
+            active_source="test"
+        )
+        recorder.record_state(snapshot)
+        time.sleep(1.0 / 30.0)  # 30 Hz
+
+    elapsed = time.time() - start_time
+    actual_rate = sample_count / elapsed
+
+    print(f"   [OK] Recorded {sample_count} samples in {elapsed:.2f}s")
+    print(f"   [OK] Actual rate: {actual_rate:.1f} Hz (target: 30 Hz)")
+    print()
 
     # Check status
-    logger.info("4. Checking recording status...")
+    print("4. Checking recording status...")
     status = recorder.get_recording_status()
-    logger.info("   Duration, status['duration'])
-    logger.info("   Sample count, status['sample_count'])
-    logger.info("   Recording rate, status['recording_rate_hz'])
-    logger.info("   [OK] Status retrieved")
-    logger.info(str())
+    print(f"   Duration: {status['duration']:.2f}s")
+    print(f"   Sample count: {status['sample_count']}")
+    print(f"   Recording rate: {status['recording_rate_hz']:.1f} Hz")
+    print("   [OK] Status retrieved")
+    print()
 
     # Stop recording
-    logger.info("5. Stopping recording...")
+    print("5. Stopping recording...")
     filepath = recorder.stop_recording()
-    logger.info("   [OK] Recording saved to, filepath)
-    logger.info(str())
+    print(f"   [OK] Recording saved to: {filepath}")
+    print()
 
     # List recordings
-    logger.info("6. Listing recordings...")
+    print("6. Listing recordings...")
     recordings = recorder.list_recordings()
-    logger.info("   [OK] Found %s recordings", len(recordings))
+    print(f"   [OK] Found {len(recordings)} recordings")
     for rec in recordings:
-        logger.info("      - %s, %s samples", rec['filename'], rec['duration'], rec['sample_count'])
-    logger.info(str())
+        print(f"      - {rec['filename']}: {rec['duration']:.1f}s, {rec['sample_count']} samples")
+    print()
 
     # Load recording
-    logger.info("7. Loading recording...")
+    print("7. Loading recording...")
     data = recorder.load_recording(recordings[0]['filename'])
-    if data, len(samples))
-        logger.info("      Session, metadata['session_id'])
-        logger.info("      Duration, metadata['duration'])
-        logger.info("      Sample rate, metadata['sample_rate_hz'])
-    logger.info(str())
+    if data:
+        metadata = data['metadata']
+        samples = data['samples']
+        print(f"   [OK] Loaded {len(samples)} samples")
+        print(f"      Session: {metadata['session_id']}")
+        print(f"      Duration: {metadata['duration']:.2f}s")
+        print(f"      Sample rate: {metadata['sample_rate_hz']:.1f} Hz")
+    print()
 
     # Cleanup
-    logger.info("8. Cleaning up...")
-    for rec in recordings)
-    logger.info("   [OK] Test recordings deleted")
-    logger.info(str())
+    print("8. Cleaning up...")
+    for rec in recordings:
+        recorder.delete_recording(rec['filename'])
+    print("   [OK] Test recordings deleted")
+    print()
 
-    logger.info("=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
+    print("=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
 
     return True
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/state_sync_manager.py b/server/state_sync_manager.py
index a13725390b6f94628511452fc948e806df383a73..5de95356e601ce958c88f04ccda90791ac428df3 100644
--- a/server/state_sync_manager.py
+++ b/server/state_sync_manager.py
@@ -1,40 +1,42 @@
 """
 StateSyncManager - Feature 017: Phi-Matrix Dashboard
 
 Manages synchronized state across all dashboard modules with shared clock
 and bidirectional WebSocket communication.
 
 Features:
 - FR-002: Synchronized clock/timestamp base across all modules
-
+- FR-003: Bidirectional WebSocket router (< 50 ms avg)
 - SC-001: Dashboard operational at <= 100 ms latency
 - SC-004: No metric/UI desync > 0.1 s for > 60 s runs
 
 Requirements:
 - FR-002: All modules share synchronized clock
 - FR-003: WebSocket messages < 50 ms average
+- SC-004: Maintain sync < 0.1 s desync for 60+ s
+"""
 
 import time
 import asyncio
 import threading
 from typing import Dict, List, Optional, Callable, Any
 from dataclasses import dataclass, asdict
 from collections import deque
 import json
 
 
 @dataclass
 class SyncState:
     """Synchronized state across all dashboard modules"""
     timestamp: float           # Master clock timestamp
 
     # Metrics state
     ici: float
     coherence: float
     criticality: float
 
     # Phi state
     phi_phase: float
     phi_depth: float
     phi_breathing: float
 
@@ -48,371 +50,501 @@ class SyncState:
     adaptive_enabled: bool
     adaptive_mode: Optional[str]
 
     # Session state
     is_recording: bool
     is_playing: bool
 
     # Cluster state
     cluster_nodes_count: int
 
 
 @dataclass
 class SyncConfig:
     """Configuration for StateSyncManager"""
     max_latency_ms: float = 100.0          # SC-001: Max acceptable latency
     max_desync_ms: float = 100.0           # SC-004: Max desync tolerance
     websocket_timeout_ms: float = 50.0     # FR-003: WebSocket message timeout
     sync_check_interval_s: float = 1.0     # How often to check sync
     enable_logging: bool = False
 
 
 class StateSyncManager:
     """
     StateSyncManager - Central state synchronization manager
 
-
-
-
+    Handles:
+    - Unified clock synchronization (FR-002)
+    - Bidirectional WebSocket routing (FR-003)
+    - Pause/resume coordination (User Story 2)
+    - Latency monitoring (SC-001, SC-004)
     """
 
-    def __init__(self, config: Optional[SyncConfig]) : Optional[float] = None
+    def __init__(self, config: Optional[SyncConfig] = None):
+        """Initialize StateSyncManager"""
+        self.config = config or SyncConfig()
+
+        # Master clock
+        self.start_time = time.time()
+        self.paused = False
+        self.pause_time: Optional[float] = None
         self.pause_offset = 0.0
 
         # Current synchronized state
-        self.current_state)
+        self.current_state: Optional[SyncState] = None
+        self.state_lock = threading.Lock()
 
         # State history for desync detection
         self.state_history = deque(maxlen=100)
 
         # WebSocket clients
-        self.ws_clients)
+        self.ws_clients: List = []
+        self.ws_lock = asyncio.Lock()
 
         # Latency tracking
         self.message_latencies = deque(maxlen=100)
         self.last_message_time = time.time()
 
         # Desync tracking
-        self.desync_events)
+        self.desync_events: List[Dict] = []
+        self.last_sync_check = time.time()
 
         # Background monitoring
         self.is_running = False
-        self.monitor_thread) :
+        self.monitor_thread: Optional[threading.Thread] = None
+
+    def get_master_time(self) -> float:
+        """
+        Get master clock timestamp (FR-002)
+
+        Returns:
             Master timestamp accounting for pause offset
         """
         if self.paused and self.pause_time:
             return self.pause_time - self.start_time - self.pause_offset
-        else) - self.start_time - self.pause_offset
+        else:
+            return time.time() - self.start_time - self.pause_offset
 
-    def pause(self) :
+    def pause(self):
+        """Pause all synchronized modules (User Story 2)"""
+        if not self.paused:
+            self.paused = True
+            self.pause_time = time.time()
+
+            if self.config.enable_logging:
+                print(f"[StateSyncManager] Paused at master time {self.get_master_time():.3f}s")
+
+    def resume(self):
+        """Resume all synchronized modules (User Story 2)"""
+        if self.paused and self.pause_time:
+            pause_duration = time.time() - self.pause_time
+            self.pause_offset += pause_duration
+            self.paused = False
+            self.pause_time = None
+
+            if self.config.enable_logging:
+                print(f"[StateSyncManager] Resumed after {pause_duration:.3f}s pause")
+
+    def update_state(self, **kwargs):
+        """
+        Update synchronized state (FR-002)
+
+        Args:
             **kwargs: State fields to update
         """
         with self.state_lock:
             # Get current state or create new
-            if self.current_state is None),
+            if self.current_state is None:
+                # Initialize with defaults
+                self.current_state = SyncState(
+                    timestamp=self.get_master_time(),
                     ici=0.5,
                     coherence=0.5,
                     criticality=1.0,
                     phi_phase=0.0,
                     phi_depth=1.0,
                     phi_breathing=0.5,
                     chromatic_enabled=False,
                     control_matrix_active=False,
                     adaptive_enabled=False,
                     adaptive_mode=None,
                     is_recording=False,
                     is_playing=False,
                     cluster_nodes_count=0
+                )
 
             # Update timestamp
             self.current_state.timestamp = self.get_master_time()
 
             # Update provided fields
-            for key, value in kwargs.items(), key), key, value)
+            for key, value in kwargs.items():
+                if hasattr(self.current_state, key):
+                    setattr(self.current_state, key, value)
 
             # Add to history
             self.state_history.append(self.current_state)
 
-    def get_state(self) :
+    def get_state(self) -> Optional[Dict]:
         """
         Get current synchronized state
 
         Returns:
             State dictionary or None
         """
         with self.state_lock:
-            if self.current_state)
+            if self.current_state:
+                return asdict(self.current_state)
             return None
 
-    async def register_client(self, websocket))
+    async def register_client(self, websocket):
+        """
+        Register WebSocket client (FR-003)
 
         Args:
             websocket: WebSocket connection
         """
-        async with self.ws_lock)
+        async with self.ws_lock:
+            self.ws_clients.append(websocket)
 
-            if self.config.enable_logging)", len(self.ws_clients))
+            if self.config.enable_logging:
+                print(f"[StateSyncManager] Client registered ({len(self.ws_clients)} total)")
 
     async def unregister_client(self, websocket):
         """
         Unregister WebSocket client
 
         Args:
             websocket: WebSocket connection
         """
         async with self.ws_lock:
-            if websocket in self.ws_clients)
+            if websocket in self.ws_clients:
+                self.ws_clients.remove(websocket)
 
-                if self.config.enable_logging)", len(self.ws_clients))
+                if self.config.enable_logging:
+                    print(f"[StateSyncManager] Client unregistered ({len(self.ws_clients)} total)")
 
-    async def broadcast_state(self), SC-001)
+    async def broadcast_state(self):
+        """
+        Broadcast current state to all WebSocket clients (FR-003, SC-001)
         """
         state = self.get_state()
         if not state:
             return
 
         message = {
-            "type",
-            "data",
-            "server_time")
+            "type": "state_sync",
+            "data": state,
+            "server_time": time.time()
         }
 
         async with self.ws_lock:
             disconnected = []
 
             for client in self.ws_clients:
-                try)
+                try:
+                    send_start = time.time()
                     await asyncio.wait_for(
                         client.send_json(message),
                         timeout=self.config.websocket_timeout_ms / 1000.0
+                    )
 
                     # Track latency
                     latency_ms = (time.time() - send_start) * 1000.0
                     self.message_latencies.append(latency_ms)
 
                 except asyncio.TimeoutError:
-                    if self.config.enable_logging)
+                    if self.config.enable_logging:
+                        print(f"[StateSyncManager] WebSocket send timeout")
                     disconnected.append(client)
                 except Exception as e:
                     if self.config.enable_logging:
-                        logger.error("[StateSyncManager] WebSocket send error, e)
+                        print(f"[StateSyncManager] WebSocket send error: {e}")
                     disconnected.append(client)
 
             # Remove disconnected clients
             for client in disconnected:
-                if client in self.ws_clients)
+                if client in self.ws_clients:
+                    self.ws_clients.remove(client)
 
-    async def handle_client_message(self, websocket, message), User Story 3)
+    async def handle_client_message(self, websocket, message: Dict):
+        """
+        Handle incoming message from client (FR-003, User Story 3)
 
         Args:
             websocket: Client WebSocket
             message: Message dictionary
 
+        Returns:
+            Response dictionary
+        """
+        receive_time = time.time()
         msg_type = message.get("type")
 
         if msg_type == "ping":
             # Latency check
             return {
-                "type",
-                "server_time"),
-                "client_time", 0)
+                "type": "pong",
+                "server_time": time.time(),
+                "client_time": message.get("client_time", 0)
             }
 
-        elif msg_type == "pause")
+        elif msg_type == "pause":
+            # Pause request (User Story 2)
             self.pause()
             return {
-                "type",
-                "master_time")
+                "type": "pause_ack",
+                "master_time": self.get_master_time()
             }
 
-        elif msg_type == "resume")
+        elif msg_type == "resume":
+            # Resume request (User Story 2)
             self.resume()
             return {
-                "type",
-                "master_time")
+                "type": "resume_ack",
+                "master_time": self.get_master_time()
             }
 
         elif msg_type == "get_state":
             # State request
             return {
-                "type",
-                "data")
+                "type": "state_response",
+                "data": self.get_state()
             }
 
         else:
             return {
-                "type",
-                "message": f"Unknown message type)
-    def get_latency_stats(self) :
+                "type": "error",
+                "message": f"Unknown message type: {msg_type}"
+            }
+
+    def get_latency_stats(self) -> Dict:
+        """
+        Get WebSocket latency statistics (FR-003, SC-001)
+
+        Returns:
             Latency stats dictionary
         """
         if not self.message_latencies:
             return {
-                "avg_latency_ms",
-                "max_latency_ms",
-                "min_latency_ms",
-                "meets_sc001",
-                "meets_fr003")
+                "avg_latency_ms": 0.0,
+                "max_latency_ms": 0.0,
+                "min_latency_ms": 0.0,
+                "meets_sc001": True,
+                "meets_fr003": True
+            }
+
+        import numpy as np
+        latencies = list(self.message_latencies)
 
         avg_latency = np.mean(latencies)
         max_latency = np.max(latencies)
         min_latency = np.min(latencies)
 
         return {
-            "avg_latency_ms"),
-            "max_latency_ms"),
-            "min_latency_ms"),
-            "meets_sc001",  # SC-001
-            "meets_fr003") :
+            "avg_latency_ms": float(avg_latency),
+            "max_latency_ms": float(max_latency),
+            "min_latency_ms": float(min_latency),
+            "meets_sc001": max_latency <= self.config.max_latency_ms,  # SC-001
+            "meets_fr003": avg_latency <= self.config.websocket_timeout_ms  # FR-003
+        }
+
+    def check_sync_health(self) -> Dict:
+        """
+        Check synchronization health (SC-004)
+
+        Returns:
+            Sync health report
+        """
+        current_time = time.time()
+        time_since_check = current_time - self.last_sync_check
+
+        # Check for recent desync events
+        recent_desyncs = [
+            d for d in self.desync_events
+            if (current_time - d['timestamp']) < 60.0
+        ]
+
+        # Check state update frequency
+        if len(self.state_history) >= 2:
+            timestamps = [s.timestamp for s in list(self.state_history)]
+            diffs = [timestamps[i+1] - timestamps[i] for i in range(len(timestamps)-1)]
+
+            import numpy as np
+            max_diff = np.max(diffs) if diffs else 0.0
+            avg_diff = np.mean(diffs) if diffs else 0.0
+        else:
             max_diff = 0.0
             avg_diff = 0.0
 
-        # SC-004)
+        # SC-004: No desync > 0.1s
+        meets_sc004 = max_diff <= (self.config.max_desync_ms / 1000.0)
 
         self.last_sync_check = current_time
 
         return {
-            "is_paused",
-            "master_time"),
-            "recent_desyncs"),
-            "max_state_diff_ms",
-            "avg_state_diff_ms",
-            "meets_sc004",
-            "active_clients")
+            "is_paused": self.paused,
+            "master_time": self.get_master_time(),
+            "recent_desyncs": len(recent_desyncs),
+            "max_state_diff_ms": max_diff * 1000.0,
+            "avg_state_diff_ms": avg_diff * 1000.0,
+            "meets_sc004": meets_sc004,
+            "active_clients": len(self.ws_clients)
         }
 
-    def start_monitoring(self) :
+    def start_monitoring(self):
         """Start background sync monitoring"""
-        if self.is_running, daemon=True)
+        if self.is_running:
+            return
+
+        self.is_running = True
+        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
         self.monitor_thread.start()
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[StateSyncManager] Monitoring started")
 
-    def stop_monitoring(self) :
+    def stop_monitoring(self):
         """Stop background monitoring"""
         self.is_running = False
 
-        if self.monitor_thread)
+        if self.monitor_thread:
+            self.monitor_thread.join(timeout=2.0)
             self.monitor_thread = None
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[StateSyncManager] Monitoring stopped")
 
-    def _monitor_loop(self) :
+    def _monitor_loop(self):
         """Background monitoring loop"""
         while self.is_running:
-            try)
+            try:
+                # Check sync health
+                health = self.check_sync_health()
 
                 # Log warnings
                 if not health['meets_sc004'] and self.config.enable_logging:
-                    logger.warning("[StateSyncManager] WARNING: Desync detected, health['max_state_diff_ms'])
+                    print(f"[StateSyncManager] WARNING: Desync detected: {health['max_state_diff_ms']:.1f}ms")
 
                 # Sleep
                 time.sleep(self.config.sync_check_interval_s)
 
             except Exception as e:
                 if self.config.enable_logging:
-                    logger.error("[StateSyncManager] Monitor error, e)
+                    print(f"[StateSyncManager] Monitor error: {e}")
 
 
 # Self-test
-def _self_test() -> None)
-    logger.info("StateSyncManager Self-Test")
-    logger.info("=" * 60)
-    logger.info(str())
+def _self_test():
+    """Run basic self-test of StateSyncManager"""
+    print("=" * 60)
+    print("StateSyncManager Self-Test")
+    print("=" * 60)
+    print()
 
     all_ok = True
 
-    # Test 1)...")
+    # Test 1: Master clock
+    print("1. Testing Master Clock (FR-002)...")
     manager = StateSyncManager(SyncConfig(enable_logging=True))
 
     t1 = manager.get_master_time()
     time.sleep(0.1)
     t2 = manager.get_master_time()
 
     time_ok = 0.09 < (t2 - t1) < 0.11
     all_ok = all_ok and time_ok
 
-    logger.info("   Time delta)", t2 - t1)
-    logger.error("   [%s] Master clock (FR-002)", 'OK' if time_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Time delta: {t2 - t1:.3f}s (expected ~0.1s)")
+    print(f"   [{'OK' if time_ok else 'FAIL'}] Master clock (FR-002)")
+    print()
 
-    # Test 2)...")
+    # Test 2: Pause/Resume
+    print("2. Testing Pause/Resume (User Story 2)...")
     t_before_pause = manager.get_master_time()
     manager.pause()
 
     time.sleep(0.2)
 
     t_during_pause = manager.get_master_time()
     pause_ok = abs(t_during_pause - t_before_pause) < 0.01
 
     manager.resume()
     time.sleep(0.1)
 
     t_after_resume = manager.get_master_time()
     resume_ok = 0.09 < (t_after_resume - t_during_pause) < 0.11
 
     all_ok = all_ok and pause_ok and resume_ok
 
-    logger.info("   Pause)", t_during_pause - t_before_pause)
-    logger.info("   Resume)", t_after_resume - t_during_pause)
-    logger.error("   [%s] Pause/Resume", 'OK' if pause_ok and resume_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Pause: {t_during_pause - t_before_pause:.3f}s (expected ~0s)")
+    print(f"   Resume: {t_after_resume - t_during_pause:.3f}s (expected ~0.1s)")
+    print(f"   [{'OK' if pause_ok and resume_ok else 'FAIL'}] Pause/Resume")
+    print()
 
-    # Test 3)
+    # Test 3: State update
+    print("3. Testing State Update...")
     manager.update_state(
         ici=0.52,
         coherence=0.88,
         phi_depth=1.2
+    )
 
     state = manager.get_state()
     state_ok = (
         state is not None and
         state['ici'] == 0.52 and
         state['coherence'] == 0.88 and
         state['phi_depth'] == 1.2
+    )
 
     all_ok = all_ok and state_ok
 
-    logger.info("   State, Coherence=%s", state['ici'], state['coherence'])
-    logger.error("   [%s] State update", 'OK' if state_ok else 'FAIL')
-    logger.info(str())
+    print(f"   State: ICI={state['ici']:.2f}, Coherence={state['coherence']:.2f}")
+    print(f"   [{'OK' if state_ok else 'FAIL'}] State update")
+    print()
 
-    # Test 4)...")
+    # Test 4: Sync health
+    print("4. Testing Sync Health (SC-004)...")
 
     # Generate some state updates
-    for i in range(10))
+    for i in range(10):
+        manager.update_state(ici=0.5 + i*0.01)
         time.sleep(0.01)
 
     health = manager.check_sync_health()
     health_ok = health['meets_sc004']
 
     all_ok = all_ok and health_ok
 
-    logger.info("   Max state diff, health['max_state_diff_ms'])
-    logger.info("   Avg state diff, health['avg_state_diff_ms'])
-    logger.error("   [%s] Sync health (SC-004)", 'OK' if health_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Max state diff: {health['max_state_diff_ms']:.2f}ms")
+    print(f"   Avg state diff: {health['avg_state_diff_ms']:.2f}ms")
+    print(f"   [{'OK' if health_ok else 'FAIL'}] Sync health (SC-004: < 100ms)")
+    print()
 
-    # Test 5)...")
+    # Test 5: Latency tracking
+    print("5. Testing Latency Tracking (FR-003)...")
 
     # Simulate message latencies
     manager.message_latencies.extend([5.0, 10.0, 8.0, 12.0, 7.0])
 
     latency_stats = manager.get_latency_stats()
     latency_ok = latency_stats['meets_fr003']
 
     all_ok = all_ok and latency_ok
 
-    logger.info("   Avg latency, latency_stats['avg_latency_ms'])
-    logger.info("   Max latency, latency_stats['max_latency_ms'])
-    logger.error("   [%s] Latency tracking (FR-003)", 'OK' if latency_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Avg latency: {latency_stats['avg_latency_ms']:.2f}ms")
+    print(f"   Max latency: {latency_stats['max_latency_ms']:.2f}ms")
+    print(f"   [{'OK' if latency_ok else 'FAIL'}] Latency tracking (FR-003: < 50ms)")
+    print()
 
-    logger.info("=" * 60)
-    if all_ok)
-    else)
-    logger.info("=" * 60)
+    print("=" * 60)
+    if all_ok:
+        print("Self-Test PASSED")
+    else:
+        print("Self-Test FAILED - Review failures above")
+    print("=" * 60)
 
     return all_ok
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/sync_profiler.py b/server/sync_profiler.py
index a23633d11cc8d6cf32444572d752843c077d2d66..85253bfd638661d26519dd37f2cdb81730acf33c 100644
--- a/server/sync_profiler.py
+++ b/server/sync_profiler.py
@@ -1,314 +1,458 @@
 """
 SyncProfiler - Feature 018: Phi-Adaptive Benchmark
 
 Measures WebSocket round-trip latency and synchronization health by injecting
 ping/pong timestamps into the WebSocket communication loop.
 
 Features:
 - FR-002: Collect latency metrics
-
+- SC-001: Round-trip latency <= 100 ms (max)
 - SC-003: Interaction delay <= 50 ms
 - User Story 2: Latency & Sync Benchmark
 
-Requirements,000 frames
+Requirements:
+- Log round-trip time across 10,000 frames
 - Avg latency <= 50ms, max <= 100ms
 """
-from functools import lru_cache
-import logging
-logger = logging.getLogger(__name__)
-
 
 import time
 import asyncio
 from typing import Optional, Dict, List
 from dataclasses import dataclass
 from collections import deque
 import numpy as np
 
 
 @dataclass
 class LatencyMeasurement:
     """Single latency measurement"""
     timestamp: float
     round_trip_ms: float
     one_way_ms: float
     jitter_ms: float
 
 
 @dataclass
 class ProfilerConfig:
     """Configuration for SyncProfiler"""
     max_samples: int = 10000
     ping_interval_ms: float = 100.0  # Send ping every 100ms
     latency_threshold_ms: float = 100.0
     jitter_threshold_ms: float = 20.0
     enable_logging: bool = False
 
 
 class SyncProfiler:
     """
     SyncProfiler - WebSocket latency and sync measurement
 
     Injects ping/pong messages into WebSocket communication to measure
     round-trip latency and detect synchronization drift.
 
-    Features, config: Optional[ProfilerConfig]) :
+    Features:
+    - Continuous latency monitoring
+    - Jitter calculation
+    - Clock drift detection
+    - Statistical analysis
+    """
+
+    def __init__(self, config: Optional[ProfilerConfig] = None):
+        """Initialize SyncProfiler"""
+        self.config = config or ProfilerConfig()
+
+        # Latency measurements
+        self.measurements: deque = deque(maxlen=self.config.max_samples)
+
+        # Ping/pong state
+        self.pending_pings: Dict[int, float] = {}  # ping_id -> send_time
+        self.next_ping_id = 0
+
+        # Statistics
+        self.total_pings_sent = 0
+        self.total_pongs_received = 0
+        self.total_timeouts = 0
+
+        # Clock drift tracking
+        self.clock_offset_ms = 0.0
+        self.clock_drift_samples = deque(maxlen=100)
+
+    def create_ping_message(self) -> Dict:
         """
         Create a ping message with timestamp
 
+        Returns:
+            Ping message dictionary
+        """
+        ping_id = self.next_ping_id
+        self.next_ping_id += 1
+
+        send_time = time.time()
         self.pending_pings[ping_id] = send_time
         self.total_pings_sent += 1
 
         return {
-            "type",
-            "ping_id",
-            "client_time")
-    def handle_pong_message(self, pong_msg: Dict) :
+            "type": "ping",
+            "ping_id": ping_id,
+            "client_time": send_time * 1000.0  # Convert to ms
+        }
+
+    def handle_pong_message(self, pong_msg: Dict):
         """
         Handle pong message and calculate latency
 
         Args:
-            pong_msg)
+            pong_msg: Pong message from server
+        """
+        receive_time = time.time()
         ping_id = pong_msg.get("ping_id")
         server_time = pong_msg.get("server_time", 0) / 1000.0  # Convert from ms
         client_send_time = pong_msg.get("client_time", 0) / 1000.0
 
         if ping_id not in self.pending_pings:
             if self.config.enable_logging:
-                logger.info("[SyncProfiler] Received pong for unknown ping_id, ping_id)
+                print(f"[SyncProfiler] Received pong for unknown ping_id: {ping_id}")
             return
 
         # Get send time
         send_time = self.pending_pings.pop(ping_id)
         self.total_pongs_received += 1
 
         # Calculate round-trip time
         round_trip_s = receive_time - send_time
         round_trip_ms = round_trip_s * 1000.0
 
         # Estimate one-way latency (half of round-trip)
         one_way_ms = round_trip_ms / 2.0
 
         # Calculate jitter (change from previous measurement)
         jitter_ms = 0.0
-        if len(self.measurements) > 0)
+        if len(self.measurements) > 0:
+            prev_measurement = self.measurements[-1]
+            jitter_ms = abs(round_trip_ms - prev_measurement.round_trip_ms)
 
         # Calculate clock offset
-        if server_time > 0)
+        if server_time > 0:
+            # Estimate server time at client receive
+            estimated_server_time = server_time + (one_way_ms / 1000.0)
             clock_offset = (receive_time - estimated_server_time) * 1000.0
             self.clock_drift_samples.append(clock_offset)
             self.clock_offset_ms = float(np.mean(list(self.clock_drift_samples)))
 
         # Create measurement
         measurement = LatencyMeasurement(
             timestamp=receive_time,
             round_trip_ms=round_trip_ms,
             one_way_ms=one_way_ms,
             jitter_ms=jitter_ms
+        )
 
         self.measurements.append(measurement)
 
         if self.config.enable_logging:
-            print(f"[SyncProfiler] RTT: {round_trip_ms, "
-                  f"One-way: {one_way_ms, "
-                  f"Jitter: {jitter_ms)
+            print(f"[SyncProfiler] RTT: {round_trip_ms:.2f}ms, "
+                  f"One-way: {one_way_ms:.2f}ms, "
+                  f"Jitter: {jitter_ms:.2f}ms")
 
-    def cleanup_pending_pings(self, timeout_s: float) :
+    def cleanup_pending_pings(self, timeout_s: float = 1.0):
         """
         Remove pending pings that have timed out
 
         Args:
-            timeout_s)
+            timeout_s: Timeout in seconds
+        """
+        current_time = time.time()
         timed_out = []
 
-        for ping_id, send_time in self.pending_pings.items()) > timeout_s)
+        for ping_id, send_time in self.pending_pings.items():
+            if (current_time - send_time) > timeout_s:
+                timed_out.append(ping_id)
 
-        for ping_id in timed_out)
+        for ping_id in timed_out:
+            self.pending_pings.pop(ping_id)
             self.total_timeouts += 1
 
-            if self.config.enable_logging, ping_id)
+            if self.config.enable_logging:
+                print(f"[SyncProfiler] Ping {ping_id} timed out")
 
-    def get_statistics(self) :
+    def get_statistics(self) -> Dict:
         """
         Get latency statistics
 
         Returns:
             Statistics dictionary
         """
         if not self.measurements:
             return {
-                "total_samples",
-                "avg_latency_ms",
-                "min_latency_ms",
-                "max_latency_ms",
-                "p50_latency_ms",
-                "p95_latency_ms",
-                "p99_latency_ms",
-                "avg_jitter_ms",
-                "max_jitter_ms",
-                "clock_offset_ms",
-                "total_pings_sent",
-                "total_pongs_received",
-                "total_timeouts",
-                "success_rate",
-                "meets_sc001",
-                "meets_sc003")
+                "total_samples": 0,
+                "avg_latency_ms": 0.0,
+                "min_latency_ms": 0.0,
+                "max_latency_ms": 0.0,
+                "p50_latency_ms": 0.0,
+                "p95_latency_ms": 0.0,
+                "p99_latency_ms": 0.0,
+                "avg_jitter_ms": 0.0,
+                "max_jitter_ms": 0.0,
+                "clock_offset_ms": 0.0,
+                "total_pings_sent": self.total_pings_sent,
+                "total_pongs_received": self.total_pongs_received,
+                "total_timeouts": self.total_timeouts,
+                "success_rate": 0.0,
+                "meets_sc001": False,
+                "meets_sc003": False
+            }
+
+        measurements = list(self.measurements)
         latencies = [m.round_trip_ms for m in measurements]
         jitters = [m.jitter_ms for m in measurements]
 
         avg_latency = float(np.mean(latencies))
         min_latency = float(np.min(latencies))
         max_latency = float(np.max(latencies))
         p50_latency = float(np.percentile(latencies, 50))
         p95_latency = float(np.percentile(latencies, 95))
         p99_latency = float(np.percentile(latencies, 99))
 
         avg_jitter = float(np.mean(jitters))
         max_jitter = float(np.max(jitters))
 
         success_rate = 0.0
-        if self.total_pings_sent > 0) * 100.0
+        if self.total_pings_sent > 0:
+            success_rate = (self.total_pongs_received / self.total_pings_sent) * 100.0
 
         # Check success criteria
         meets_sc001 = max_latency <= 100.0  # SC-001: Max RTT <= 100ms
         meets_sc003 = avg_latency / 2.0 <= 50.0  # SC-003: Avg one-way <= 50ms
 
         return {
-            "total_samples"),
-            "avg_latency_ms",
-            "min_latency_ms",
-            "max_latency_ms",
-            "p50_latency_ms",
-            "p95_latency_ms",
-            "p99_latency_ms",
-            "avg_jitter_ms",
-            "max_jitter_ms",
-            "clock_offset_ms",
-            "total_pings_sent",
-            "total_pongs_received",
-            "total_timeouts",
-            "success_rate",
-            "meets_sc001",
-            "meets_sc003", bucket_size_ms) :
+            "total_samples": len(measurements),
+            "avg_latency_ms": avg_latency,
+            "min_latency_ms": min_latency,
+            "max_latency_ms": max_latency,
+            "p50_latency_ms": p50_latency,
+            "p95_latency_ms": p95_latency,
+            "p99_latency_ms": p99_latency,
+            "avg_jitter_ms": avg_jitter,
+            "max_jitter_ms": max_jitter,
+            "clock_offset_ms": self.clock_offset_ms,
+            "total_pings_sent": self.total_pings_sent,
+            "total_pongs_received": self.total_pongs_received,
+            "total_timeouts": self.total_timeouts,
+            "success_rate": success_rate,
+            "meets_sc001": meets_sc001,
+            "meets_sc003": meets_sc003
+        }
+
+    def get_latency_distribution(self, bucket_size_ms: float = 10.0) -> Dict:
         """
         Get latency distribution histogram
 
         Args:
             bucket_size_ms: Size of each histogram bucket in ms
 
         Returns:
             Distribution dictionary
         """
         if not self.measurements:
             return {
-                "buckets",
-                "counts",
-                "percentages")
+                "buckets": [],
+                "counts": [],
+                "percentages": []
+            }
+
+        latencies = [m.round_trip_ms for m in self.measurements]
+        max_latency = max(latencies)
 
         # Create buckets
         num_buckets = int(np.ceil(max_latency / bucket_size_ms))
         buckets = [i * bucket_size_ms for i in range(num_buckets + 1)]
 
         # Calculate histogram
         counts, _ = np.histogram(latencies, bins=buckets)
         percentages = (counts / len(latencies)) * 100.0
 
         return {
-            "buckets": [f"{buckets[i]:.0f}-{buckets[i+1])-1)],
-            "counts"),
-            "percentages")
+            "buckets": [f"{buckets[i]:.0f}-{buckets[i+1]:.0f}ms" for i in range(len(buckets)-1)],
+            "counts": counts.tolist(),
+            "percentages": percentages.tolist()
         }
 
-    def reset(self) :
+    def reset(self):
+        """Reset all measurements and statistics"""
+        self.measurements.clear()
+        self.pending_pings.clear()
+        self.clock_drift_samples.clear()
+        self.next_ping_id = 0
+        self.total_pings_sent = 0
+        self.total_pongs_received = 0
+        self.total_timeouts = 0
+        self.clock_offset_ms = 0.0
+
+
+# Self-test
+def _self_test():
+    """Run basic self-test of SyncProfiler"""
+    print("=" * 60)
+    print("SyncProfiler Self-Test")
+    print("=" * 60)
+    print()
+
+    all_ok = True
+
+    # Test 1: Initialization
+    print("1. Testing Initialization...")
+    config = ProfilerConfig(enable_logging=True)
+    profiler = SyncProfiler(config)
+
+    init_ok = profiler.total_pings_sent == 0
+    all_ok = all_ok and init_ok
+
+    print(f"   Initial state: pings={profiler.total_pings_sent}, pongs={profiler.total_pongs_received}")
+    print(f"   [{'OK' if init_ok else 'FAIL'}] Initialization")
+    print()
+
+    # Test 2: Create ping message
+    print("2. Testing Ping Message Creation...")
+    ping_msg = profiler.create_ping_message()
+
+    ping_ok = (
+        ping_msg['type'] == 'ping' and
+        'ping_id' in ping_msg and
+        'client_time' in ping_msg and
+        profiler.total_pings_sent == 1
+    )
+
+    all_ok = all_ok and ping_ok
+
+    print(f"   Ping message: {ping_msg}")
+    print(f"   Total pings sent: {profiler.total_pings_sent}")
+    print(f"   [{'OK' if ping_ok else 'FAIL'}] Ping creation")
+    print()
+
+    # Test 3: Handle pong message
+    print("3. Testing Pong Message Handling...")
+
+    # Simulate server response
+    time.sleep(0.05)  # Simulate 50ms latency
+    pong_msg = {
+        "type": "pong",
+        "ping_id": ping_msg['ping_id'],
+        "server_time": time.time() * 1000.0,
+        "client_time": ping_msg['client_time']
+    }
+
+    profiler.handle_pong_message(pong_msg)
+
+    pong_ok = (
+        profiler.total_pongs_received == 1 and
+        len(profiler.measurements) == 1 and
+        profiler.measurements[0].round_trip_ms >= 50.0
+    )
+
+    all_ok = all_ok and pong_ok
+
+    if profiler.measurements:
         m = profiler.measurements[0]
-        logger.info("   RTT, m.round_trip_ms)
-        logger.info("   One-way, m.one_way_ms)
-        logger.info("   Jitter, m.jitter_ms)
+        print(f"   RTT: {m.round_trip_ms:.2f}ms")
+        print(f"   One-way: {m.one_way_ms:.2f}ms")
+        print(f"   Jitter: {m.jitter_ms:.2f}ms")
 
-    logger.error("   [%s] Pong handling", 'OK' if pong_ok else 'FAIL')
-    logger.info(str())
+    print(f"   [{'OK' if pong_ok else 'FAIL'}] Pong handling")
+    print()
 
-    # Test 4)
+    # Test 4: Multiple measurements
+    print("4. Testing Multiple Measurements...")
 
-    for i in range(100))
+    for i in range(100):
+        ping = profiler.create_ping_message()
         time.sleep(0.001)  # Simulate small delay
 
         pong = {
-            "type",
-            "ping_id",
-            "server_time") * 1000.0,
-            "client_time")
+            "type": "pong",
+            "ping_id": ping['ping_id'],
+            "server_time": time.time() * 1000.0,
+            "client_time": ping['client_time']
+        }
+
+        profiler.handle_pong_message(pong)
 
     multi_ok = (
         profiler.total_pings_sent == 101 and
         profiler.total_pongs_received == 101 and
         len(profiler.measurements) == 101
+    )
 
     all_ok = all_ok and multi_ok
 
-    logger.info("   Total measurements, len(profiler.measurements))
-    logger.info("   Success rate, profiler.total_pongs_received / profiler.total_pings_sent * 100)
-    logger.error("   [%s] Multiple measurements", 'OK' if multi_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Total measurements: {len(profiler.measurements)}")
+    print(f"   Success rate: {profiler.total_pongs_received / profiler.total_pings_sent * 100:.1f}%")
+    print(f"   [{'OK' if multi_ok else 'FAIL'}] Multiple measurements")
+    print()
 
-    # Test 5)
+    # Test 5: Statistics
+    print("5. Testing Statistics...")
     stats = profiler.get_statistics()
 
     stats_ok = (
         stats['total_samples'] == 101 and
         stats['avg_latency_ms'] > 0 and
         stats['success_rate'] == 100.0
+    )
 
     all_ok = all_ok and stats_ok
 
-    logger.info("   Avg latency, stats['avg_latency_ms'])
-    logger.info("   Min latency, stats['min_latency_ms'])
-    logger.info("   Max latency, stats['max_latency_ms'])
-    logger.info("   P95 latency, stats['p95_latency_ms'])
-    logger.info("   P99 latency, stats['p99_latency_ms'])
-    logger.info("   Avg jitter, stats['avg_jitter_ms'])
-    logger.info("   Success rate, stats['success_rate'])
-    logger.info("   Meets SC-001, stats['meets_sc001'])
-    logger.info("   Meets SC-003, stats['meets_sc003'])
-    logger.error("   [%s] Statistics", 'OK' if stats_ok else 'FAIL')
-    logger.info(str())
-
-    # Test 6)
+    print(f"   Avg latency: {stats['avg_latency_ms']:.2f}ms")
+    print(f"   Min latency: {stats['min_latency_ms']:.2f}ms")
+    print(f"   Max latency: {stats['max_latency_ms']:.2f}ms")
+    print(f"   P95 latency: {stats['p95_latency_ms']:.2f}ms")
+    print(f"   P99 latency: {stats['p99_latency_ms']:.2f}ms")
+    print(f"   Avg jitter: {stats['avg_jitter_ms']:.2f}ms")
+    print(f"   Success rate: {stats['success_rate']:.1f}%")
+    print(f"   Meets SC-001: {stats['meets_sc001']}")
+    print(f"   Meets SC-003: {stats['meets_sc003']}")
+    print(f"   [{'OK' if stats_ok else 'FAIL'}] Statistics")
+    print()
+
+    # Test 6: Latency distribution
+    print("6. Testing Latency Distribution...")
     dist = profiler.get_latency_distribution(bucket_size_ms=10.0)
 
     dist_ok = len(dist['buckets']) > 0
 
     all_ok = all_ok and dist_ok
 
-    logger.info("   Distribution buckets, len(dist['buckets']))
-    for i in range(min(5, len(dist['buckets'])):
-        logger.info("     %s)", dist['buckets'][i], dist['counts'][i], dist['percentages'][i])
+    print(f"   Distribution buckets: {len(dist['buckets'])}")
+    for i in range(min(5, len(dist['buckets']))):
+        print(f"     {dist['buckets'][i]}: {dist['counts'][i]} ({dist['percentages'][i]:.1f}%)")
 
-    logger.error("   [%s] Distribution", 'OK' if dist_ok else 'FAIL')
-    logger.info(str())
+    print(f"   [{'OK' if dist_ok else 'FAIL'}] Distribution")
+    print()
 
-    # Test 7)
+    # Test 7: Timeout handling
+    print("7. Testing Timeout Handling...")
 
     # Create ping but don't respond
     timeout_ping = profiler.create_ping_message()
     time.sleep(1.1)  # Wait longer than timeout
 
     profiler.cleanup_pending_pings(timeout_s=1.0)
 
     timeout_ok = profiler.total_timeouts == 1
 
     all_ok = all_ok and timeout_ok
 
-    logger.info("   Total timeouts, profiler.total_timeouts)
-    logger.error("   [%s] Timeout handling", 'OK' if timeout_ok else 'FAIL')
-    logger.info(str())
+    print(f"   Total timeouts: {profiler.total_timeouts}")
+    print(f"   [{'OK' if timeout_ok else 'FAIL'}] Timeout handling")
+    print()
 
-    logger.info("=" * 60)
-    if all_ok)
-    else)
-    logger.info("=" * 60)
+    print("=" * 60)
+    if all_ok:
+        print("Self-Test PASSED")
+    else:
+        print("Self-Test FAILED - Review failures above")
+    print("=" * 60)
 
     return all_ok
 
 
-if __name__ == "__main__")
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/timeline_player.py b/server/timeline_player.py
index 38eb2898c5004c1aa39cd1835fe09133fc06c203..33fcd374d071efadb8e9963b22279692b0ac08be 100644
--- a/server/timeline_player.py
+++ b/server/timeline_player.py
@@ -1,492 +1,700 @@
 """
 Timeline Player - Feature 018
 Playback system for recorded sessions with synchronized replay, scrubbing, and speed control
 
 Implements:
-
+- FR-001: Playback API (/api/play, /api/pause, /api/seek, /api/speed)
 - FR-002: Load session files from Feature 017
 - FR-003: Sync master clock to audio sample time; interpolate metrics
 - FR-004: WebSocket /ws/playback streaming
-
+- FR-005: Frontend timeline support (backend)
 - FR-006: Playback range selection and loop
 - FR-007: Playback progress display
 
 Success Criteria:
 - SC-001: Audio/visual sync error <10ms
 - SC-002: Scrubbing latency <100ms
 - SC-003: Memory footprint <250MB for 30-min session
+- SC-004: No dropped frames
+"""
 
 import os
 import json
 import time
 import wave
 import threading
 import numpy as np
 from pathlib import Path
 from typing import Optional, Dict, List, Any, Callable
 from dataclasses import dataclass
 from enum import Enum
 
 
 class PlaybackState(Enum):
     """Playback state"""
     STOPPED = "stopped"
     PLAYING = "playing"
     PAUSED = "paused"
 
 
 @dataclass
-class TimelinePlayerConfig)
-    update_rate)
+class TimelinePlayerConfig:
+    """Configuration for Timeline Player"""
+
+    # Playback update rate (Hz)
+    update_rate: float = 30.0
+
+    # Audio chunk size for streaming (samples)
     audio_chunk_size: int = 4096
 
     # Default playback speed
     default_speed: float = 1.0
 
     # Enable logging
     enable_logging: bool = True
 
 
 class TimelinePlayer:
     """
     Timeline Player for synchronized session playback
 
     Features:
     - Synchronized audio and metrics playback
-
-
+    - Scrubbing and seeking (SC-002: <100ms latency)
+    - Speed control (0.5x, 1x, 2x)
     - Range selection and loop support
-
+    - Memory-efficient streaming (SC-003: <250MB)
     """
 
-    def __init__(self, config: Optional[TimelinePlayerConfig]) :
+    def __init__(self, config: Optional[TimelinePlayerConfig] = None):
         """
         Initialize Timeline Player
 
         Args:
-            config)
+            config: TimelinePlayerConfig (uses defaults if None)
         """
         self.config = config or TimelinePlayerConfig()
 
         # Playback state
         self.state: PlaybackState = PlaybackState.STOPPED
         self.current_time: float = 0.0  # Current playback time in seconds
         self.playback_speed: float = self.config.default_speed
 
         # Session data
         self.session_path: Optional[Path] = None
         self.session_metadata: Optional[Dict] = None
         self.duration: float = 0.0  # Total duration in seconds
 
         # Audio data
         self.audio_file: Optional[wave.Wave_read] = None
         self.sample_rate: int = 48000
-        self.num_channels)
+        self.num_channels: int = 2
+
+        # Metrics data (loaded in memory - small footprint)
         self.metrics_frames: List[Dict] = []
         self.phi_frames: List[Dict] = []
         self.controls_events: List[Dict] = []
 
         # Playback range and loop
         self.range_start: float = 0.0
         self.range_end: Optional[float] = None  # None = end of session
         self.loop_enabled: bool = False
 
         # Playback thread
         self.playback_thread: Optional[threading.Thread] = None
-        self.playback_stop_event)
+        self.playback_stop_event: threading.Event = threading.Event()
 
         # Frame callback (for WebSocket streaming)
-        self.frame_callback, None]] = None
+        self.frame_callback: Optional[Callable[[Dict], None]] = None
 
         # Last frame time (for update rate control)
         self.last_frame_time: float = 0.0
 
         # Statistics
-        self.frames_streamed)
+        self.frames_streamed: int = 0
 
-    def load_session(self, session_path) :
+        print("[TimelinePlayer] Initialized")
+
+    def load_session(self, session_path: str) -> bool:
+        """
+        Load recorded session (FR-002)
+
+        Args:
             session_path: Path to session directory
 
-        Returns, False otherwise
+        Returns:
+            True if loaded successfully, False otherwise
         """
-        try)
+        try:
+            self.session_path = Path(session_path)
 
             if not self.session_path.exists():
-                logger.error("[TimelinePlayer] ERROR: Session path does not exist, session_path)
+                print(f"[TimelinePlayer] ERROR: Session path does not exist: {session_path}")
                 return False
 
             # Load metadata
             metadata_path = self.session_path / "session.json"
-            if metadata_path.exists(), 'r') as f)
+            if metadata_path.exists():
+                with open(metadata_path, 'r') as f:
+                    self.session_metadata = json.load(f)
                     self.duration = self.session_metadata.get('duration_seconds', 0.0)
 
             # Open audio file (streaming mode - SC-003)
             audio_path = self.session_path / "audio.wav"
-            if audio_path.exists()), 'rb')
+            if audio_path.exists():
+                self.audio_file = wave.open(str(audio_path), 'rb')
                 self.sample_rate = self.audio_file.getframerate()
                 self.num_channels = self.audio_file.getnchannels()
 
                 # Calculate duration from audio if metadata not available
-                if self.duration == 0.0)
+                if self.duration == 0.0:
+                    num_frames = self.audio_file.getnframes()
                     self.duration = num_frames / self.sample_rate
 
             # Load metrics (small memory footprint)
             self._load_metrics()
             self._load_phi()
             self._load_controls()
 
             # Set range end to duration
             if self.range_end is None:
                 self.range_end = self.duration
 
             if self.config.enable_logging:
-                logger.info("[TimelinePlayer] Session loaded, self.session_path.name)
-                logger.info("[TimelinePlayer]   duration, self.duration)
-                logger.info("[TimelinePlayer]   sample_rate, self.sample_rate)
-                logger.info("[TimelinePlayer]   metrics_frames, len(self.metrics_frames))
+                print(f"[TimelinePlayer] Session loaded: {self.session_path.name}")
+                print(f"[TimelinePlayer]   duration: {self.duration:.2f}s")
+                print(f"[TimelinePlayer]   sample_rate: {self.sample_rate}")
+                print(f"[TimelinePlayer]   metrics_frames: {len(self.metrics_frames)}")
 
             return True
 
         except Exception as e:
-            logger.error("[TimelinePlayer] ERROR loading session, e)
+            print(f"[TimelinePlayer] ERROR loading session: {e}")
             import traceback
             traceback.print_exc()
             return False
 
-    def _load_metrics(self) :
-                for line in f)))
+    def _load_metrics(self):
+        """Load metrics JSONL file"""
+        metrics_path = self.session_path / "metrics.jsonl"
+        if metrics_path.exists():
+            self.metrics_frames = []
+            with open(metrics_path, 'r') as f:
+                for line in f:
+                    if line.strip():
+                        self.metrics_frames.append(json.loads(line))
+
+    def _load_phi(self):
+        """Load phi JSONL file"""
+        phi_path = self.session_path / "phi.jsonl"
+        if phi_path.exists():
+            self.phi_frames = []
+            with open(phi_path, 'r') as f:
+                for line in f:
+                    if line.strip():
+                        self.phi_frames.append(json.loads(line))
+
+    def _load_controls(self):
+        """Load controls JSONL file"""
+        controls_path = self.session_path / "controls.jsonl"
+        if controls_path.exists():
+            self.controls_events = []
+            with open(controls_path, 'r') as f:
+                for line in f:
+                    if line.strip():
+                        self.controls_events.append(json.loads(line))
+
+    def play(self) -> bool:
+        """
+        Start playback (FR-001)
 
-    def _load_phi(self) :
-                for line in f)))
+        Returns:
+            True if playback started, False otherwise
+        """
+        if self.session_path is None:
+            print("[TimelinePlayer] ERROR: No session loaded")
+            return False
 
-    def _load_controls(self) :
-                for line in f)))
+        if self.state == PlaybackState.PLAYING:
+            return True  # Already playing
 
-    def play(self) :
-            logger.error("[TimelinePlayer] ERROR)
-            return False
+        self.state = PlaybackState.PLAYING
 
-        if self.state == PlaybackState.PLAYING)
+        # Start playback thread
+        self.playback_stop_event.clear()
         self.playback_thread = threading.Thread(target=self._playback_loop, daemon=True)
         self.playback_thread.start()
 
-        if self.config.enable_logging, self.current_time)
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Playback started at {self.current_time:.2f}s")
 
         return True
 
-    def pause(self) :
+    def pause(self) -> bool:
+        """
+        Pause playback (FR-001)
+
+        Returns:
+            True if paused, False otherwise
+        """
+        if self.state != PlaybackState.PLAYING:
+            return False
+
+        self.state = PlaybackState.PAUSED
+
+        # Stop playback thread
+        self.playback_stop_event.set()
+        if self.playback_thread:
+            self.playback_thread.join(timeout=1.0)
+
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Playback paused at {self.current_time:.2f}s")
+
+        return True
+
+    def stop(self) -> bool:
         """
         Stop playback
 
-        Returns, False otherwise
+        Returns:
+            True if stopped, False otherwise
         """
-        if self.state == PlaybackState.STOPPED)
-        if self.playback_thread)
+        if self.state == PlaybackState.STOPPED:
+            return True
+
+        self.state = PlaybackState.STOPPED
+
+        # Stop playback thread
+        self.playback_stop_event.set()
+        if self.playback_thread:
+            self.playback_thread.join(timeout=1.0)
 
         # Reset to beginning
         self.current_time = self.range_start
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[TimelinePlayer] Playback stopped")
 
         return True
 
-    def seek(self, time_seconds) :
+    def seek(self, time_seconds: float) -> bool:
+        """
+        Seek to specific time (FR-001, SC-002: <100ms latency)
+
+        Args:
             time_seconds: Time to seek to in seconds
 
-        Returns, False otherwise
+        Returns:
+            True if seek successful, False otherwise
         """
         # Clamp to valid range
         time_seconds = max(self.range_start, min(time_seconds, self.range_end))
 
         was_playing = (self.state == PlaybackState.PLAYING)
 
         # Pause if playing
-        if was_playing)
+        if was_playing:
+            self.pause()
 
         # Update current time
         self.current_time = time_seconds
 
         # Resume if was playing
-        if was_playing)
+        if was_playing:
+            self.play()
 
-        if self.config.enable_logging, time_seconds)
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Seeked to {time_seconds:.2f}s")
 
         return True
 
-    def set_speed(self, speed) :
-            speed, 1.0, 2.0, etc.)
+    def set_speed(self, speed: float) -> bool:
+        """
+        Set playback speed (FR-001)
+
+        Args:
+            speed: Playback speed multiplier (0.5, 1.0, 2.0, etc.)
 
-        Returns, False otherwise
+        Returns:
+            True if speed set, False otherwise
         """
         if speed <= 0:
             return False
 
         self.playback_speed = speed
 
-        if self.config.enable_logging, speed)
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Speed set to {speed}x")
 
         return True
 
-    def set_range(self, start, end) :
+    def set_range(self, start: float, end: float) -> bool:
+        """
+        Set playback range (FR-006)
+
+        Args:
             start: Start time in seconds
             end: End time in seconds
 
-        Returns, False otherwise
+        Returns:
+            True if range set, False otherwise
         """
         # Validate range
         if start < 0 or end > self.duration or start >= end:
             return False
 
         self.range_start = start
         self.range_end = end
 
         # Clamp current time to range
         if self.current_time < start or self.current_time > end:
             self.current_time = start
 
-        if self.config.enable_logging, %s]", start, end)
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Range set to [{start:.2f}, {end:.2f}]")
 
         return True
 
-    def set_loop(self, enabled) :
-            enabled, False to disable
+    def set_loop(self, enabled: bool) -> bool:
+        """
+        Enable/disable loop (FR-006)
+
+        Args:
+            enabled: True to enable loop, False to disable
 
         Returns:
             True always
         """
         self.loop_enabled = enabled
 
-        if self.config.enable_logging, 'enabled' if enabled else 'disabled')
+        if self.config.enable_logging:
+            print(f"[TimelinePlayer] Loop {'enabled' if enabled else 'disabled'}")
 
         return True
 
-    def _playback_loop(self) :
-                if self.loop_enabled)
+    def _playback_loop(self):
+        """
+        Playback thread loop (FR-003, SC-001, SC-004)
+
+        Streams interpolated frames at update_rate Hz
+        """
+        update_interval = 1.0 / self.config.update_rate
+        last_update_time = time.perf_counter()
+
+        while not self.playback_stop_event.is_set() and self.state == PlaybackState.PLAYING:
+            current_perf = time.perf_counter()
+            elapsed = current_perf - last_update_time
+
+            # Advance playback time based on speed (FR-003)
+            time_delta = elapsed * self.playback_speed
+            self.current_time += time_delta
+
+            # Check range bounds
+            if self.current_time >= self.range_end:
+                if self.loop_enabled:
+                    # Loop back to start (FR-006)
                     self.current_time = self.range_start
-                else)
+                else:
+                    # Stop at end
+                    self.state = PlaybackState.PAUSED
+                    self.current_time = self.range_end
+                    break
+
+            # Generate and stream frame (FR-004)
             frame = self._generate_playback_frame()
-            if self.frame_callback and frame)
+            if self.frame_callback and frame:
+                self.frame_callback(frame)
                 self.frames_streamed += 1
 
             last_update_time = current_perf
 
             # Sleep to maintain update rate
             sleep_time = update_interval - (time.perf_counter() - current_perf)
-            if sleep_time > 0)
+            if sleep_time > 0:
+                time.sleep(sleep_time)
+
+    def _generate_playback_frame(self) -> Dict:
+        """
+        Generate playback frame with interpolated metrics (FR-003, SC-001)
 
-    def _generate_playback_frame(self) :
+        Returns:
             Frame dictionary with interpolated data
         """
         frame = {
-            'type',
-            'timestamp',
-            'playback_speed',
-            'state',
-            'progress')
+            'type': 'playback_frame',
+            'timestamp': self.current_time,
+            'playback_speed': self.playback_speed,
+            'state': self.state.value,
+            'progress': self.current_time / self.duration if self.duration > 0 else 0.0
+        }
+
+        # Interpolate metrics (FR-003)
         metrics = self._interpolate_metrics(self.current_time)
-        if metrics)
+        if metrics:
+            frame.update(metrics)
 
         # Interpolate phi
         phi = self._interpolate_phi(self.current_time)
-        if phi, window=0.1)
-        if controls, time) :
+        if phi:
+            frame['phi'] = phi
+
+        # Include recent control events
+        controls = self._get_recent_controls(self.current_time, window=0.1)
+        if controls:
+            frame['controls'] = controls
+
+        return frame
+
+    def _interpolate_metrics(self, time: float) -> Optional[Dict]:
+        """
+        Interpolate metrics at given time (FR-003)
+
+        Uses linear interpolation between frames for smooth playback
+
+        Args:
             time: Time in seconds
 
         Returns:
             Interpolated metrics dictionary or None
         """
         if not self.metrics_frames:
             return None
 
         # Find surrounding frames
         before_frame = None
         after_frame = None
 
-        for frame in self.metrics_frames, 0.0)
+        for frame in self.metrics_frames:
+            frame_time = frame.get('timestamp', 0.0)
             if frame_time <= time:
                 before_frame = frame
             elif frame_time > time:
                 after_frame = frame
                 break
 
         # Edge cases: missing metrics fill with last valid value
         if before_frame is None and after_frame is None:
             return None
-        elif before_frame is None)
-        elif after_frame is None)
+        elif before_frame is None:
+            return self._extract_metrics(after_frame)
+        elif after_frame is None:
+            return self._extract_metrics(before_frame)
 
         # Linear interpolation
         t_before = before_frame.get('timestamp', 0.0)
         t_after = after_frame.get('timestamp', 0.0)
 
-        if t_after - t_before < 0.001, just use before
+        if t_after - t_before < 0.001:  # Too close, just use before
             return self._extract_metrics(before_frame)
 
         alpha = (time - t_before) / (t_after - t_before)
         alpha = np.clip(alpha, 0.0, 1.0)
 
         # Interpolate numeric values
         interpolated = {}
         for key in before_frame.keys():
-            if key == 'timestamp')
+            if key == 'timestamp':
+                continue
+
+            val_before = before_frame.get(key)
             val_after = after_frame.get(key)
 
-            if isinstance(val_before, (int, float)) and isinstance(val_after, (int, float)))
+            if isinstance(val_before, (int, float)) and isinstance(val_after, (int, float)):
+                interpolated[key] = val_before + alpha * (val_after - val_before)
             else:
-                # Non-numeric, frame) :
+                # Non-numeric: use before value
+                interpolated[key] = val_before
+
+        return interpolated
+
+    def _extract_metrics(self, frame: Dict) -> Dict:
+        """Extract metrics from frame (exclude timestamp)"""
+        return {k: v for k, v in frame.items() if k != 'timestamp'}
+
+    def _interpolate_phi(self, time: float) -> Optional[Dict]:
         """
         Interpolate phi parameters at given time
 
         Args:
             time: Time in seconds
 
         Returns:
             Interpolated phi dictionary or None
         """
         if not self.phi_frames:
             return None
 
         # Same logic as metrics interpolation
         before_frame = None
         after_frame = None
 
-        for frame in self.phi_frames, 0.0)
+        for frame in self.phi_frames:
+            frame_time = frame.get('timestamp', 0.0)
             if frame_time <= time:
                 before_frame = frame
             elif frame_time > time:
                 after_frame = frame
                 break
 
         if before_frame is None and after_frame is None:
             return None
-        elif before_frame is None)
-        elif after_frame is None)
+        elif before_frame is None:
+            return self._extract_metrics(after_frame)
+        elif after_frame is None:
+            return self._extract_metrics(before_frame)
 
         t_before = before_frame.get('timestamp', 0.0)
         t_after = after_frame.get('timestamp', 0.0)
 
-        if t_after - t_before < 0.001)
+        if t_after - t_before < 0.001:
+            return self._extract_metrics(before_frame)
 
         alpha = (time - t_before) / (t_after - t_before)
         alpha = np.clip(alpha, 0.0, 1.0)
 
         interpolated = {}
         for key in before_frame.keys():
-            if key == 'timestamp')
+            if key == 'timestamp':
+                continue
+
+            val_before = before_frame.get(key)
             val_after = after_frame.get(key)
 
-            if isinstance(val_before, (int, float)) and isinstance(val_after, (int, float)))
-            else, time, window) :
+            if isinstance(val_before, (int, float)) and isinstance(val_after, (int, float)):
+                interpolated[key] = val_before + alpha * (val_after - val_before)
+            else:
+                interpolated[key] = val_before
+
+        return interpolated
+
+    def _get_recent_controls(self, time: float, window: float = 0.1) -> List[Dict]:
         """
         Get control events within time window
 
         Args:
             time: Current time in seconds
             window: Time window in seconds
 
         Returns:
             List of control events
         """
         recent = []
 
-        for event in self.controls_events, 0.0)
-            if time - window <= event_time <= time)
+        for event in self.controls_events:
+            event_time = event.get('timestamp', 0.0)
+            if time - window <= event_time <= time:
+                recent.append(event)
 
         return recent
 
-    def get_status(self) :
+    def get_status(self) -> Dict:
+        """
+        Get playback status (FR-007)
+
+        Returns:
             Status dictionary
         """
         return {
-            'state',
-            'current_time',
-            'duration',
-            'progress',
-            'playback_speed',
-            'range_start',
-            'range_end',
-            'loop_enabled',
-            'session_loaded',
-            'session_name',
-            'frames_streamed') -> None)
+            'state': self.state.value,
+            'current_time': self.current_time,
+            'duration': self.duration,
+            'progress': self.current_time / self.duration if self.duration > 0 else 0.0,
+            'playback_speed': self.playback_speed,
+            'range_start': self.range_start,
+            'range_end': self.range_end,
+            'loop_enabled': self.loop_enabled,
+            'session_loaded': self.session_path is not None,
+            'session_name': self.session_path.name if self.session_path else None,
+            'frames_streamed': self.frames_streamed
+        }
+
+    def unload_session(self):
+        """Unload current session and free resources"""
+        # Stop playback
+        self.stop()
 
         # Close audio file
-        if self.audio_file)
+        if self.audio_file:
+            self.audio_file.close()
             self.audio_file = None
 
         # Clear data
         self.metrics_frames = []
         self.phi_frames = []
         self.controls_events = []
         self.session_path = None
         self.session_metadata = None
         self.duration = 0.0
         self.current_time = 0.0
 
-        if self.config.enable_logging)
+        if self.config.enable_logging:
+            print("[TimelinePlayer] Session unloaded")
 
 
 # Self-test function
-def _self_test() -> None)
-    logger.info("Timeline Player Self-Test")
-    logger.info("=" * 60)
-
-    # Test 1)
+def _self_test():
+    """Test Timeline Player"""
+    print("=" * 60)
+    print("Timeline Player Self-Test")
+    print("=" * 60)
+
+    # Test 1: Initialization
+    print("\n1. Testing initialization...")
     config = TimelinePlayerConfig(update_rate=30.0)
     player = TimelinePlayer(config)
 
     assert player.state == PlaybackState.STOPPED
     assert player.current_time == 0.0
-    logger.info("   OK)
+    print("   OK: Initialization")
 
-    # Note, just test the basic structure
+    # Note: Full testing requires a recorded session from Feature 017
+    # For now, just test the basic structure
 
-    logger.info("\n2. Testing playback controls...")
+    print("\n2. Testing playback controls...")
 
     # Can't start without loaded session
     assert player.play() == False
-    logger.info("   OK)
+    print("   OK: Play requires loaded session")
 
     # Speed control
     assert player.set_speed(0.5) == True
     assert player.playback_speed == 0.5
     assert player.set_speed(2.0) == True
     assert player.playback_speed == 2.0
-    logger.info("   OK)
+    print("   OK: Speed control")
 
     # Range control
     player.duration = 100.0  # Mock duration
     player.range_end = 100.0
     assert player.set_range(10.0, 50.0) == True
     assert player.range_start == 10.0
     assert player.range_end == 50.0
-    logger.info("   OK)
+    print("   OK: Range control")
 
     # Loop control
     assert player.set_loop(True) == True
     assert player.loop_enabled == True
-    logger.info("   OK)
+    print("   OK: Loop control")
 
     # Seek control
     assert player.seek(25.0) == True
     assert player.current_time == 25.0
-    logger.info("   OK)
+    print("   OK: Seek control")
 
-    logger.info("\n3. Testing status...")
+    print("\n3. Testing status...")
     status = player.get_status()
     assert status['state'] == 'stopped'
     assert status['playback_speed'] == 2.0
     assert status['loop_enabled'] == True
-    logger.info("   OK)
+    print("   OK: Status")
 
-    logger.info("\n" + "=" * 60)
-    logger.info("Self-Test PASSED")
-    logger.info("=" * 60)
-    logger.info("Note)
+    print("\n" + "=" * 60)
+    print("Self-Test PASSED")
+    print("=" * 60)
+    print("Note: Full playback testing requires recorded session")
 
     return True
 
 
-if __name__ == "__main__")
-
-"""  # auto-closed missing docstring
+if __name__ == "__main__":
+    _self_test()
diff --git a/server/websockets/__init__.py b/server/websockets/__init__.py
index 5c43adc62ca86cbd22601257798a1499e6101758..b906d04baa6d734880d4ea341395fba01e730d5c 100644
--- a/server/websockets/__init__.py
+++ b/server/websockets/__init__.py
@@ -1,10 +1,10 @@
 """
 WebSocket handlers
 
 This package contains WebSocket streaming handlers:
-
-
+- metrics_ws: Metrics streaming (30Hz)
+- latency_ws: Latency streaming (10Hz)
 - ui_ws: UI synchronization
 """
 
 __all__ = []
diff --git a/server/websockets/__pycache__/__init__.cpython-311.pyc b/server/websockets/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index d6e39b28ce71871e4626d7090177b6bb85876457..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 277
zcmZ3^%ge<81atp<&S(SDk3k$5V1zP0^8p#t8B!Rc7}6P{7*iO788n%y(z(J@lY;Y;
zvr|hHG7|Gra#D+mxwt|yGK&=o5|gtN(^D0a^Ycm)GxLfS5GspHic%AEGxO5ndaSs(
zxO5dtGvmvPtrS8%6^bkKk~50(^D?UvOEUBGxcoGkZn4M5r{pKc$AcJ&IXUt1D;Yk6
z9QRAMIJKxOwJ4@MHK`cvl;W89_{_Y_lK6PNg34bUHo5sJr8%i~MeIP;ApaCg0f`UH
cjEsyQnAjOvJ}`g?AqH;V26hlEVg*VA01>87#Q*>R

diff --git a/server/ws_gatekeeper.py b/server/ws_gatekeeper.py
index 9bf2fa8876d0925b5c5b5ffbee8f11bac6d2b6ae..0518c004d7fec65c715e7a0988cafab2f798e4f5 100644
--- a/server/ws_gatekeeper.py
+++ b/server/ws_gatekeeper.py
@@ -1,256 +1,319 @@
-"""
-WebSocket Gatekeeper - Feature 024 (FR-002)
+"""WebSocket Gatekeeper - Feature 024 (FR-002)
 
 Enforces WebSocket origin checks, protocol validation, and token binding
 for secure WebSocket connections.
 
 Requirements:
-- FR-002, Dict, Set
+- FR-002: Enforce Origin/Sec-WebSocket-Protocol and host allowlist
+- Token binding for authenticated WS sessions
+- Connection caps per IP to prevent handshake floods
+"""
+
+import logging
+from typing import Any, Dict, Optional, Set
 from urllib.parse import urlparse
 from fastapi import WebSocket, WebSocketDisconnect, status
 from starlette.websockets import WebSocketState
 import time
 
 logger = logging.getLogger(__name__)
 
 
-class WebSocketGatekeeper)"""
+class WebSocketGatekeeper:
+    """WebSocket security gatekeeper (FR-002)"""
 
-    def __init__(self, config: dict) ://localhost))
+    def __init__(self, config: Dict[str, Any]) -> None:
+        self.config = config
+        self.allowed_origins = set(config.get('allowed_origins', ['http://localhost:3000']))
         self.allowed_protocols = set(config.get('allowed_protocols', ['soundlab-v1']))
         self.require_auth = config.get('ws_require_auth', True)
         self.max_connections_per_ip = config.get('ws_max_connections_per_ip', 10)
 
         # Track connections per IP
-        self.connections_by_ip, int] = {}
+        self.connections_by_ip: Dict[str, int] = {}
 
         # Active authenticated connections
-        self.authenticated_connections)
+        self.authenticated_connections: Set[WebSocket] = set()
 
-    def check_origin(self, origin) :
+    def check_origin(self, origin: Optional[str]) -> bool:
+        """
+        Verify Origin header against allowlist (FR-002).
+
+        Args:
             origin: Origin header value
 
-        Returns, False otherwise
+        Returns:
+            True if origin is allowed, False otherwise
         """
-        if not origin)
+        if not origin:
+            logger.warning("WebSocket connection without Origin header")
             return False
 
         # Parse and normalize origin
         parsed = urlparse(origin)
         normalized_origin = f"{parsed.scheme}://{parsed.netloc}"
 
         # Check against allowlist
         if normalized_origin not in self.allowed_origins:
-            logger.warning(f"WebSocket origin not allowed)
+            logger.warning(f"WebSocket origin not allowed: {origin}")
             return False
 
         return True
 
-    def check_protocol(self, protocol) :
+    def check_protocol(self, protocol: Optional[str]) -> bool:
+        """
+        Verify Sec-WebSocket-Protocol header (FR-002).
+
+        Args:
             protocol: WebSocket protocol value
 
-        Returns, False otherwise
+        Returns:
+            True if protocol is allowed, False otherwise
         """
-        if not protocol)
+        if not protocol:
+            logger.warning("WebSocket connection without protocol")
             return False
 
         if protocol not in self.allowed_protocols:
-            logger.warning(f"WebSocket protocol not allowed)
+            logger.warning(f"WebSocket protocol not allowed: {protocol}")
             return False
 
         return True
 
-    def check_connection_limit(self, client_ip) :
+    def check_connection_limit(self, client_ip: str) -> bool:
         """
         Check connection limit per IP to prevent handshake floods.
 
         Args:
             client_ip: Client IP address
 
-        Returns, False otherwise
+        Returns:
+            True if within limit, False otherwise
         """
         current_connections = self.connections_by_ip.get(client_ip, 0)
 
         if current_connections >= self.max_connections_per_ip:
             logger.warning(
-                f"Connection limit exceeded for IP {client_ip})
+                f"Connection limit exceeded for IP {client_ip}: "
+                f"{current_connections}/{self.max_connections_per_ip}"
+            )
             return False
 
         return True
 
-    def register_connection(self, client_ip: str, websocket: WebSocket) : str, websocket: WebSocket) :
+    def register_connection(self, client_ip: str, websocket: WebSocket) -> None:
+        """Register new connection"""
+        self.connections_by_ip[client_ip] = \
+            self.connections_by_ip.get(client_ip, 0) + 1
+
+        logger.info(
+            f"WebSocket connection registered: {client_ip} "
+            f"({self.connections_by_ip[client_ip]} total)"
+        )
+
+    def unregister_connection(self, client_ip: str, websocket: WebSocket) -> None:
         """Unregister closed connection"""
         if client_ip in self.connections_by_ip:
             self.connections_by_ip[client_ip] -= 1
 
-            if self.connections_by_ip[client_ip] <= 0)
+            if self.connections_by_ip[client_ip] <= 0:
+                del self.connections_by_ip[client_ip]
+
+        self.authenticated_connections.discard(websocket)
 
         logger.info(
-            f"WebSocket connection unregistered, 0)} remaining)"
+            f"WebSocket connection unregistered: {client_ip} "
+            f"({self.connections_by_ip.get(client_ip, 0)} remaining)"
+        )
 
     async def authenticate_websocket(
         self,
-        websocket,
+        websocket: WebSocket,
         token_validator
-    ) :
+    ) -> Optional[dict]:
         """
         Authenticate WebSocket connection via token.
 
         Args:
             websocket: WebSocket connection
             token_validator: TokenValidator instance
 
-        Returns, None otherwise
+        Returns:
+            Decoded token payload if valid, None otherwise
         """
         # Expect first message to be auth token
-        try)
+        try:
+            # Wait for auth message with timeout
+            message = await websocket.receive_json()
 
-            if message.get('type') != 'auth')
+            if message.get('type') != 'auth':
+                logger.warning("First WS message not auth type")
                 await websocket.close(code=4401, reason="Authentication required")
                 return None
 
             token = message.get('token')
-            if not token)
+            if not token:
+                logger.warning("Auth message missing token")
                 await websocket.close(code=4401, reason="Token required")
                 return None
 
             # Verify token
-            try)
+            try:
+                payload = token_validator.verify_token(token)
 
                 # Mark connection as authenticated
                 self.authenticated_connections.add(websocket)
 
-                logger.info(f"WebSocket authenticated)}")
+                logger.info(f"WebSocket authenticated: user={payload.get('sub')}")
                 return payload
 
             except Exception as e:
-                logger.warning(f"WebSocket auth failed)
+                logger.warning(f"WebSocket auth failed: {e}")
                 await websocket.close(code=4401, reason="Invalid token")
                 return None
 
-        except WebSocketDisconnect)
+        except WebSocketDisconnect:
+            logger.info("WebSocket disconnected during auth")
             return None
         except Exception as e:
-            logger.error(f"WebSocket auth error)
+            logger.error(f"WebSocket auth error: {e}")
             await websocket.close(code=4500, reason="Internal error")
             return None
 
     async def accept_connection(
         self,
-        websocket,
+        websocket: WebSocket,
         token_validator=None
-    ) :
+    ) -> Optional[dict]:
         """
         Accept and validate WebSocket connection.
 
         Args:
             websocket: WebSocket connection
             token_validator: Optional TokenValidator for auth
 
-        Returns, None otherwise
+        Returns:
+            User payload if authenticated, None otherwise
         """
         # Get client info
         client_ip = websocket.client.host
         origin = websocket.headers.get('origin')
         protocol = websocket.headers.get('sec-websocket-protocol')
 
         # Check origin (FR-002)
-        if not self.check_origin(origin), reason="Origin not allowed")
+        if not self.check_origin(origin):
+            await websocket.close(code=4403, reason="Origin not allowed")
             return None
 
         # Check protocol (FR-002)
-        if not self.check_protocol(protocol), reason="Protocol not supported")
+        if not self.check_protocol(protocol):
+            await websocket.close(code=4400, reason="Protocol not supported")
             return None
 
         # Check connection limit (handshake flood protection)
-        if not self.check_connection_limit(client_ip), reason="Too many connections")
+        if not self.check_connection_limit(client_ip):
+            await websocket.close(code=4429, reason="Too many connections")
             return None
 
         # Accept connection
         await websocket.accept(subprotocol=protocol)
 
         # Register connection
         self.register_connection(client_ip, websocket)
 
         # Authenticate if required
         user_payload = None
-        if self.require_auth and token_validator,
+        if self.require_auth and token_validator:
+            user_payload = await self.authenticate_websocket(
+                websocket,
                 token_validator
+            )
 
-            if not user_payload, websocket)
+            if not user_payload:
+                self.unregister_connection(client_ip, websocket)
                 return None
 
         return user_payload
 
-    async def close_connection(self, websocket):
+    async def close_connection(self, websocket: WebSocket):
         """Safely close WebSocket connection"""
         client_ip = websocket.client.host
 
         try:
-            if websocket.client_state != WebSocketState.DISCONNECTED)
+            if websocket.client_state != WebSocketState.DISCONNECTED:
+                await websocket.close()
         except Exception as e:
-            logger.error(f"Error closing WebSocket)
+            logger.error(f"Error closing WebSocket: {e}")
 
         self.unregister_connection(client_ip, websocket)
 
-    def get_stats(self) :
+    def get_stats(self) -> dict:
         """Get gatekeeper statistics"""
         return {
-            'total_connections')),
-            'authenticated_connections'),
-            'connections_by_ip'),
+            'total_connections': sum(self.connections_by_ip.values()),
+            'authenticated_connections': len(self.authenticated_connections),
+            'connections_by_ip': dict(self.connections_by_ip),
             'config': {
-                'allowed_origins'),
-                'allowed_protocols'),
+                'allowed_origins': list(self.allowed_origins),
+                'allowed_protocols': list(self.allowed_protocols),
                 'max_per_ip': self.max_connections_per_ip
             }
         }
 
 
 async def websocket_endpoint_with_auth(
-    websocket,
-    gatekeeper,
+    websocket: WebSocket,
+    gatekeeper: WebSocketGatekeeper,
     token_validator,
     handler
 ):
     """
     WebSocket endpoint wrapper with authentication and security checks.
 
-    Usage)
-        async def dashboard_ws(websocket),
+    Usage:
+        @app.websocket("/ws/dashboard")
+        async def dashboard_ws(websocket: WebSocket):
+            await websocket_endpoint_with_auth(
+                websocket,
                 app.state.gatekeeper,
                 app.state.token_validator,
                 handle_dashboard_messages
+            )
 
     Args:
         websocket: WebSocket connection
         gatekeeper: WebSocketGatekeeper instance
         token_validator: TokenValidator instance
         handler: Async function to handle messages
     """
     client_ip = websocket.client.host
 
-    try,
+    try:
+        # Accept and authenticate connection
+        user_payload = await gatekeeper.accept_connection(
+            websocket,
             token_validator
+        )
 
         if not user_payload:
-            logger.warning(f"WebSocket auth failed)
+            logger.warning(f"WebSocket auth failed: {client_ip}")
             return
 
         # Connection accepted and authenticated
-        logger.info(f"WebSocket ready, user={user_payload.get('sub')}")
+        logger.info(f"WebSocket ready: {client_ip}, user={user_payload.get('sub')}")
 
         # Send auth success
         await websocket.send_json({
-            'type',
-            'user')
+            'type': 'auth_success',
+            'user': user_payload.get('sub')
         })
 
         # Handle messages
         await handler(websocket, user_payload)
 
     except WebSocketDisconnect:
-        logger.info(f"WebSocket disconnected)
+        logger.info(f"WebSocket disconnected: {client_ip}")
     except Exception as e:
-        logger.error(f"WebSocket error)
-    finally)
+        logger.error(f"WebSocket error: {e}")
+    finally:
+        await gatekeeper.close_connection(websocket)
